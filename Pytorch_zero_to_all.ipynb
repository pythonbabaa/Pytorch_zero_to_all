{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Lecture 2: Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1.0 # a random guess: random value\n",
    "\n",
    "# model for forward pass\n",
    "def forward(x):\n",
    "    return x * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1, 2, 3]\n",
    "y_data = [2, 4, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 0.0\n",
      "\t 1 2 0.0 4.0\n",
      "\t 2 4 0.0 16.0\n",
      "\t 3 6 0.0 36.0\n",
      "MSE =  18.666666666666668\n",
      "w = 0.0\n",
      "\t 1 2 0.1 3.61\n",
      "\t 2 4 0.2 14.44\n",
      "\t 3 6 0.30000000000000004 32.49\n",
      "MSE =  16.846666666666668\n",
      "w = 0.0\n",
      "\t 1 2 0.2 3.24\n",
      "\t 2 4 0.4 12.96\n",
      "\t 3 6 0.6000000000000001 29.160000000000004\n",
      "MSE =  15.120000000000003\n",
      "w = 0.0\n",
      "\t 1 2 0.30000000000000004 2.8899999999999997\n",
      "\t 2 4 0.6000000000000001 11.559999999999999\n",
      "\t 3 6 0.9000000000000001 26.009999999999998\n",
      "MSE =  13.486666666666665\n",
      "w = 0.0\n",
      "\t 1 2 0.4 2.5600000000000005\n",
      "\t 2 4 0.8 10.240000000000002\n",
      "\t 3 6 1.2000000000000002 23.04\n",
      "MSE =  11.946666666666667\n",
      "w = 0.0\n",
      "\t 1 2 0.5 2.25\n",
      "\t 2 4 1.0 9.0\n",
      "\t 3 6 1.5 20.25\n",
      "MSE =  10.5\n",
      "w = 1.0\n",
      "\t 1 2 0.6000000000000001 1.9599999999999997\n",
      "\t 2 4 1.2000000000000002 7.839999999999999\n",
      "\t 3 6 1.8000000000000003 17.639999999999993\n",
      "MSE =  9.146666666666663\n",
      "w = 1.0\n",
      "\t 1 2 0.7000000000000001 1.6899999999999995\n",
      "\t 2 4 1.4000000000000001 6.759999999999998\n",
      "\t 3 6 2.1 15.209999999999999\n",
      "MSE =  7.886666666666666\n",
      "w = 1.0\n",
      "\t 1 2 0.8 1.44\n",
      "\t 2 4 1.6 5.76\n",
      "\t 3 6 2.4000000000000004 12.959999999999997\n",
      "MSE =  6.719999999999999\n",
      "w = 1.0\n",
      "\t 1 2 0.9 1.2100000000000002\n",
      "\t 2 4 1.8 4.840000000000001\n",
      "\t 3 6 2.7 10.889999999999999\n",
      "MSE =  5.646666666666666\n",
      "w = 1.0\n",
      "\t 1 2 1.0 1.0\n",
      "\t 2 4 2.0 4.0\n",
      "\t 3 6 3.0 9.0\n",
      "MSE =  4.666666666666667\n",
      "w = 1.0\n",
      "\t 1 2 1.1 0.8099999999999998\n",
      "\t 2 4 2.2 3.2399999999999993\n",
      "\t 3 6 3.3000000000000003 7.289999999999998\n",
      "MSE =  3.779999999999999\n",
      "w = 1.0\n",
      "\t 1 2 1.2000000000000002 0.6399999999999997\n",
      "\t 2 4 2.4000000000000004 2.5599999999999987\n",
      "\t 3 6 3.6000000000000005 5.759999999999997\n",
      "MSE =  2.986666666666665\n",
      "w = 1.0\n",
      "\t 1 2 1.3 0.48999999999999994\n",
      "\t 2 4 2.6 1.9599999999999997\n",
      "\t 3 6 3.9000000000000004 4.409999999999998\n",
      "MSE =  2.2866666666666657\n",
      "w = 1.0\n",
      "\t 1 2 1.4000000000000001 0.3599999999999998\n",
      "\t 2 4 2.8000000000000003 1.4399999999999993\n",
      "\t 3 6 4.2 3.2399999999999993\n",
      "MSE =  1.6799999999999995\n",
      "w = 2.0\n",
      "\t 1 2 1.5 0.25\n",
      "\t 2 4 3.0 1.0\n",
      "\t 3 6 4.5 2.25\n",
      "MSE =  1.1666666666666667\n",
      "w = 2.0\n",
      "\t 1 2 1.6 0.15999999999999992\n",
      "\t 2 4 3.2 0.6399999999999997\n",
      "\t 3 6 4.800000000000001 1.4399999999999984\n",
      "MSE =  0.746666666666666\n",
      "w = 2.0\n",
      "\t 1 2 1.7000000000000002 0.0899999999999999\n",
      "\t 2 4 3.4000000000000004 0.3599999999999996\n",
      "\t 3 6 5.1000000000000005 0.809999999999999\n",
      "MSE =  0.4199999999999995\n",
      "w = 2.0\n",
      "\t 1 2 1.8 0.03999999999999998\n",
      "\t 2 4 3.6 0.15999999999999992\n",
      "\t 3 6 5.4 0.3599999999999996\n",
      "MSE =  0.1866666666666665\n",
      "w = 2.0\n",
      "\t 1 2 1.9000000000000001 0.009999999999999974\n",
      "\t 2 4 3.8000000000000003 0.0399999999999999\n",
      "\t 3 6 5.7 0.0899999999999999\n",
      "MSE =  0.046666666666666586\n",
      "w = 2.0\n",
      "\t 1 2 2.0 0.0\n",
      "\t 2 4 4.0 0.0\n",
      "\t 3 6 6.0 0.0\n",
      "MSE =  0.0\n",
      "w = 2.0\n",
      "\t 1 2 2.1 0.010000000000000018\n",
      "\t 2 4 4.2 0.04000000000000007\n",
      "\t 3 6 6.300000000000001 0.09000000000000043\n",
      "MSE =  0.046666666666666835\n",
      "w = 2.0\n",
      "\t 1 2 2.2 0.04000000000000007\n",
      "\t 2 4 4.4 0.16000000000000028\n",
      "\t 3 6 6.6000000000000005 0.36000000000000065\n",
      "MSE =  0.18666666666666698\n",
      "w = 2.0\n",
      "\t 1 2 2.3000000000000003 0.09000000000000016\n",
      "\t 2 4 4.6000000000000005 0.36000000000000065\n",
      "\t 3 6 6.9 0.8100000000000006\n",
      "MSE =  0.42000000000000054\n",
      "w = 2.0\n",
      "\t 1 2 2.4000000000000004 0.16000000000000028\n",
      "\t 2 4 4.800000000000001 0.6400000000000011\n",
      "\t 3 6 7.200000000000001 1.4400000000000026\n",
      "MSE =  0.7466666666666679\n",
      "w = 2.0\n",
      "\t 1 2 2.5 0.25\n",
      "\t 2 4 5.0 1.0\n",
      "\t 3 6 7.5 2.25\n",
      "MSE =  1.1666666666666667\n",
      "w = 3.0\n",
      "\t 1 2 2.6 0.3600000000000001\n",
      "\t 2 4 5.2 1.4400000000000004\n",
      "\t 3 6 7.800000000000001 3.2400000000000024\n",
      "MSE =  1.6800000000000008\n",
      "w = 3.0\n",
      "\t 1 2 2.7 0.49000000000000027\n",
      "\t 2 4 5.4 1.960000000000001\n",
      "\t 3 6 8.100000000000001 4.410000000000006\n",
      "MSE =  2.2866666666666693\n",
      "w = 3.0\n",
      "\t 1 2 2.8000000000000003 0.6400000000000005\n",
      "\t 2 4 5.6000000000000005 2.560000000000002\n",
      "\t 3 6 8.4 5.760000000000002\n",
      "MSE =  2.986666666666668\n",
      "w = 3.0\n",
      "\t 1 2 2.9000000000000004 0.8100000000000006\n",
      "\t 2 4 5.800000000000001 3.2400000000000024\n",
      "\t 3 6 8.700000000000001 7.290000000000005\n",
      "MSE =  3.780000000000003\n",
      "w = 3.0\n",
      "\t 1 2 3.0 1.0\n",
      "\t 2 4 6.0 4.0\n",
      "\t 3 6 9.0 9.0\n",
      "MSE =  4.666666666666667\n",
      "w = 3.0\n",
      "\t 1 2 3.1 1.2100000000000002\n",
      "\t 2 4 6.2 4.840000000000001\n",
      "\t 3 6 9.3 10.890000000000004\n",
      "MSE =  5.646666666666668\n",
      "w = 3.0\n",
      "\t 1 2 3.2 1.4400000000000004\n",
      "\t 2 4 6.4 5.760000000000002\n",
      "\t 3 6 9.600000000000001 12.96000000000001\n",
      "MSE =  6.720000000000003\n",
      "w = 3.0\n",
      "\t 1 2 3.3000000000000003 1.6900000000000006\n",
      "\t 2 4 6.6000000000000005 6.7600000000000025\n",
      "\t 3 6 9.9 15.210000000000003\n",
      "MSE =  7.886666666666668\n",
      "w = 3.0\n",
      "\t 1 2 3.4000000000000004 1.960000000000001\n",
      "\t 2 4 6.800000000000001 7.840000000000004\n",
      "\t 3 6 10.200000000000001 17.640000000000008\n",
      "MSE =  9.14666666666667\n",
      "w = 4.0\n",
      "\t 1 2 3.5 2.25\n",
      "\t 2 4 7.0 9.0\n",
      "\t 3 6 10.5 20.25\n",
      "MSE =  10.5\n",
      "w = 4.0\n",
      "\t 1 2 3.6 2.5600000000000005\n",
      "\t 2 4 7.2 10.240000000000002\n",
      "\t 3 6 10.8 23.040000000000006\n",
      "MSE =  11.94666666666667\n",
      "w = 4.0\n",
      "\t 1 2 3.7 2.8900000000000006\n",
      "\t 2 4 7.4 11.560000000000002\n",
      "\t 3 6 11.100000000000001 26.010000000000016\n",
      "MSE =  13.486666666666673\n",
      "w = 4.0\n",
      "\t 1 2 3.8000000000000003 3.240000000000001\n",
      "\t 2 4 7.6000000000000005 12.960000000000004\n",
      "\t 3 6 11.4 29.160000000000004\n",
      "MSE =  15.120000000000005\n",
      "w = 4.0\n",
      "\t 1 2 3.9000000000000004 3.610000000000001\n",
      "\t 2 4 7.800000000000001 14.440000000000005\n",
      "\t 3 6 11.700000000000001 32.49000000000001\n",
      "MSE =  16.84666666666667\n",
      "w = 4.0\n",
      "\t 1 2 4.0 4.0\n",
      "\t 2 4 8.0 16.0\n",
      "\t 3 6 12.0 36.0\n",
      "MSE =  18.666666666666668\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_list = []\n",
    "mse_list = []\n",
    "for w in np.arange(0, 4.1, 0.1):\n",
    "    print(\"w =\", w.round())\n",
    "    l_sum = 0\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        y_pred_val = forward(x_val)\n",
    "        l = loss(x_val, y_val)\n",
    "        l_sum += l\n",
    "        print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
    "    \n",
    "    print(\"MSE = \", l_sum / 3)\n",
    "    w_list.append(w)\n",
    "    mse_list.append(l_sum / 3)\n",
    "    \n",
    "plt.plot(w_list, mse_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Lecture 3: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1, 2, 3]\n",
    "y_data = [2, 4, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1.0 # a random guess: random value\n",
    "\n",
    "# model for forward pass\n",
    "def forward(x):\n",
    "    return x * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gradient\n",
    "\n",
    "def gradient(x, y):   #d_loss/d_w\n",
    "    return 2*x*(x*w - y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 4.0\n",
      "\tgrad:  1 2 -2.0\n",
      "\tgrad:  2 4 -7.84\n",
      "\tgrad:  3 6 -16.2288\n",
      "progress: 0 w= 1.260688 loss= 4.919240100095999\n",
      "\tgrad:  1 2 -1.478624\n",
      "\tgrad:  2 4 -5.796206079999999\n",
      "\tgrad:  3 6 -11.998146585599997\n",
      "progress: 1 w= 1.453417766656 loss= 2.688769240265834\n",
      "\tgrad:  1 2 -1.093164466688\n",
      "\tgrad:  2 4 -4.285204709416961\n",
      "\tgrad:  3 6 -8.87037374849311\n",
      "progress: 2 w= 1.5959051959019805 loss= 1.4696334962911515\n",
      "\tgrad:  1 2 -0.8081896081960389\n",
      "\tgrad:  2 4 -3.1681032641284723\n",
      "\tgrad:  3 6 -6.557973756745939\n",
      "progress: 3 w= 1.701247862192685 loss= 0.8032755585999681\n",
      "\tgrad:  1 2 -0.59750427561463\n",
      "\tgrad:  2 4 -2.3422167604093502\n",
      "\tgrad:  3 6 -4.848388694047353\n",
      "progress: 4 w= 1.7791289594933983 loss= 0.43905614881022015\n",
      "\tgrad:  1 2 -0.44174208101320334\n",
      "\tgrad:  2 4 -1.7316289575717576\n",
      "\tgrad:  3 6 -3.584471942173538\n",
      "progress: 5 w= 1.836707389300983 loss= 0.2399802903801062\n",
      "\tgrad:  1 2 -0.3265852213980338\n",
      "\tgrad:  2 4 -1.2802140678802925\n",
      "\tgrad:  3 6 -2.650043120512205\n",
      "progress: 6 w= 1.8792758133988885 loss= 0.1311689630744999\n",
      "\tgrad:  1 2 -0.241448373202223\n",
      "\tgrad:  2 4 -0.946477622952715\n",
      "\tgrad:  3 6 -1.9592086795121197\n",
      "progress: 7 w= 1.910747160155559 loss= 0.07169462478267678\n",
      "\tgrad:  1 2 -0.17850567968888198\n",
      "\tgrad:  2 4 -0.6997422643804168\n",
      "\tgrad:  3 6 -1.4484664872674653\n",
      "progress: 8 w= 1.9340143044689266 loss= 0.03918700813247573\n",
      "\tgrad:  1 2 -0.13197139106214673\n",
      "\tgrad:  2 4 -0.5173278529636143\n",
      "\tgrad:  3 6 -1.0708686556346834\n",
      "progress: 9 w= 1.9512159834655312 loss= 0.021418922423117836\n",
      "\tgrad:  1 2 -0.09756803306893769\n",
      "\tgrad:  2 4 -0.38246668963023644\n",
      "\tgrad:  3 6 -0.7917060475345892\n",
      "progress: 10 w= 1.9639333911678687 loss= 0.01170720245384975\n",
      "\tgrad:  1 2 -0.07213321766426262\n",
      "\tgrad:  2 4 -0.2827622132439096\n",
      "\tgrad:  3 6 -0.5853177814148953\n",
      "progress: 11 w= 1.9733355232910992 loss= 0.006398948863435593\n",
      "\tgrad:  1 2 -0.05332895341780164\n",
      "\tgrad:  2 4 -0.2090494973977819\n",
      "\tgrad:  3 6 -0.4327324596134101\n",
      "progress: 12 w= 1.9802866323953892 loss= 0.003497551760830656\n",
      "\tgrad:  1 2 -0.039426735209221686\n",
      "\tgrad:  2 4 -0.15455280202014876\n",
      "\tgrad:  3 6 -0.3199243001817109\n",
      "progress: 13 w= 1.9854256707695 loss= 0.001911699652671057\n",
      "\tgrad:  1 2 -0.02914865846100012\n",
      "\tgrad:  2 4 -0.11426274116712065\n",
      "\tgrad:  3 6 -0.2365238742159388\n",
      "progress: 14 w= 1.9892250235079405 loss= 0.0010449010656399273\n",
      "\tgrad:  1 2 -0.021549952984118992\n",
      "\tgrad:  2 4 -0.08447581569774698\n",
      "\tgrad:  3 6 -0.17486493849433593\n",
      "progress: 15 w= 1.9920339305797026 loss= 0.0005711243580809696\n",
      "\tgrad:  1 2 -0.015932138840594856\n",
      "\tgrad:  2 4 -0.062453984255132156\n",
      "\tgrad:  3 6 -0.12927974740812687\n",
      "progress: 16 w= 1.994110589284741 loss= 0.0003121664271570621\n",
      "\tgrad:  1 2 -0.011778821430517894\n",
      "\tgrad:  2 4 -0.046172980007630926\n",
      "\tgrad:  3 6 -0.09557806861579543\n",
      "progress: 17 w= 1.9956458879852805 loss= 0.0001706246229305199\n",
      "\tgrad:  1 2 -0.008708224029438938\n",
      "\tgrad:  2 4 -0.03413623819540135\n",
      "\tgrad:  3 6 -0.07066201306448505\n",
      "progress: 18 w= 1.9967809527381737 loss= 9.326038746484765e-05\n",
      "\tgrad:  1 2 -0.006438094523652627\n",
      "\tgrad:  2 4 -0.02523733053271826\n",
      "\tgrad:  3 6 -0.052241274202728505\n",
      "progress: 19 w= 1.9976201197307648 loss= 5.097447086306101e-05\n",
      "\tgrad:  1 2 -0.004759760538470381\n",
      "\tgrad:  2 4 -0.01865826131080439\n",
      "\tgrad:  3 6 -0.03862260091336722\n",
      "progress: 20 w= 1.998240525958391 loss= 2.7861740127856012e-05\n",
      "\tgrad:  1 2 -0.0035189480832178432\n",
      "\tgrad:  2 4 -0.01379427648621423\n",
      "\tgrad:  3 6 -0.028554152326460525\n",
      "progress: 21 w= 1.99869919972735 loss= 1.5228732143933469e-05\n",
      "\tgrad:  1 2 -0.002601600545300009\n",
      "\tgrad:  2 4 -0.01019827413757568\n",
      "\tgrad:  3 6 -0.021110427464781978\n",
      "progress: 22 w= 1.9990383027488265 loss= 8.323754426231206e-06\n",
      "\tgrad:  1 2 -0.001923394502346909\n",
      "\tgrad:  2 4 -0.007539706449199102\n",
      "\tgrad:  3 6 -0.01560719234984198\n",
      "progress: 23 w= 1.9992890056818404 loss= 4.549616284094891e-06\n",
      "\tgrad:  1 2 -0.0014219886363191492\n",
      "\tgrad:  2 4 -0.005574195454370212\n",
      "\tgrad:  3 6 -0.011538584590544687\n",
      "progress: 24 w= 1.999474353368653 loss= 2.486739429417538e-06\n",
      "\tgrad:  1 2 -0.0010512932626940419\n",
      "\tgrad:  2 4 -0.004121069589761106\n",
      "\tgrad:  3 6 -0.008530614050808794\n",
      "progress: 25 w= 1.9996113831376856 loss= 1.3592075910762856e-06\n",
      "\tgrad:  1 2 -0.0007772337246287897\n",
      "\tgrad:  2 4 -0.0030467562005451754\n",
      "\tgrad:  3 6 -0.006306785335127074\n",
      "progress: 26 w= 1.9997126908902887 loss= 7.429187207079447e-07\n",
      "\tgrad:  1 2 -0.0005746182194226179\n",
      "\tgrad:  2 4 -0.002252503420136165\n",
      "\tgrad:  3 6 -0.00466268207967957\n",
      "progress: 27 w= 1.9997875889274812 loss= 4.060661735575354e-07\n",
      "\tgrad:  1 2 -0.0004248221450375844\n",
      "\tgrad:  2 4 -0.0016653028085471533\n",
      "\tgrad:  3 6 -0.0034471768136938863\n",
      "progress: 28 w= 1.9998429619451539 loss= 2.2194855602869353e-07\n",
      "\tgrad:  1 2 -0.00031407610969225175\n",
      "\tgrad:  2 4 -0.0012311783499932005\n",
      "\tgrad:  3 6 -0.0025485391844828342\n",
      "progress: 29 w= 1.9998838998815958 loss= 1.213131374411496e-07\n",
      "\tgrad:  1 2 -0.00023220023680847746\n",
      "\tgrad:  2 4 -0.0009102249282886277\n",
      "\tgrad:  3 6 -0.0018841656015560204\n",
      "progress: 30 w= 1.9999141657892625 loss= 6.630760559646474e-08\n",
      "\tgrad:  1 2 -0.00017166842147497974\n",
      "\tgrad:  2 4 -0.0006729402121816719\n",
      "\tgrad:  3 6 -0.0013929862392156878\n",
      "progress: 31 w= 1.9999365417379913 loss= 3.624255915449335e-08\n",
      "\tgrad:  1 2 -0.0001269165240174175\n",
      "\tgrad:  2 4 -0.0004975127741477792\n",
      "\tgrad:  3 6 -0.0010298514424817995\n",
      "progress: 32 w= 1.9999530845453979 loss= 1.9809538924707548e-08\n",
      "\tgrad:  1 2 -9.383090920422887e-05\n",
      "\tgrad:  2 4 -0.00036781716408107457\n",
      "\tgrad:  3 6 -0.0007613815296476645\n",
      "progress: 33 w= 1.9999653148414271 loss= 1.0827542027017377e-08\n",
      "\tgrad:  1 2 -6.937031714571162e-05\n",
      "\tgrad:  2 4 -0.0002719316432120422\n",
      "\tgrad:  3 6 -0.0005628985014531906\n",
      "progress: 34 w= 1.999974356846045 loss= 5.9181421028034105e-09\n",
      "\tgrad:  1 2 -5.1286307909848006e-05\n",
      "\tgrad:  2 4 -0.00020104232700646207\n",
      "\tgrad:  3 6 -0.0004161576169003922\n",
      "progress: 35 w= 1.9999810417085633 loss= 3.2347513278475087e-09\n",
      "\tgrad:  1 2 -3.7916582873442906e-05\n",
      "\tgrad:  2 4 -0.0001486330048638962\n",
      "\tgrad:  3 6 -0.0003076703200690645\n",
      "progress: 36 w= 1.9999859839076413 loss= 1.7680576050779005e-09\n",
      "\tgrad:  1 2 -2.8032184717474706e-05\n",
      "\tgrad:  2 4 -0.0001098861640933535\n",
      "\tgrad:  3 6 -0.00022746435967313516\n",
      "progress: 37 w= 1.9999896377347262 loss= 9.6638887447731e-10\n",
      "\tgrad:  1 2 -2.0724530547688857e-05\n",
      "\tgrad:  2 4 -8.124015974608767e-05\n",
      "\tgrad:  3 6 -0.00016816713067413502\n",
      "progress: 38 w= 1.999992339052936 loss= 5.282109892545845e-10\n",
      "\tgrad:  1 2 -1.5321894128117464e-05\n",
      "\tgrad:  2 4 -6.006182498197177e-05\n",
      "\tgrad:  3 6 -0.00012432797771566584\n",
      "progress: 39 w= 1.9999943361699042 loss= 2.887107421958329e-10\n",
      "\tgrad:  1 2 -1.1327660191629008e-05\n",
      "\tgrad:  2 4 -4.4404427951505454e-05\n",
      "\tgrad:  3 6 -9.191716585732479e-05\n",
      "progress: 40 w= 1.9999958126624442 loss= 1.5780416225633037e-10\n",
      "\tgrad:  1 2 -8.37467511161094e-06\n",
      "\tgrad:  2 4 -3.282872643772805e-05\n",
      "\tgrad:  3 6 -6.795546372551087e-05\n",
      "progress: 41 w= 1.999996904251097 loss= 8.625295142578772e-11\n",
      "\tgrad:  1 2 -6.191497806007362e-06\n",
      "\tgrad:  2 4 -2.4270671399762023e-05\n",
      "\tgrad:  3 6 -5.0240289795056015e-05\n",
      "progress: 42 w= 1.999997711275687 loss= 4.71443308235547e-11\n",
      "\tgrad:  1 2 -4.5774486259198e-06\n",
      "\tgrad:  2 4 -1.794359861406747e-05\n",
      "\tgrad:  3 6 -3.714324913239864e-05\n",
      "progress: 43 w= 1.9999983079186507 loss= 2.5768253628059826e-11\n",
      "\tgrad:  1 2 -3.3841626985164908e-06\n",
      "\tgrad:  2 4 -1.326591777761621e-05\n",
      "\tgrad:  3 6 -2.7460449796734565e-05\n",
      "progress: 44 w= 1.9999987490239537 loss= 1.4084469615916932e-11\n",
      "\tgrad:  1 2 -2.5019520926150562e-06\n",
      "\tgrad:  2 4 -9.807652203264183e-06\n",
      "\tgrad:  3 6 -2.0301840059744336e-05\n",
      "progress: 45 w= 1.9999990751383971 loss= 7.698320862431846e-12\n",
      "\tgrad:  1 2 -1.8497232057157476e-06\n",
      "\tgrad:  2 4 -7.250914967116273e-06\n",
      "\tgrad:  3 6 -1.5009393983689279e-05\n",
      "progress: 46 w= 1.9999993162387186 loss= 4.20776540913866e-12\n",
      "\tgrad:  1 2 -1.3675225627451937e-06\n",
      "\tgrad:  2 4 -5.3606884460322135e-06\n",
      "\tgrad:  3 6 -1.109662508014253e-05\n",
      "progress: 47 w= 1.9999994944870796 loss= 2.299889814334344e-12\n",
      "\tgrad:  1 2 -1.0110258408246864e-06\n",
      "\tgrad:  2 4 -3.963221296032771e-06\n",
      "\tgrad:  3 6 -8.20386808086937e-06\n",
      "progress: 48 w= 1.9999996262682318 loss= 1.2570789110540446e-12\n",
      "\tgrad:  1 2 -7.474635363990956e-07\n",
      "\tgrad:  2 4 -2.930057062755509e-06\n",
      "\tgrad:  3 6 -6.065218119744031e-06\n",
      "progress: 49 w= 1.999999723695619 loss= 6.870969979249939e-13\n",
      "\tgrad:  1 2 -5.526087618612507e-07\n",
      "\tgrad:  2 4 -2.166226346744793e-06\n",
      "\tgrad:  3 6 -4.484088535150477e-06\n",
      "progress: 50 w= 1.9999997957248556 loss= 3.7555501141274804e-13\n",
      "\tgrad:  1 2 -4.08550288710785e-07\n",
      "\tgrad:  2 4 -1.6015171322436572e-06\n",
      "\tgrad:  3 6 -3.3151404608133817e-06\n",
      "progress: 51 w= 1.9999998489769344 loss= 2.052716967104274e-13\n",
      "\tgrad:  1 2 -3.020461312175371e-07\n",
      "\tgrad:  2 4 -1.1840208351543424e-06\n",
      "\tgrad:  3 6 -2.4509231284497446e-06\n",
      "progress: 52 w= 1.9999998883468353 loss= 1.1219786256679713e-13\n",
      "\tgrad:  1 2 -2.2330632942768602e-07\n",
      "\tgrad:  2 4 -8.753608113920563e-07\n",
      "\tgrad:  3 6 -1.811996877876254e-06\n",
      "progress: 53 w= 1.9999999174534755 loss= 6.132535848018759e-14\n",
      "\tgrad:  1 2 -1.6509304900935717e-07\n",
      "\tgrad:  2 4 -6.471647520100987e-07\n",
      "\tgrad:  3 6 -1.3396310407642886e-06\n",
      "progress: 54 w= 1.999999938972364 loss= 3.351935118167793e-14\n",
      "\tgrad:  1 2 -1.220552721115098e-07\n",
      "\tgrad:  2 4 -4.784566662863199e-07\n",
      "\tgrad:  3 6 -9.904052991061008e-07\n",
      "progress: 55 w= 1.9999999548815364 loss= 1.8321081844499955e-14\n",
      "\tgrad:  1 2 -9.023692726373156e-08\n",
      "\tgrad:  2 4 -3.5372875473171916e-07\n",
      "\tgrad:  3 6 -7.322185204827747e-07\n",
      "progress: 56 w= 1.9999999666433785 loss= 1.0013977760018664e-14\n",
      "\tgrad:  1 2 -6.671324292994996e-08\n",
      "\tgrad:  2 4 -2.615159129248923e-07\n",
      "\tgrad:  3 6 -5.413379398078177e-07\n",
      "progress: 57 w= 1.9999999753390494 loss= 5.473462367088053e-15\n",
      "\tgrad:  1 2 -4.932190122985958e-08\n",
      "\tgrad:  2 4 -1.9334185274999527e-07\n",
      "\tgrad:  3 6 -4.002176350326181e-07\n",
      "progress: 58 w= 1.9999999817678633 loss= 2.991697274308627e-15\n",
      "\tgrad:  1 2 -3.6464273378555845e-08\n",
      "\tgrad:  2 4 -1.429399514307761e-07\n",
      "\tgrad:  3 6 -2.9588569994132286e-07\n",
      "progress: 59 w= 1.9999999865207625 loss= 1.6352086111474931e-15\n",
      "\tgrad:  1 2 -2.6958475007887728e-08\n",
      "\tgrad:  2 4 -1.0567722164012139e-07\n",
      "\tgrad:  3 6 -2.1875184863517916e-07\n",
      "progress: 60 w= 1.999999990034638 loss= 8.937759877335403e-16\n",
      "\tgrad:  1 2 -1.993072418216002e-08\n",
      "\tgrad:  2 4 -7.812843882959442e-08\n",
      "\tgrad:  3 6 -1.617258700292723e-07\n",
      "progress: 61 w= 1.9999999926324883 loss= 4.885220495987371e-16\n",
      "\tgrad:  1 2 -1.473502342363986e-08\n",
      "\tgrad:  2 4 -5.7761292637792394e-08\n",
      "\tgrad:  3 6 -1.195658771990793e-07\n",
      "progress: 62 w= 1.99999999455311 loss= 2.670175009618106e-16\n",
      "\tgrad:  1 2 -1.0893780100218464e-08\n",
      "\tgrad:  2 4 -4.270361841918202e-08\n",
      "\tgrad:  3 6 -8.839649012770678e-08\n",
      "progress: 63 w= 1.9999999959730488 loss= 1.4594702493172377e-16\n",
      "\tgrad:  1 2 -8.05390243385773e-09\n",
      "\tgrad:  2 4 -3.1571296688071016e-08\n",
      "\tgrad:  3 6 -6.53525820126788e-08\n",
      "progress: 64 w= 1.9999999970228268 loss= 7.977204100704301e-17\n",
      "\tgrad:  1 2 -5.9543463493128e-09\n",
      "\tgrad:  2 4 -2.334103754719763e-08\n",
      "\tgrad:  3 6 -4.8315948575350376e-08\n",
      "progress: 65 w= 1.9999999977989402 loss= 4.360197735196887e-17\n",
      "\tgrad:  1 2 -4.402119557767037e-09\n",
      "\tgrad:  2 4 -1.725630838222969e-08\n",
      "\tgrad:  3 6 -3.5720557178819945e-08\n",
      "progress: 66 w= 1.9999999983727301 loss= 2.3832065197304227e-17\n",
      "\tgrad:  1 2 -3.254539748809293e-09\n",
      "\tgrad:  2 4 -1.2757796596929438e-08\n",
      "\tgrad:  3 6 -2.6408640607655798e-08\n",
      "progress: 67 w= 1.9999999987969397 loss= 1.3026183953845832e-17\n",
      "\tgrad:  1 2 -2.406120636067044e-09\n",
      "\tgrad:  2 4 -9.431992964437086e-09\n",
      "\tgrad:  3 6 -1.9524227568012975e-08\n",
      "progress: 68 w= 1.999999999110563 loss= 7.11988308874388e-18\n",
      "\tgrad:  1 2 -1.7788739370416806e-09\n",
      "\tgrad:  2 4 -6.97318647269185e-09\n",
      "\tgrad:  3 6 -1.4434496264925656e-08\n",
      "progress: 69 w= 1.9999999993424284 loss= 3.89160224698574e-18\n",
      "\tgrad:  1 2 -1.3151431055291596e-09\n",
      "\tgrad:  2 4 -5.155360582875801e-09\n",
      "\tgrad:  3 6 -1.067159693945996e-08\n",
      "progress: 70 w= 1.9999999995138495 loss= 2.1270797208746147e-18\n",
      "\tgrad:  1 2 -9.72300906454393e-10\n",
      "\tgrad:  2 4 -3.811418736177075e-09\n",
      "\tgrad:  3 6 -7.88963561149103e-09\n",
      "progress: 71 w= 1.9999999996405833 loss= 1.1626238773828175e-18\n",
      "\tgrad:  1 2 -7.18833437218791e-10\n",
      "\tgrad:  2 4 -2.8178277489132597e-09\n",
      "\tgrad:  3 6 -5.832902161273523e-09\n",
      "progress: 72 w= 1.999999999734279 loss= 6.354692062078993e-19\n",
      "\tgrad:  1 2 -5.314420015167798e-10\n",
      "\tgrad:  2 4 -2.0832526814729135e-09\n",
      "\tgrad:  3 6 -4.31233715403323e-09\n",
      "progress: 73 w= 1.9999999998035491 loss= 3.4733644793346653e-19\n",
      "\tgrad:  1 2 -3.92901711165905e-10\n",
      "\tgrad:  2 4 -1.5401742103904326e-09\n",
      "\tgrad:  3 6 -3.188159070077745e-09\n",
      "progress: 74 w= 1.9999999998547615 loss= 1.8984796531526204e-19\n",
      "\tgrad:  1 2 -2.9047697580608656e-10\n",
      "\tgrad:  2 4 -1.1386696030513122e-09\n",
      "\tgrad:  3 6 -2.3570478902001923e-09\n",
      "progress: 75 w= 1.9999999998926234 loss= 1.0376765851119951e-19\n",
      "\tgrad:  1 2 -2.1475310418850313e-10\n",
      "\tgrad:  2 4 -8.418314934033333e-10\n",
      "\tgrad:  3 6 -1.7425900722400911e-09\n",
      "progress: 76 w= 1.9999999999206153 loss= 5.671751114309842e-20\n",
      "\tgrad:  1 2 -1.5876944203796484e-10\n",
      "\tgrad:  2 4 -6.223768167501476e-10\n",
      "\tgrad:  3 6 -1.2883241140571045e-09\n",
      "progress: 77 w= 1.9999999999413098 loss= 3.100089617511693e-20\n",
      "\tgrad:  1 2 -1.17380327679939e-10\n",
      "\tgrad:  2 4 -4.601314884666863e-10\n",
      "\tgrad:  3 6 -9.524754318590567e-10\n",
      "progress: 78 w= 1.9999999999566096 loss= 1.6944600977692705e-20\n",
      "\tgrad:  1 2 -8.678080476443029e-11\n",
      "\tgrad:  2 4 -3.4018121652934497e-10\n",
      "\tgrad:  3 6 -7.041780492045291e-10\n",
      "progress: 79 w= 1.9999999999679208 loss= 9.2616919156479e-21\n",
      "\tgrad:  1 2 -6.415845632545825e-11\n",
      "\tgrad:  2 4 -2.5150193039280566e-10\n",
      "\tgrad:  3 6 -5.206075570640678e-10\n",
      "progress: 80 w= 1.9999999999762834 loss= 5.062350511130293e-21\n",
      "\tgrad:  1 2 -4.743316850408519e-11\n",
      "\tgrad:  2 4 -1.8593837580738182e-10\n",
      "\tgrad:  3 6 -3.8489211817704927e-10\n",
      "progress: 81 w= 1.999999999982466 loss= 2.7669155644059242e-21\n",
      "\tgrad:  1 2 -3.5067948545020045e-11\n",
      "\tgrad:  2 4 -1.3746692673066718e-10\n",
      "\tgrad:  3 6 -2.845563784603655e-10\n",
      "progress: 82 w= 1.9999999999870368 loss= 1.5124150106147723e-21\n",
      "\tgrad:  1 2 -2.5926372160256506e-11\n",
      "\tgrad:  2 4 -1.0163070385260653e-10\n",
      "\tgrad:  3 6 -2.1037571684701106e-10\n",
      "progress: 83 w= 1.999999999990416 loss= 8.26683933105326e-22\n",
      "\tgrad:  1 2 -1.9167778475548403e-11\n",
      "\tgrad:  2 4 -7.51381179497912e-11\n",
      "\tgrad:  3 6 -1.5553425214420713e-10\n",
      "progress: 84 w= 1.9999999999929146 loss= 4.518126871054872e-22\n",
      "\tgrad:  1 2 -1.4170886686315498e-11\n",
      "\tgrad:  2 4 -5.555023108172463e-11\n",
      "\tgrad:  3 6 -1.1499068364173581e-10\n",
      "progress: 85 w= 1.9999999999947617 loss= 2.469467919185614e-22\n",
      "\tgrad:  1 2 -1.0476508549572827e-11\n",
      "\tgrad:  2 4 -4.106759377009439e-11\n",
      "\tgrad:  3 6 -8.500933290633839e-11\n",
      "progress: 86 w= 1.9999999999961273 loss= 1.349840097651456e-22\n",
      "\tgrad:  1 2 -7.745359908994942e-12\n",
      "\tgrad:  2 4 -3.036149109902908e-11\n",
      "\tgrad:  3 6 -6.285105769165966e-11\n",
      "progress: 87 w= 1.999999999997137 loss= 7.376551550022107e-23\n",
      "\tgrad:  1 2 -5.726086271806707e-12\n",
      "\tgrad:  2 4 -2.2446045022661565e-11\n",
      "\tgrad:  3 6 -4.646416584819235e-11\n",
      "progress: 88 w= 1.9999999999978835 loss= 4.031726170507742e-23\n",
      "\tgrad:  1 2 -4.233058348290797e-12\n",
      "\tgrad:  2 4 -1.659294923683774e-11\n",
      "\tgrad:  3 6 -3.4351188560322043e-11\n",
      "progress: 89 w= 1.9999999999984353 loss= 2.2033851437431755e-23\n",
      "\tgrad:  1 2 -3.1294966618133913e-12\n",
      "\tgrad:  2 4 -1.226752033289813e-11\n",
      "\tgrad:  3 6 -2.539835008974478e-11\n",
      "progress: 90 w= 1.9999999999988431 loss= 1.2047849775995315e-23\n",
      "\tgrad:  1 2 -2.3137047833188262e-12\n",
      "\tgrad:  2 4 -9.070078021977679e-12\n",
      "\tgrad:  3 6 -1.8779644506139448e-11\n",
      "progress: 91 w= 1.9999999999991447 loss= 6.5840863393251405e-24\n",
      "\tgrad:  1 2 -1.7106316363424412e-12\n",
      "\tgrad:  2 4 -6.7057470687359455e-12\n",
      "\tgrad:  3 6 -1.3882228699912957e-11\n",
      "progress: 92 w= 1.9999999999993676 loss= 3.5991747246272455e-24\n",
      "\tgrad:  1 2 -1.2647660696529783e-12\n",
      "\tgrad:  2 4 -4.957811938766099e-12\n",
      "\tgrad:  3 6 -1.0263789818054647e-11\n",
      "progress: 93 w= 1.9999999999995324 loss= 1.969312363793734e-24\n",
      "\tgrad:  1 2 -9.352518759442319e-13\n",
      "\tgrad:  2 4 -3.666400516522117e-12\n",
      "\tgrad:  3 6 -7.58859641791787e-12\n",
      "progress: 94 w= 1.9999999999996543 loss= 1.0761829795642296e-24\n",
      "\tgrad:  1 2 -6.914468997365475e-13\n",
      "\tgrad:  2 4 -2.7107205369247822e-12\n",
      "\tgrad:  3 6 -5.611511255665391e-12\n",
      "progress: 95 w= 1.9999999999997444 loss= 5.875191475205477e-25\n",
      "\tgrad:  1 2 -5.111466805374221e-13\n",
      "\tgrad:  2 4 -2.0037305148434825e-12\n",
      "\tgrad:  3 6 -4.1460168631601846e-12\n",
      "progress: 96 w= 1.999999999999811 loss= 3.2110109830478153e-25\n",
      "\tgrad:  1 2 -3.779199175824033e-13\n",
      "\tgrad:  2 4 -1.4814816040598089e-12\n",
      "\tgrad:  3 6 -3.064215547965432e-12\n",
      "progress: 97 w= 1.9999999999998603 loss= 1.757455879087579e-25\n",
      "\tgrad:  1 2 -2.793321129956894e-13\n",
      "\tgrad:  2 4 -1.0942358130705543e-12\n",
      "\tgrad:  3 6 -2.2648549702353193e-12\n",
      "progress: 98 w= 1.9999999999998967 loss= 9.608404711682446e-26\n",
      "\tgrad:  1 2 -2.0650148258027912e-13\n",
      "\tgrad:  2 4 -8.100187187665142e-13\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 99 w= 1.9999999999999236 loss= 5.250973729513143e-26\n",
      "predict (after training) 4 hours 7.9999999999996945\n"
     ]
    }
   ],
   "source": [
    "# Before training\n",
    "print(\"predict (before training)\", 4, forward(4))\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        grad = gradient(x_val, y_val)\n",
    "        w = w - 0.01 * grad   # learning rate x gradient\n",
    "        print(\"\\tgrad: \", x_val, y_val, grad)\n",
    "        l = loss(x_val, y_val)\n",
    "        \n",
    "    print(\"progress:\", epoch, \"w=\", w, \"loss=\", l)\n",
    "    \n",
    "\n",
    "# After training\n",
    "\n",
    "print(\"predict (after training)\", \"4 hours\", forward(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3-1: Compute gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src= \"excercise_3.png\" style=width:600px;height:400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In pictoral form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"pictoral_form.jpeg\" style=width:950px;height:400px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change in loss wrt change in w1 = x\n",
    "#change in loss wrt change in w2 = x^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3-2: Implement gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 21.0\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 0 w= 1.9999999999999236 loss= 48.79860736000002\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 1 w= 1.9999999999999236 loss= 48.59762943999999\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 2 w= 1.9999999999999236 loss= 48.397066240000015\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 3 w= 1.9999999999999236 loss= 48.19691776000004\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 4 w= 1.9999999999999236 loss= 47.99718400000003\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 5 w= 1.9999999999999236 loss= 47.797864960000034\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 6 w= 1.9999999999999236 loss= 47.59896064000006\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 7 w= 1.9999999999999236 loss= 47.40047104000008\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 8 w= 1.9999999999999236 loss= 47.20239616000008\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 9 w= 1.9999999999999236 loss= 47.00473600000007\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 10 w= 1.9999999999999236 loss= 46.80749056000009\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 11 w= 1.9999999999999236 loss= 46.61065984000009\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 12 w= 1.9999999999999236 loss= 46.41424384000011\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 13 w= 1.9999999999999236 loss= 46.21824256000011\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 14 w= 1.9999999999999236 loss= 46.022656000000104\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 15 w= 1.9999999999999236 loss= 45.827484160000125\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 16 w= 1.9999999999999236 loss= 45.63272704000015\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 17 w= 1.9999999999999236 loss= 45.438384640000145\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 18 w= 1.9999999999999236 loss= 45.24445696000014\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 19 w= 1.9999999999999236 loss= 45.050944000000165\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 20 w= 1.9999999999999236 loss= 44.85784576000016\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 21 w= 1.9999999999999236 loss= 44.66516224000018\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 22 w= 1.9999999999999236 loss= 44.47289344000018\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 23 w= 1.9999999999999236 loss= 44.28103936000018\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 24 w= 1.9999999999999236 loss= 44.089600000000196\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 25 w= 1.9999999999999236 loss= 43.898575360000216\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 26 w= 1.9999999999999236 loss= 43.707965440000216\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 27 w= 1.9999999999999236 loss= 43.51777024000021\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 28 w= 1.9999999999999236 loss= 43.32798976000023\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 29 w= 1.9999999999999236 loss= 43.13862400000025\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 30 w= 1.9999999999999236 loss= 42.94967296000023\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 31 w= 1.9999999999999236 loss= 42.761136640000245\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 32 w= 1.9999999999999236 loss= 42.573015040000264\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 33 w= 1.9999999999999236 loss= 42.38530816000024\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 34 w= 1.9999999999999236 loss= 42.19801600000026\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 35 w= 1.9999999999999236 loss= 42.011138560000276\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 36 w= 1.9999999999999236 loss= 41.824675840000275\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 37 w= 1.9999999999999236 loss= 41.638627840000275\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 38 w= 1.9999999999999236 loss= 41.45299456000029\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 39 w= 1.9999999999999236 loss= 41.26777600000031\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 40 w= 1.9999999999999236 loss= 41.08297216000031\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 41 w= 1.9999999999999236 loss= 40.898583040000304\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 42 w= 1.9999999999999236 loss= 40.71460864000033\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 43 w= 1.9999999999999236 loss= 40.53104896000033\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 44 w= 1.9999999999999236 loss= 40.34790400000034\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 45 w= 1.9999999999999236 loss= 40.165173760000336\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 46 w= 1.9999999999999236 loss= 39.98285824000033\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 47 w= 1.9999999999999236 loss= 39.80095744000035\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 48 w= 1.9999999999999236 loss= 39.619471360000375\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 49 w= 1.9999999999999236 loss= 39.43840000000037\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 50 w= 1.9999999999999236 loss= 39.25774336000037\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 51 w= 1.9999999999999236 loss= 39.07750144000039\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 52 w= 1.9999999999999236 loss= 38.897674240000384\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 53 w= 1.9999999999999236 loss= 38.7182617600004\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 54 w= 1.9999999999999236 loss= 38.5392640000004\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 55 w= 1.9999999999999236 loss= 38.360680960000394\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 56 w= 1.9999999999999236 loss= 38.18251264000042\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 57 w= 1.9999999999999236 loss= 38.00475904000044\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 58 w= 1.9999999999999236 loss= 37.82742016000043\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 59 w= 1.9999999999999236 loss= 37.65049600000042\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 60 w= 1.9999999999999236 loss= 37.47398656000045\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 61 w= 1.9999999999999236 loss= 37.29789184000046\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 62 w= 1.9999999999999236 loss= 37.12221184000044\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 63 w= 1.9999999999999236 loss= 36.946946560000455\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 64 w= 1.9999999999999236 loss= 36.772096000000474\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 65 w= 1.9999999999999236 loss= 36.59766016000045\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 66 w= 1.9999999999999236 loss= 36.42363904000047\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 67 w= 1.9999999999999236 loss= 36.250032640000484\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 68 w= 1.9999999999999236 loss= 36.07684096000048\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 69 w= 1.9999999999999236 loss= 35.90406400000048\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 70 w= 1.9999999999999236 loss= 35.7317017600005\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 71 w= 1.9999999999999236 loss= 35.559754240000515\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 72 w= 1.9999999999999236 loss= 35.388221440000514\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 73 w= 1.9999999999999236 loss= 35.21710336000051\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 74 w= 1.9999999999999236 loss= 35.046400000000524\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 75 w= 1.9999999999999236 loss= 34.87611136000052\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 76 w= 1.9999999999999236 loss= 34.706237440000535\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 77 w= 1.9999999999999236 loss= 34.53677824000054\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 78 w= 1.9999999999999236 loss= 34.36773376000053\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 79 w= 1.9999999999999236 loss= 34.199104000000546\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 80 w= 1.9999999999999236 loss= 34.030888960000546\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 81 w= 1.9999999999999236 loss= 33.86308864000056\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 82 w= 1.9999999999999236 loss= 33.69570304000056\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 83 w= 1.9999999999999236 loss= 33.52873216000057\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 84 w= 1.9999999999999236 loss= 33.362176000000574\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 85 w= 1.9999999999999236 loss= 33.19603456000059\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 86 w= 1.9999999999999236 loss= 33.03030784000059\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 87 w= 1.9999999999999236 loss= 32.86499584000058\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 88 w= 1.9999999999999236 loss= 32.7000985600006\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 89 w= 1.9999999999999236 loss= 32.535616000000616\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 90 w= 1.9999999999999236 loss= 32.371548160000614\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 91 w= 1.9999999999999236 loss= 32.20789504000061\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 92 w= 1.9999999999999236 loss= 32.044656640000625\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 93 w= 1.9999999999999236 loss= 31.88183296000062\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 94 w= 1.9999999999999236 loss= 31.719424000000636\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 95 w= 1.9999999999999236 loss= 31.557429760000634\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 96 w= 1.9999999999999236 loss= 31.39585024000063\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 97 w= 1.9999999999999236 loss= 31.234685440000646\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 98 w= 1.9999999999999236 loss= 31.073935360000643\n",
      "\tgrad:  1 2 -1.6786572132332367e-12\n",
      "\tgrad:  2 4 -1.6786572132332367e-12\n",
      "\tgrad:  3 6 -1.6786572132332367e-12\n",
      "progress: 99 w= 1.9999999999999236 loss= 30.913600000000656\n",
      "predict (after training) 4 hours 18.520000000000095\n"
     ]
    }
   ],
   "source": [
    "x_data = [1, 2, 3]\n",
    "y_data = [2, 4, 6]\n",
    "\n",
    "\n",
    "w1 = 1.0 # a random guess: random value\n",
    "w2 = 1.0\n",
    "b = 1\n",
    "\n",
    "# model for forward pass\n",
    "def forward(x):\n",
    "    return (x**2)* w2 + (x*w1) + b\n",
    "\n",
    "# Loss function\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "\n",
    "# compute gradient\n",
    "\n",
    "def gradient_w1(x, y):   #d_loss/d_w1\n",
    "    return x\n",
    "\n",
    "def gradient_w2(x, y):   #d_loss/d_w2\n",
    "    return x**2\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\", 4, forward(4))\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        grad_1 = gradient_w1(x_val, y_val)\n",
    "        grad_2 = gradient_w2(x_val, y_val)\n",
    "        w1 = w1 - 0.0001 * grad_1   # learning rate x gradient\n",
    "        w2 = w2 - 0.0001 * grad_2   # learning rate x gradient\n",
    "        print(\"\\tgrad: \", x_val, y_val, grad)\n",
    "        l = loss(x_val, y_val)\n",
    "        \n",
    "    print(\"progress:\", epoch, \"w=\", w, \"loss=\", l)\n",
    "    \n",
    "\n",
    "# After training\n",
    "\n",
    "print(\"predict (after training)\", \"4 hours\", forward(4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"exercise_41.jpg\" style=width:600px;height:400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Exercise 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"exercise_41_sol.jpeg\" style=width:600px;height:400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"exercise_42.jpg\" style=width:600px;height:400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Exercise 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"Exercise_42_sol.jpeg\" style=width:600px;height:400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 tensor(4.)\n",
      "\tgrad:  1.0 2.0 tensor(-2.)\n",
      "\tgrad:  2.0 4.0 tensor(-7.8400)\n",
      "\tgrad:  3.0 6.0 tensor(-16.2288)\n",
      "progress: 0 w= tensor([1.2607], requires_grad=True) loss= tensor([7.3159], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-1.4786)\n",
      "\tgrad:  2.0 4.0 tensor(-5.7962)\n",
      "\tgrad:  3.0 6.0 tensor(-11.9981)\n",
      "progress: 1 w= tensor([1.4534], requires_grad=True) loss= tensor([3.9988], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-1.0932)\n",
      "\tgrad:  2.0 4.0 tensor(-4.2852)\n",
      "\tgrad:  3.0 6.0 tensor(-8.8704)\n",
      "progress: 2 w= tensor([1.5959], requires_grad=True) loss= tensor([2.1857], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.8082)\n",
      "\tgrad:  2.0 4.0 tensor(-3.1681)\n",
      "\tgrad:  3.0 6.0 tensor(-6.5580)\n",
      "progress: 3 w= tensor([1.7012], requires_grad=True) loss= tensor([1.1946], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.5975)\n",
      "\tgrad:  2.0 4.0 tensor(-2.3422)\n",
      "\tgrad:  3.0 6.0 tensor(-4.8484)\n",
      "progress: 4 w= tensor([1.7791], requires_grad=True) loss= tensor([0.6530], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4417)\n",
      "\tgrad:  2.0 4.0 tensor(-1.7316)\n",
      "\tgrad:  3.0 6.0 tensor(-3.5845)\n",
      "progress: 5 w= tensor([1.8367], requires_grad=True) loss= tensor([0.3569], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3266)\n",
      "\tgrad:  2.0 4.0 tensor(-1.2802)\n",
      "\tgrad:  3.0 6.0 tensor(-2.6500)\n",
      "progress: 6 w= tensor([1.8793], requires_grad=True) loss= tensor([0.1951], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.2414)\n",
      "\tgrad:  2.0 4.0 tensor(-0.9465)\n",
      "\tgrad:  3.0 6.0 tensor(-1.9592)\n",
      "progress: 7 w= tensor([1.9107], requires_grad=True) loss= tensor([0.1066], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.1785)\n",
      "\tgrad:  2.0 4.0 tensor(-0.6997)\n",
      "\tgrad:  3.0 6.0 tensor(-1.4485)\n",
      "progress: 8 w= tensor([1.9340], requires_grad=True) loss= tensor([0.0583], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.1320)\n",
      "\tgrad:  2.0 4.0 tensor(-0.5173)\n",
      "\tgrad:  3.0 6.0 tensor(-1.0709)\n",
      "progress: 9 w= tensor([1.9512], requires_grad=True) loss= tensor([0.0319], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0976)\n",
      "\tgrad:  2.0 4.0 tensor(-0.3825)\n",
      "\tgrad:  3.0 6.0 tensor(-0.7917)\n",
      "progress: 10 w= tensor([1.9639], requires_grad=True) loss= tensor([0.0174], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0721)\n",
      "\tgrad:  2.0 4.0 tensor(-0.2828)\n",
      "\tgrad:  3.0 6.0 tensor(-0.5853)\n",
      "progress: 11 w= tensor([1.9733], requires_grad=True) loss= tensor([0.0095], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0533)\n",
      "\tgrad:  2.0 4.0 tensor(-0.2090)\n",
      "\tgrad:  3.0 6.0 tensor(-0.4327)\n",
      "progress: 12 w= tensor([1.9803], requires_grad=True) loss= tensor([0.0052], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0394)\n",
      "\tgrad:  2.0 4.0 tensor(-0.1546)\n",
      "\tgrad:  3.0 6.0 tensor(-0.3199)\n",
      "progress: 13 w= tensor([1.9854], requires_grad=True) loss= tensor([0.0028], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0291)\n",
      "\tgrad:  2.0 4.0 tensor(-0.1143)\n",
      "\tgrad:  3.0 6.0 tensor(-0.2365)\n",
      "progress: 14 w= tensor([1.9892], requires_grad=True) loss= tensor([0.0016], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0215)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0845)\n",
      "\tgrad:  3.0 6.0 tensor(-0.1749)\n",
      "progress: 15 w= tensor([1.9920], requires_grad=True) loss= tensor([0.0008], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0159)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0625)\n",
      "\tgrad:  3.0 6.0 tensor(-0.1293)\n",
      "progress: 16 w= tensor([1.9941], requires_grad=True) loss= tensor([0.0005], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0118)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0462)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0956)\n",
      "progress: 17 w= tensor([1.9956], requires_grad=True) loss= tensor([0.0003], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0087)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0341)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0707)\n",
      "progress: 18 w= tensor([1.9968], requires_grad=True) loss= tensor([0.0001], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0064)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0252)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0522)\n",
      "progress: 19 w= tensor([1.9976], requires_grad=True) loss= tensor([7.5804e-05], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0048)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0187)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0386)\n",
      "progress: 20 w= tensor([1.9982], requires_grad=True) loss= tensor([4.1433e-05], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0035)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0138)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0286)\n",
      "progress: 21 w= tensor([1.9987], requires_grad=True) loss= tensor([2.2647e-05], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0026)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0102)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0211)\n",
      "progress: 22 w= tensor([1.9990], requires_grad=True) loss= tensor([1.2377e-05], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0019)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0075)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0156)\n",
      "progress: 23 w= tensor([1.9993], requires_grad=True) loss= tensor([6.7684e-06], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0014)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0056)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0115)\n",
      "progress: 24 w= tensor([1.9995], requires_grad=True) loss= tensor([3.7001e-06], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0011)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0041)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0085)\n",
      "progress: 25 w= tensor([1.9996], requires_grad=True) loss= tensor([2.0219e-06], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0008)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0030)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0063)\n",
      "progress: 26 w= tensor([1.9997], requires_grad=True) loss= tensor([1.1045e-06], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0006)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0023)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0047)\n",
      "progress: 27 w= tensor([1.9998], requires_grad=True) loss= tensor([6.0411e-07], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0004)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0017)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0034)\n",
      "progress: 28 w= tensor([1.9998], requires_grad=True) loss= tensor([3.2960e-07], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0003)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0012)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0025)\n",
      "progress: 29 w= tensor([1.9999], requires_grad=True) loss= tensor([1.8051e-07], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0002)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0009)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0019)\n",
      "progress: 30 w= tensor([1.9999], requires_grad=True) loss= tensor([9.8744e-08], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0002)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0007)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0014)\n",
      "progress: 31 w= tensor([1.9999], requires_grad=True) loss= tensor([5.4148e-08], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0001)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0005)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0010)\n",
      "progress: 32 w= tensor([2.0000], requires_grad=True) loss= tensor([2.9468e-08], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-9.3937e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0004)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0008)\n",
      "progress: 33 w= tensor([2.0000], requires_grad=True) loss= tensor([1.6088e-08], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-6.9380e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0003)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0006)\n",
      "progress: 34 w= tensor([2.0000], requires_grad=True) loss= tensor([8.7348e-09], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-5.1260e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0002)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0004)\n",
      "progress: 35 w= tensor([2.0000], requires_grad=True) loss= tensor([4.8467e-09], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-3.7909e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0001)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0003)\n",
      "progress: 36 w= tensor([2.0000], requires_grad=True) loss= tensor([2.6521e-09], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-2.8133e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0001)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0002)\n",
      "progress: 37 w= tensor([2.0000], requires_grad=True) loss= tensor([1.4552e-09], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-2.0981e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-8.2016e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0002)\n",
      "progress: 38 w= tensor([2.0000], requires_grad=True) loss= tensor([7.9149e-10], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-1.5497e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-6.1035e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0001)\n",
      "progress: 39 w= tensor([2.0000], requires_grad=True) loss= tensor([4.4020e-10], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-1.1444e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-4.4823e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-9.1553e-05)\n",
      "progress: 40 w= tensor([2.0000], requires_grad=True) loss= tensor([2.3283e-10], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-8.3447e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-3.2425e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-6.5804e-05)\n",
      "progress: 41 w= tensor([2.0000], requires_grad=True) loss= tensor([1.2028e-10], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-5.9605e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-2.2888e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-4.5776e-05)\n",
      "progress: 42 w= tensor([2.0000], requires_grad=True) loss= tensor([5.8208e-11], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-4.2915e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-1.7166e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-3.7193e-05)\n",
      "progress: 43 w= tensor([2.0000], requires_grad=True) loss= tensor([3.8426e-11], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-3.3379e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-1.3351e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-2.8610e-05)\n",
      "progress: 44 w= tensor([2.0000], requires_grad=True) loss= tensor([2.2737e-11], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-2.6226e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-1.0490e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-2.2888e-05)\n",
      "progress: 45 w= tensor([2.0000], requires_grad=True) loss= tensor([1.4552e-11], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-1.9073e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-7.6294e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-1.4305e-05)\n",
      "progress: 46 w= tensor([2.0000], requires_grad=True) loss= tensor([5.6843e-12], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-1.4305e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-5.7220e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-1.1444e-05)\n",
      "progress: 47 w= tensor([2.0000], requires_grad=True) loss= tensor([3.6380e-12], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-1.1921e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-4.7684e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-1.1444e-05)\n",
      "progress: 48 w= tensor([2.0000], requires_grad=True) loss= tensor([3.6380e-12], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-9.5367e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-3.8147e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-8.5831e-06)\n",
      "progress: 49 w= tensor([2.0000], requires_grad=True) loss= tensor([2.0464e-12], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 50 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 51 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 52 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 53 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 54 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 55 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 56 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 57 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 58 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 59 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 60 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 61 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 62 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 63 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 64 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 65 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 66 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 67 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 68 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 69 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 70 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 71 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 72 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 73 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 74 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 75 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 76 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 77 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 78 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 79 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 80 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 81 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 82 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 83 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 84 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 85 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 86 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 87 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 88 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 89 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 90 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 91 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 92 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 93 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 94 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 95 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 96 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 97 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 98 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 99 w= tensor([2.0000], requires_grad=True) loss= tensor([9.0949e-13], grad_fn=<MulBackward0>)\n",
      "predict (after training) 4 hours tensor([8.0000], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:145: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554788289/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    }
   ],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = torch.tensor([1.0], requires_grad = True) # any random value\n",
    "\n",
    "\n",
    "# In PyTorch torch.Tensor is the main tensor class. So all tensors are just instances of torch.Tensor.\n",
    "# When you call torch.Tensor() you will get an empty tensor without any data.\n",
    "# In contrast torch.tensor is a function which returns a tensor.\n",
    "\n",
    "x = torch.randn(1, 10)\n",
    "\n",
    "# model for forward pass\n",
    "def forward(x):\n",
    "    return x*w\n",
    "\n",
    "# Loss function\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\", 4, forward(4).data[0])\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, y_val)\n",
    "        l.backward()\n",
    "        print(\"\\tgrad: \", x_val, y_val, w.grad.data[0])\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "        \n",
    "        w.grad.data.zero_()\n",
    "        \n",
    "    print(\"progress:\", epoch, \"w=\", w, \"loss=\", l)\n",
    "    \n",
    "\n",
    "# After training\n",
    "\n",
    "print(\"predict (after training)\", \"4 hours\", forward(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2222,  0.8151,  0.4093, -1.3671,  1.5453, -0.5290,  1.3133,  0.4395,\n",
       "         -0.3063,  2.9765]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"w_data.jpg\" style=width:600px;height:400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4-3: Implement computational graph and backprop using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 4.0\n",
      "\tgrad:  1.0 2.0 -2.0\n",
      "\tgrad:  2.0 4.0 -7.84\n",
      "\tgrad:  3.0 6.0 -16.2288\n",
      "progress: 0 w= 1.260688 loss= 4.919240100095999\n",
      "\tgrad:  1.0 2.0 -1.478624\n",
      "\tgrad:  2.0 4.0 -5.796206079999999\n",
      "\tgrad:  3.0 6.0 -11.998146585599997\n",
      "progress: 1 w= 1.453417766656 loss= 2.688769240265834\n",
      "\tgrad:  1.0 2.0 -1.093164466688\n",
      "\tgrad:  2.0 4.0 -4.285204709416961\n",
      "\tgrad:  3.0 6.0 -8.87037374849311\n",
      "progress: 2 w= 1.5959051959019805 loss= 1.4696334962911515\n",
      "\tgrad:  1.0 2.0 -0.8081896081960389\n",
      "\tgrad:  2.0 4.0 -3.1681032641284723\n",
      "\tgrad:  3.0 6.0 -6.557973756745939\n",
      "progress: 3 w= 1.701247862192685 loss= 0.8032755585999681\n",
      "\tgrad:  1.0 2.0 -0.59750427561463\n",
      "\tgrad:  2.0 4.0 -2.3422167604093502\n",
      "\tgrad:  3.0 6.0 -4.848388694047353\n",
      "progress: 4 w= 1.7791289594933983 loss= 0.43905614881022015\n",
      "\tgrad:  1.0 2.0 -0.44174208101320334\n",
      "\tgrad:  2.0 4.0 -1.7316289575717576\n",
      "\tgrad:  3.0 6.0 -3.584471942173538\n",
      "progress: 5 w= 1.836707389300983 loss= 0.2399802903801062\n",
      "\tgrad:  1.0 2.0 -0.3265852213980338\n",
      "\tgrad:  2.0 4.0 -1.2802140678802925\n",
      "\tgrad:  3.0 6.0 -2.650043120512205\n",
      "progress: 6 w= 1.8792758133988885 loss= 0.1311689630744999\n",
      "\tgrad:  1.0 2.0 -0.241448373202223\n",
      "\tgrad:  2.0 4.0 -0.946477622952715\n",
      "\tgrad:  3.0 6.0 -1.9592086795121197\n",
      "progress: 7 w= 1.910747160155559 loss= 0.07169462478267678\n",
      "\tgrad:  1.0 2.0 -0.17850567968888198\n",
      "\tgrad:  2.0 4.0 -0.6997422643804168\n",
      "\tgrad:  3.0 6.0 -1.4484664872674653\n",
      "progress: 8 w= 1.9340143044689266 loss= 0.03918700813247573\n",
      "\tgrad:  1.0 2.0 -0.13197139106214673\n",
      "\tgrad:  2.0 4.0 -0.5173278529636143\n",
      "\tgrad:  3.0 6.0 -1.0708686556346834\n",
      "progress: 9 w= 1.9512159834655312 loss= 0.021418922423117836\n",
      "\tgrad:  1.0 2.0 -0.09756803306893769\n",
      "\tgrad:  2.0 4.0 -0.38246668963023644\n",
      "\tgrad:  3.0 6.0 -0.7917060475345892\n",
      "progress: 10 w= 1.9639333911678687 loss= 0.01170720245384975\n",
      "\tgrad:  1.0 2.0 -0.07213321766426262\n",
      "\tgrad:  2.0 4.0 -0.2827622132439096\n",
      "\tgrad:  3.0 6.0 -0.5853177814148953\n",
      "progress: 11 w= 1.9733355232910992 loss= 0.006398948863435593\n",
      "\tgrad:  1.0 2.0 -0.05332895341780164\n",
      "\tgrad:  2.0 4.0 -0.2090494973977819\n",
      "\tgrad:  3.0 6.0 -0.4327324596134101\n",
      "progress: 12 w= 1.9802866323953892 loss= 0.003497551760830656\n",
      "\tgrad:  1.0 2.0 -0.039426735209221686\n",
      "\tgrad:  2.0 4.0 -0.15455280202014876\n",
      "\tgrad:  3.0 6.0 -0.3199243001817109\n",
      "progress: 13 w= 1.9854256707695 loss= 0.001911699652671057\n",
      "\tgrad:  1.0 2.0 -0.02914865846100012\n",
      "\tgrad:  2.0 4.0 -0.11426274116712065\n",
      "\tgrad:  3.0 6.0 -0.2365238742159388\n",
      "progress: 14 w= 1.9892250235079405 loss= 0.0010449010656399273\n",
      "\tgrad:  1.0 2.0 -0.021549952984118992\n",
      "\tgrad:  2.0 4.0 -0.08447581569774698\n",
      "\tgrad:  3.0 6.0 -0.17486493849433593\n",
      "progress: 15 w= 1.9920339305797026 loss= 0.0005711243580809696\n",
      "\tgrad:  1.0 2.0 -0.015932138840594856\n",
      "\tgrad:  2.0 4.0 -0.062453984255132156\n",
      "\tgrad:  3.0 6.0 -0.12927974740812687\n",
      "progress: 16 w= 1.994110589284741 loss= 0.0003121664271570621\n",
      "\tgrad:  1.0 2.0 -0.011778821430517894\n",
      "\tgrad:  2.0 4.0 -0.046172980007630926\n",
      "\tgrad:  3.0 6.0 -0.09557806861579543\n",
      "progress: 17 w= 1.9956458879852805 loss= 0.0001706246229305199\n",
      "\tgrad:  1.0 2.0 -0.008708224029438938\n",
      "\tgrad:  2.0 4.0 -0.03413623819540135\n",
      "\tgrad:  3.0 6.0 -0.07066201306448505\n",
      "progress: 18 w= 1.9967809527381737 loss= 9.326038746484765e-05\n",
      "\tgrad:  1.0 2.0 -0.006438094523652627\n",
      "\tgrad:  2.0 4.0 -0.02523733053271826\n",
      "\tgrad:  3.0 6.0 -0.052241274202728505\n",
      "progress: 19 w= 1.9976201197307648 loss= 5.097447086306101e-05\n",
      "\tgrad:  1.0 2.0 -0.004759760538470381\n",
      "\tgrad:  2.0 4.0 -0.01865826131080439\n",
      "\tgrad:  3.0 6.0 -0.03862260091336722\n",
      "progress: 20 w= 1.998240525958391 loss= 2.7861740127856012e-05\n",
      "\tgrad:  1.0 2.0 -0.0035189480832178432\n",
      "\tgrad:  2.0 4.0 -0.01379427648621423\n",
      "\tgrad:  3.0 6.0 -0.028554152326460525\n",
      "progress: 21 w= 1.99869919972735 loss= 1.5228732143933469e-05\n",
      "\tgrad:  1.0 2.0 -0.002601600545300009\n",
      "\tgrad:  2.0 4.0 -0.01019827413757568\n",
      "\tgrad:  3.0 6.0 -0.021110427464781978\n",
      "progress: 22 w= 1.9990383027488265 loss= 8.323754426231206e-06\n",
      "\tgrad:  1.0 2.0 -0.001923394502346909\n",
      "\tgrad:  2.0 4.0 -0.007539706449199102\n",
      "\tgrad:  3.0 6.0 -0.01560719234984198\n",
      "progress: 23 w= 1.9992890056818404 loss= 4.549616284094891e-06\n",
      "\tgrad:  1.0 2.0 -0.0014219886363191492\n",
      "\tgrad:  2.0 4.0 -0.005574195454370212\n",
      "\tgrad:  3.0 6.0 -0.011538584590544687\n",
      "progress: 24 w= 1.999474353368653 loss= 2.486739429417538e-06\n",
      "\tgrad:  1.0 2.0 -0.0010512932626940419\n",
      "\tgrad:  2.0 4.0 -0.004121069589761106\n",
      "\tgrad:  3.0 6.0 -0.008530614050808794\n",
      "progress: 25 w= 1.9996113831376856 loss= 1.3592075910762856e-06\n",
      "\tgrad:  1.0 2.0 -0.0007772337246287897\n",
      "\tgrad:  2.0 4.0 -0.0030467562005451754\n",
      "\tgrad:  3.0 6.0 -0.006306785335127074\n",
      "progress: 26 w= 1.9997126908902887 loss= 7.429187207079447e-07\n",
      "\tgrad:  1.0 2.0 -0.0005746182194226179\n",
      "\tgrad:  2.0 4.0 -0.002252503420136165\n",
      "\tgrad:  3.0 6.0 -0.00466268207967957\n",
      "progress: 27 w= 1.9997875889274812 loss= 4.060661735575354e-07\n",
      "\tgrad:  1.0 2.0 -0.0004248221450375844\n",
      "\tgrad:  2.0 4.0 -0.0016653028085471533\n",
      "\tgrad:  3.0 6.0 -0.0034471768136938863\n",
      "progress: 28 w= 1.9998429619451539 loss= 2.2194855602869353e-07\n",
      "\tgrad:  1.0 2.0 -0.00031407610969225175\n",
      "\tgrad:  2.0 4.0 -0.0012311783499932005\n",
      "\tgrad:  3.0 6.0 -0.0025485391844828342\n",
      "progress: 29 w= 1.9998838998815958 loss= 1.213131374411496e-07\n",
      "\tgrad:  1.0 2.0 -0.00023220023680847746\n",
      "\tgrad:  2.0 4.0 -0.0009102249282886277\n",
      "\tgrad:  3.0 6.0 -0.0018841656015560204\n",
      "progress: 30 w= 1.9999141657892625 loss= 6.630760559646474e-08\n",
      "\tgrad:  1.0 2.0 -0.00017166842147497974\n",
      "\tgrad:  2.0 4.0 -0.0006729402121816719\n",
      "\tgrad:  3.0 6.0 -0.0013929862392156878\n",
      "progress: 31 w= 1.9999365417379913 loss= 3.624255915449335e-08\n",
      "\tgrad:  1.0 2.0 -0.0001269165240174175\n",
      "\tgrad:  2.0 4.0 -0.0004975127741477792\n",
      "\tgrad:  3.0 6.0 -0.0010298514424817995\n",
      "progress: 32 w= 1.9999530845453979 loss= 1.9809538924707548e-08\n",
      "\tgrad:  1.0 2.0 -9.383090920422887e-05\n",
      "\tgrad:  2.0 4.0 -0.00036781716408107457\n",
      "\tgrad:  3.0 6.0 -0.0007613815296476645\n",
      "progress: 33 w= 1.9999653148414271 loss= 1.0827542027017377e-08\n",
      "\tgrad:  1.0 2.0 -6.937031714571162e-05\n",
      "\tgrad:  2.0 4.0 -0.0002719316432120422\n",
      "\tgrad:  3.0 6.0 -0.0005628985014531906\n",
      "progress: 34 w= 1.999974356846045 loss= 5.9181421028034105e-09\n",
      "\tgrad:  1.0 2.0 -5.1286307909848006e-05\n",
      "\tgrad:  2.0 4.0 -0.00020104232700646207\n",
      "\tgrad:  3.0 6.0 -0.0004161576169003922\n",
      "progress: 35 w= 1.9999810417085633 loss= 3.2347513278475087e-09\n",
      "\tgrad:  1.0 2.0 -3.7916582873442906e-05\n",
      "\tgrad:  2.0 4.0 -0.0001486330048638962\n",
      "\tgrad:  3.0 6.0 -0.0003076703200690645\n",
      "progress: 36 w= 1.9999859839076413 loss= 1.7680576050779005e-09\n",
      "\tgrad:  1.0 2.0 -2.8032184717474706e-05\n",
      "\tgrad:  2.0 4.0 -0.0001098861640933535\n",
      "\tgrad:  3.0 6.0 -0.00022746435967313516\n",
      "progress: 37 w= 1.9999896377347262 loss= 9.6638887447731e-10\n",
      "\tgrad:  1.0 2.0 -2.0724530547688857e-05\n",
      "\tgrad:  2.0 4.0 -8.124015974608767e-05\n",
      "\tgrad:  3.0 6.0 -0.00016816713067413502\n",
      "progress: 38 w= 1.999992339052936 loss= 5.282109892545845e-10\n",
      "\tgrad:  1.0 2.0 -1.5321894128117464e-05\n",
      "\tgrad:  2.0 4.0 -6.006182498197177e-05\n",
      "\tgrad:  3.0 6.0 -0.00012432797771566584\n",
      "progress: 39 w= 1.9999943361699042 loss= 2.887107421958329e-10\n",
      "\tgrad:  1.0 2.0 -1.1327660191629008e-05\n",
      "\tgrad:  2.0 4.0 -4.4404427951505454e-05\n",
      "\tgrad:  3.0 6.0 -9.191716585732479e-05\n",
      "progress: 40 w= 1.9999958126624442 loss= 1.5780416225633037e-10\n",
      "\tgrad:  1.0 2.0 -8.37467511161094e-06\n",
      "\tgrad:  2.0 4.0 -3.282872643772805e-05\n",
      "\tgrad:  3.0 6.0 -6.795546372551087e-05\n",
      "progress: 41 w= 1.999996904251097 loss= 8.625295142578772e-11\n",
      "\tgrad:  1.0 2.0 -6.191497806007362e-06\n",
      "\tgrad:  2.0 4.0 -2.4270671399762023e-05\n",
      "\tgrad:  3.0 6.0 -5.0240289795056015e-05\n",
      "progress: 42 w= 1.999997711275687 loss= 4.71443308235547e-11\n",
      "\tgrad:  1.0 2.0 -4.5774486259198e-06\n",
      "\tgrad:  2.0 4.0 -1.794359861406747e-05\n",
      "\tgrad:  3.0 6.0 -3.714324913239864e-05\n",
      "progress: 43 w= 1.9999983079186507 loss= 2.5768253628059826e-11\n",
      "\tgrad:  1.0 2.0 -3.3841626985164908e-06\n",
      "\tgrad:  2.0 4.0 -1.326591777761621e-05\n",
      "\tgrad:  3.0 6.0 -2.7460449796734565e-05\n",
      "progress: 44 w= 1.9999987490239537 loss= 1.4084469615916932e-11\n",
      "\tgrad:  1.0 2.0 -2.5019520926150562e-06\n",
      "\tgrad:  2.0 4.0 -9.807652203264183e-06\n",
      "\tgrad:  3.0 6.0 -2.0301840059744336e-05\n",
      "progress: 45 w= 1.9999990751383971 loss= 7.698320862431846e-12\n",
      "\tgrad:  1.0 2.0 -1.8497232057157476e-06\n",
      "\tgrad:  2.0 4.0 -7.250914967116273e-06\n",
      "\tgrad:  3.0 6.0 -1.5009393983689279e-05\n",
      "progress: 46 w= 1.9999993162387186 loss= 4.20776540913866e-12\n",
      "\tgrad:  1.0 2.0 -1.3675225627451937e-06\n",
      "\tgrad:  2.0 4.0 -5.3606884460322135e-06\n",
      "\tgrad:  3.0 6.0 -1.109662508014253e-05\n",
      "progress: 47 w= 1.9999994944870796 loss= 2.299889814334344e-12\n",
      "\tgrad:  1.0 2.0 -1.0110258408246864e-06\n",
      "\tgrad:  2.0 4.0 -3.963221296032771e-06\n",
      "\tgrad:  3.0 6.0 -8.20386808086937e-06\n",
      "progress: 48 w= 1.9999996262682318 loss= 1.2570789110540446e-12\n",
      "\tgrad:  1.0 2.0 -7.474635363990956e-07\n",
      "\tgrad:  2.0 4.0 -2.930057062755509e-06\n",
      "\tgrad:  3.0 6.0 -6.065218119744031e-06\n",
      "progress: 49 w= 1.999999723695619 loss= 6.870969979249939e-13\n",
      "\tgrad:  1.0 2.0 -5.526087618612507e-07\n",
      "\tgrad:  2.0 4.0 -2.166226346744793e-06\n",
      "\tgrad:  3.0 6.0 -4.484088535150477e-06\n",
      "progress: 50 w= 1.9999997957248556 loss= 3.7555501141274804e-13\n",
      "\tgrad:  1.0 2.0 -4.08550288710785e-07\n",
      "\tgrad:  2.0 4.0 -1.6015171322436572e-06\n",
      "\tgrad:  3.0 6.0 -3.3151404608133817e-06\n",
      "progress: 51 w= 1.9999998489769344 loss= 2.052716967104274e-13\n",
      "\tgrad:  1.0 2.0 -3.020461312175371e-07\n",
      "\tgrad:  2.0 4.0 -1.1840208351543424e-06\n",
      "\tgrad:  3.0 6.0 -2.4509231284497446e-06\n",
      "progress: 52 w= 1.9999998883468353 loss= 1.1219786256679713e-13\n",
      "\tgrad:  1.0 2.0 -2.2330632942768602e-07\n",
      "\tgrad:  2.0 4.0 -8.753608113920563e-07\n",
      "\tgrad:  3.0 6.0 -1.811996877876254e-06\n",
      "progress: 53 w= 1.9999999174534755 loss= 6.132535848018759e-14\n",
      "\tgrad:  1.0 2.0 -1.6509304900935717e-07\n",
      "\tgrad:  2.0 4.0 -6.471647520100987e-07\n",
      "\tgrad:  3.0 6.0 -1.3396310407642886e-06\n",
      "progress: 54 w= 1.999999938972364 loss= 3.351935118167793e-14\n",
      "\tgrad:  1.0 2.0 -1.220552721115098e-07\n",
      "\tgrad:  2.0 4.0 -4.784566662863199e-07\n",
      "\tgrad:  3.0 6.0 -9.904052991061008e-07\n",
      "progress: 55 w= 1.9999999548815364 loss= 1.8321081844499955e-14\n",
      "\tgrad:  1.0 2.0 -9.023692726373156e-08\n",
      "\tgrad:  2.0 4.0 -3.5372875473171916e-07\n",
      "\tgrad:  3.0 6.0 -7.322185204827747e-07\n",
      "progress: 56 w= 1.9999999666433785 loss= 1.0013977760018664e-14\n",
      "\tgrad:  1.0 2.0 -6.671324292994996e-08\n",
      "\tgrad:  2.0 4.0 -2.615159129248923e-07\n",
      "\tgrad:  3.0 6.0 -5.413379398078177e-07\n",
      "progress: 57 w= 1.9999999753390494 loss= 5.473462367088053e-15\n",
      "\tgrad:  1.0 2.0 -4.932190122985958e-08\n",
      "\tgrad:  2.0 4.0 -1.9334185274999527e-07\n",
      "\tgrad:  3.0 6.0 -4.002176350326181e-07\n",
      "progress: 58 w= 1.9999999817678633 loss= 2.991697274308627e-15\n",
      "\tgrad:  1.0 2.0 -3.6464273378555845e-08\n",
      "\tgrad:  2.0 4.0 -1.429399514307761e-07\n",
      "\tgrad:  3.0 6.0 -2.9588569994132286e-07\n",
      "progress: 59 w= 1.9999999865207625 loss= 1.6352086111474931e-15\n",
      "\tgrad:  1.0 2.0 -2.6958475007887728e-08\n",
      "\tgrad:  2.0 4.0 -1.0567722164012139e-07\n",
      "\tgrad:  3.0 6.0 -2.1875184863517916e-07\n",
      "progress: 60 w= 1.999999990034638 loss= 8.937759877335403e-16\n",
      "\tgrad:  1.0 2.0 -1.993072418216002e-08\n",
      "\tgrad:  2.0 4.0 -7.812843882959442e-08\n",
      "\tgrad:  3.0 6.0 -1.617258700292723e-07\n",
      "progress: 61 w= 1.9999999926324883 loss= 4.885220495987371e-16\n",
      "\tgrad:  1.0 2.0 -1.473502342363986e-08\n",
      "\tgrad:  2.0 4.0 -5.7761292637792394e-08\n",
      "\tgrad:  3.0 6.0 -1.195658771990793e-07\n",
      "progress: 62 w= 1.99999999455311 loss= 2.670175009618106e-16\n",
      "\tgrad:  1.0 2.0 -1.0893780100218464e-08\n",
      "\tgrad:  2.0 4.0 -4.270361841918202e-08\n",
      "\tgrad:  3.0 6.0 -8.839649012770678e-08\n",
      "progress: 63 w= 1.9999999959730488 loss= 1.4594702493172377e-16\n",
      "\tgrad:  1.0 2.0 -8.05390243385773e-09\n",
      "\tgrad:  2.0 4.0 -3.1571296688071016e-08\n",
      "\tgrad:  3.0 6.0 -6.53525820126788e-08\n",
      "progress: 64 w= 1.9999999970228268 loss= 7.977204100704301e-17\n",
      "\tgrad:  1.0 2.0 -5.9543463493128e-09\n",
      "\tgrad:  2.0 4.0 -2.334103754719763e-08\n",
      "\tgrad:  3.0 6.0 -4.8315948575350376e-08\n",
      "progress: 65 w= 1.9999999977989402 loss= 4.360197735196887e-17\n",
      "\tgrad:  1.0 2.0 -4.402119557767037e-09\n",
      "\tgrad:  2.0 4.0 -1.725630838222969e-08\n",
      "\tgrad:  3.0 6.0 -3.5720557178819945e-08\n",
      "progress: 66 w= 1.9999999983727301 loss= 2.3832065197304227e-17\n",
      "\tgrad:  1.0 2.0 -3.254539748809293e-09\n",
      "\tgrad:  2.0 4.0 -1.2757796596929438e-08\n",
      "\tgrad:  3.0 6.0 -2.6408640607655798e-08\n",
      "progress: 67 w= 1.9999999987969397 loss= 1.3026183953845832e-17\n",
      "\tgrad:  1.0 2.0 -2.406120636067044e-09\n",
      "\tgrad:  2.0 4.0 -9.431992964437086e-09\n",
      "\tgrad:  3.0 6.0 -1.9524227568012975e-08\n",
      "progress: 68 w= 1.999999999110563 loss= 7.11988308874388e-18\n",
      "\tgrad:  1.0 2.0 -1.7788739370416806e-09\n",
      "\tgrad:  2.0 4.0 -6.97318647269185e-09\n",
      "\tgrad:  3.0 6.0 -1.4434496264925656e-08\n",
      "progress: 69 w= 1.9999999993424284 loss= 3.89160224698574e-18\n",
      "\tgrad:  1.0 2.0 -1.3151431055291596e-09\n",
      "\tgrad:  2.0 4.0 -5.155360582875801e-09\n",
      "\tgrad:  3.0 6.0 -1.067159693945996e-08\n",
      "progress: 70 w= 1.9999999995138495 loss= 2.1270797208746147e-18\n",
      "\tgrad:  1.0 2.0 -9.72300906454393e-10\n",
      "\tgrad:  2.0 4.0 -3.811418736177075e-09\n",
      "\tgrad:  3.0 6.0 -7.88963561149103e-09\n",
      "progress: 71 w= 1.9999999996405833 loss= 1.1626238773828175e-18\n",
      "\tgrad:  1.0 2.0 -7.18833437218791e-10\n",
      "\tgrad:  2.0 4.0 -2.8178277489132597e-09\n",
      "\tgrad:  3.0 6.0 -5.832902161273523e-09\n",
      "progress: 72 w= 1.999999999734279 loss= 6.354692062078993e-19\n",
      "\tgrad:  1.0 2.0 -5.314420015167798e-10\n",
      "\tgrad:  2.0 4.0 -2.0832526814729135e-09\n",
      "\tgrad:  3.0 6.0 -4.31233715403323e-09\n",
      "progress: 73 w= 1.9999999998035491 loss= 3.4733644793346653e-19\n",
      "\tgrad:  1.0 2.0 -3.92901711165905e-10\n",
      "\tgrad:  2.0 4.0 -1.5401742103904326e-09\n",
      "\tgrad:  3.0 6.0 -3.188159070077745e-09\n",
      "progress: 74 w= 1.9999999998547615 loss= 1.8984796531526204e-19\n",
      "\tgrad:  1.0 2.0 -2.9047697580608656e-10\n",
      "\tgrad:  2.0 4.0 -1.1386696030513122e-09\n",
      "\tgrad:  3.0 6.0 -2.3570478902001923e-09\n",
      "progress: 75 w= 1.9999999998926234 loss= 1.0376765851119951e-19\n",
      "\tgrad:  1.0 2.0 -2.1475310418850313e-10\n",
      "\tgrad:  2.0 4.0 -8.418314934033333e-10\n",
      "\tgrad:  3.0 6.0 -1.7425900722400911e-09\n",
      "progress: 76 w= 1.9999999999206153 loss= 5.671751114309842e-20\n",
      "\tgrad:  1.0 2.0 -1.5876944203796484e-10\n",
      "\tgrad:  2.0 4.0 -6.223768167501476e-10\n",
      "\tgrad:  3.0 6.0 -1.2883241140571045e-09\n",
      "progress: 77 w= 1.9999999999413098 loss= 3.100089617511693e-20\n",
      "\tgrad:  1.0 2.0 -1.17380327679939e-10\n",
      "\tgrad:  2.0 4.0 -4.601314884666863e-10\n",
      "\tgrad:  3.0 6.0 -9.524754318590567e-10\n",
      "progress: 78 w= 1.9999999999566096 loss= 1.6944600977692705e-20\n",
      "\tgrad:  1.0 2.0 -8.678080476443029e-11\n",
      "\tgrad:  2.0 4.0 -3.4018121652934497e-10\n",
      "\tgrad:  3.0 6.0 -7.041780492045291e-10\n",
      "progress: 79 w= 1.9999999999679208 loss= 9.2616919156479e-21\n",
      "\tgrad:  1.0 2.0 -6.415845632545825e-11\n",
      "\tgrad:  2.0 4.0 -2.5150193039280566e-10\n",
      "\tgrad:  3.0 6.0 -5.206075570640678e-10\n",
      "progress: 80 w= 1.9999999999762834 loss= 5.062350511130293e-21\n",
      "\tgrad:  1.0 2.0 -4.743316850408519e-11\n",
      "\tgrad:  2.0 4.0 -1.8593837580738182e-10\n",
      "\tgrad:  3.0 6.0 -3.8489211817704927e-10\n",
      "progress: 81 w= 1.999999999982466 loss= 2.7669155644059242e-21\n",
      "\tgrad:  1.0 2.0 -3.5067948545020045e-11\n",
      "\tgrad:  2.0 4.0 -1.3746692673066718e-10\n",
      "\tgrad:  3.0 6.0 -2.845563784603655e-10\n",
      "progress: 82 w= 1.9999999999870368 loss= 1.5124150106147723e-21\n",
      "\tgrad:  1.0 2.0 -2.5926372160256506e-11\n",
      "\tgrad:  2.0 4.0 -1.0163070385260653e-10\n",
      "\tgrad:  3.0 6.0 -2.1037571684701106e-10\n",
      "progress: 83 w= 1.999999999990416 loss= 8.26683933105326e-22\n",
      "\tgrad:  1.0 2.0 -1.9167778475548403e-11\n",
      "\tgrad:  2.0 4.0 -7.51381179497912e-11\n",
      "\tgrad:  3.0 6.0 -1.5553425214420713e-10\n",
      "progress: 84 w= 1.9999999999929146 loss= 4.518126871054872e-22\n",
      "\tgrad:  1.0 2.0 -1.4170886686315498e-11\n",
      "\tgrad:  2.0 4.0 -5.555023108172463e-11\n",
      "\tgrad:  3.0 6.0 -1.1499068364173581e-10\n",
      "progress: 85 w= 1.9999999999947617 loss= 2.469467919185614e-22\n",
      "\tgrad:  1.0 2.0 -1.0476508549572827e-11\n",
      "\tgrad:  2.0 4.0 -4.106759377009439e-11\n",
      "\tgrad:  3.0 6.0 -8.500933290633839e-11\n",
      "progress: 86 w= 1.9999999999961273 loss= 1.349840097651456e-22\n",
      "\tgrad:  1.0 2.0 -7.745359908994942e-12\n",
      "\tgrad:  2.0 4.0 -3.036149109902908e-11\n",
      "\tgrad:  3.0 6.0 -6.285105769165966e-11\n",
      "progress: 87 w= 1.999999999997137 loss= 7.376551550022107e-23\n",
      "\tgrad:  1.0 2.0 -5.726086271806707e-12\n",
      "\tgrad:  2.0 4.0 -2.2446045022661565e-11\n",
      "\tgrad:  3.0 6.0 -4.646416584819235e-11\n",
      "progress: 88 w= 1.9999999999978835 loss= 4.031726170507742e-23\n",
      "\tgrad:  1.0 2.0 -4.233058348290797e-12\n",
      "\tgrad:  2.0 4.0 -1.659294923683774e-11\n",
      "\tgrad:  3.0 6.0 -3.4351188560322043e-11\n",
      "progress: 89 w= 1.9999999999984353 loss= 2.2033851437431755e-23\n",
      "\tgrad:  1.0 2.0 -3.1294966618133913e-12\n",
      "\tgrad:  2.0 4.0 -1.226752033289813e-11\n",
      "\tgrad:  3.0 6.0 -2.539835008974478e-11\n",
      "progress: 90 w= 1.9999999999988431 loss= 1.2047849775995315e-23\n",
      "\tgrad:  1.0 2.0 -2.3137047833188262e-12\n",
      "\tgrad:  2.0 4.0 -9.070078021977679e-12\n",
      "\tgrad:  3.0 6.0 -1.8779644506139448e-11\n",
      "progress: 91 w= 1.9999999999991447 loss= 6.5840863393251405e-24\n",
      "\tgrad:  1.0 2.0 -1.7106316363424412e-12\n",
      "\tgrad:  2.0 4.0 -6.7057470687359455e-12\n",
      "\tgrad:  3.0 6.0 -1.3882228699912957e-11\n",
      "progress: 92 w= 1.9999999999993676 loss= 3.5991747246272455e-24\n",
      "\tgrad:  1.0 2.0 -1.2647660696529783e-12\n",
      "\tgrad:  2.0 4.0 -4.957811938766099e-12\n",
      "\tgrad:  3.0 6.0 -1.0263789818054647e-11\n",
      "progress: 93 w= 1.9999999999995324 loss= 1.969312363793734e-24\n",
      "\tgrad:  1.0 2.0 -9.352518759442319e-13\n",
      "\tgrad:  2.0 4.0 -3.666400516522117e-12\n",
      "\tgrad:  3.0 6.0 -7.58859641791787e-12\n",
      "progress: 94 w= 1.9999999999996543 loss= 1.0761829795642296e-24\n",
      "\tgrad:  1.0 2.0 -6.914468997365475e-13\n",
      "\tgrad:  2.0 4.0 -2.7107205369247822e-12\n",
      "\tgrad:  3.0 6.0 -5.611511255665391e-12\n",
      "progress: 95 w= 1.9999999999997444 loss= 5.875191475205477e-25\n",
      "\tgrad:  1.0 2.0 -5.111466805374221e-13\n",
      "\tgrad:  2.0 4.0 -2.0037305148434825e-12\n",
      "\tgrad:  3.0 6.0 -4.1460168631601846e-12\n",
      "progress: 96 w= 1.999999999999811 loss= 3.2110109830478153e-25\n",
      "\tgrad:  1.0 2.0 -3.779199175824033e-13\n",
      "\tgrad:  2.0 4.0 -1.4814816040598089e-12\n",
      "\tgrad:  3.0 6.0 -3.064215547965432e-12\n",
      "progress: 97 w= 1.9999999999998603 loss= 1.757455879087579e-25\n",
      "\tgrad:  1.0 2.0 -2.793321129956894e-13\n",
      "\tgrad:  2.0 4.0 -1.0942358130705543e-12\n",
      "\tgrad:  3.0 6.0 -2.2648549702353193e-12\n",
      "progress: 98 w= 1.9999999999998967 loss= 9.608404711682446e-26\n",
      "\tgrad:  1.0 2.0 -2.0650148258027912e-13\n",
      "\tgrad:  2.0 4.0 -8.100187187665142e-13\n",
      "\tgrad:  3.0 6.0 -1.6786572132332367e-12\n",
      "progress: 99 w= 1.9999999999999236 loss= 5.250973729513143e-26\n",
      "predict (after training) 4 hours 7.9999999999996945\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([1.0, 2.0, 3.0])\n",
    "y_data = np.array([2.0, 4.0, 6.0])\n",
    "\n",
    "w = 1.0\n",
    "\n",
    "# model for forward pass\n",
    "def forward(x):\n",
    "    return np.multiply(x, w)\n",
    "    \n",
    "# Loss function\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return np.multiply(np.subtract(y_pred, y), np.subtract(y_pred, y))\n",
    "\n",
    "# compute gradient\n",
    "\n",
    "def gradient(x, y):   #d_loss/d_w\n",
    "    return 2*x*(x*w - y)\n",
    "    \n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\", 4, forward(4))\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        grad = gradient(x_val, y_val)\n",
    "        w = w - 0.01 * grad   # learning rate x gradient\n",
    "        print(\"\\tgrad: \", x_val, y_val, grad)\n",
    "        l = loss(x_val, y_val)\n",
    "        \n",
    "    print(\"progress:\", epoch, \"w=\", w, \"loss=\", l)\n",
    "    \n",
    "\n",
    "# After training\n",
    "\n",
    "print(\"predict (after training)\", \"4 hours\", forward(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4-4 Compute gradients using computational graph (manually)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"ex4.4_manual_sol.jpeg\" style=width:600px;height:400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch lecture 5 (Linear Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model class in PyTorch way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.tensor([[2.0], [4.0], [6.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.linear = torch.nn.Linear(1,1) #Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training: forward, loss, backward, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(46.3567)\n",
      "1 tensor(20.7668)\n",
      "2 tensor(9.3730)\n",
      "3 tensor(4.2990)\n",
      "4 tensor(2.0384)\n",
      "5 tensor(1.0302)\n",
      "6 tensor(0.5796)\n",
      "7 tensor(0.3773)\n",
      "8 tensor(0.2855)\n",
      "9 tensor(0.2430)\n",
      "10 tensor(0.2224)\n",
      "11 tensor(0.2116)\n",
      "12 tensor(0.2051)\n",
      "13 tensor(0.2007)\n",
      "14 tensor(0.1971)\n",
      "15 tensor(0.1940)\n",
      "16 tensor(0.1911)\n",
      "17 tensor(0.1883)\n",
      "18 tensor(0.1855)\n",
      "19 tensor(0.1828)\n",
      "20 tensor(0.1802)\n",
      "21 tensor(0.1776)\n",
      "22 tensor(0.1751)\n",
      "23 tensor(0.1726)\n",
      "24 tensor(0.1701)\n",
      "25 tensor(0.1676)\n",
      "26 tensor(0.1652)\n",
      "27 tensor(0.1628)\n",
      "28 tensor(0.1605)\n",
      "29 tensor(0.1582)\n",
      "30 tensor(0.1559)\n",
      "31 tensor(0.1537)\n",
      "32 tensor(0.1515)\n",
      "33 tensor(0.1493)\n",
      "34 tensor(0.1472)\n",
      "35 tensor(0.1450)\n",
      "36 tensor(0.1430)\n",
      "37 tensor(0.1409)\n",
      "38 tensor(0.1389)\n",
      "39 tensor(0.1369)\n",
      "40 tensor(0.1349)\n",
      "41 tensor(0.1330)\n",
      "42 tensor(0.1311)\n",
      "43 tensor(0.1292)\n",
      "44 tensor(0.1273)\n",
      "45 tensor(0.1255)\n",
      "46 tensor(0.1237)\n",
      "47 tensor(0.1219)\n",
      "48 tensor(0.1202)\n",
      "49 tensor(0.1184)\n",
      "50 tensor(0.1167)\n",
      "51 tensor(0.1151)\n",
      "52 tensor(0.1134)\n",
      "53 tensor(0.1118)\n",
      "54 tensor(0.1102)\n",
      "55 tensor(0.1086)\n",
      "56 tensor(0.1070)\n",
      "57 tensor(0.1055)\n",
      "58 tensor(0.1040)\n",
      "59 tensor(0.1025)\n",
      "60 tensor(0.1010)\n",
      "61 tensor(0.0995)\n",
      "62 tensor(0.0981)\n",
      "63 tensor(0.0967)\n",
      "64 tensor(0.0953)\n",
      "65 tensor(0.0939)\n",
      "66 tensor(0.0926)\n",
      "67 tensor(0.0913)\n",
      "68 tensor(0.0900)\n",
      "69 tensor(0.0887)\n",
      "70 tensor(0.0874)\n",
      "71 tensor(0.0861)\n",
      "72 tensor(0.0849)\n",
      "73 tensor(0.0837)\n",
      "74 tensor(0.0825)\n",
      "75 tensor(0.0813)\n",
      "76 tensor(0.0801)\n",
      "77 tensor(0.0790)\n",
      "78 tensor(0.0778)\n",
      "79 tensor(0.0767)\n",
      "80 tensor(0.0756)\n",
      "81 tensor(0.0745)\n",
      "82 tensor(0.0735)\n",
      "83 tensor(0.0724)\n",
      "84 tensor(0.0714)\n",
      "85 tensor(0.0703)\n",
      "86 tensor(0.0693)\n",
      "87 tensor(0.0683)\n",
      "88 tensor(0.0673)\n",
      "89 tensor(0.0664)\n",
      "90 tensor(0.0654)\n",
      "91 tensor(0.0645)\n",
      "92 tensor(0.0636)\n",
      "93 tensor(0.0626)\n",
      "94 tensor(0.0617)\n",
      "95 tensor(0.0609)\n",
      "96 tensor(0.0600)\n",
      "97 tensor(0.0591)\n",
      "98 tensor(0.0583)\n",
      "99 tensor(0.0574)\n",
      "100 tensor(0.0566)\n",
      "101 tensor(0.0558)\n",
      "102 tensor(0.0550)\n",
      "103 tensor(0.0542)\n",
      "104 tensor(0.0534)\n",
      "105 tensor(0.0527)\n",
      "106 tensor(0.0519)\n",
      "107 tensor(0.0511)\n",
      "108 tensor(0.0504)\n",
      "109 tensor(0.0497)\n",
      "110 tensor(0.0490)\n",
      "111 tensor(0.0483)\n",
      "112 tensor(0.0476)\n",
      "113 tensor(0.0469)\n",
      "114 tensor(0.0462)\n",
      "115 tensor(0.0456)\n",
      "116 tensor(0.0449)\n",
      "117 tensor(0.0443)\n",
      "118 tensor(0.0436)\n",
      "119 tensor(0.0430)\n",
      "120 tensor(0.0424)\n",
      "121 tensor(0.0418)\n",
      "122 tensor(0.0412)\n",
      "123 tensor(0.0406)\n",
      "124 tensor(0.0400)\n",
      "125 tensor(0.0394)\n",
      "126 tensor(0.0388)\n",
      "127 tensor(0.0383)\n",
      "128 tensor(0.0377)\n",
      "129 tensor(0.0372)\n",
      "130 tensor(0.0367)\n",
      "131 tensor(0.0361)\n",
      "132 tensor(0.0356)\n",
      "133 tensor(0.0351)\n",
      "134 tensor(0.0346)\n",
      "135 tensor(0.0341)\n",
      "136 tensor(0.0336)\n",
      "137 tensor(0.0331)\n",
      "138 tensor(0.0327)\n",
      "139 tensor(0.0322)\n",
      "140 tensor(0.0317)\n",
      "141 tensor(0.0313)\n",
      "142 tensor(0.0308)\n",
      "143 tensor(0.0304)\n",
      "144 tensor(0.0299)\n",
      "145 tensor(0.0295)\n",
      "146 tensor(0.0291)\n",
      "147 tensor(0.0287)\n",
      "148 tensor(0.0283)\n",
      "149 tensor(0.0278)\n",
      "150 tensor(0.0274)\n",
      "151 tensor(0.0271)\n",
      "152 tensor(0.0267)\n",
      "153 tensor(0.0263)\n",
      "154 tensor(0.0259)\n",
      "155 tensor(0.0255)\n",
      "156 tensor(0.0252)\n",
      "157 tensor(0.0248)\n",
      "158 tensor(0.0244)\n",
      "159 tensor(0.0241)\n",
      "160 tensor(0.0237)\n",
      "161 tensor(0.0234)\n",
      "162 tensor(0.0231)\n",
      "163 tensor(0.0227)\n",
      "164 tensor(0.0224)\n",
      "165 tensor(0.0221)\n",
      "166 tensor(0.0218)\n",
      "167 tensor(0.0215)\n",
      "168 tensor(0.0212)\n",
      "169 tensor(0.0208)\n",
      "170 tensor(0.0205)\n",
      "171 tensor(0.0203)\n",
      "172 tensor(0.0200)\n",
      "173 tensor(0.0197)\n",
      "174 tensor(0.0194)\n",
      "175 tensor(0.0191)\n",
      "176 tensor(0.0188)\n",
      "177 tensor(0.0186)\n",
      "178 tensor(0.0183)\n",
      "179 tensor(0.0180)\n",
      "180 tensor(0.0178)\n",
      "181 tensor(0.0175)\n",
      "182 tensor(0.0173)\n",
      "183 tensor(0.0170)\n",
      "184 tensor(0.0168)\n",
      "185 tensor(0.0165)\n",
      "186 tensor(0.0163)\n",
      "187 tensor(0.0161)\n",
      "188 tensor(0.0158)\n",
      "189 tensor(0.0156)\n",
      "190 tensor(0.0154)\n",
      "191 tensor(0.0152)\n",
      "192 tensor(0.0149)\n",
      "193 tensor(0.0147)\n",
      "194 tensor(0.0145)\n",
      "195 tensor(0.0143)\n",
      "196 tensor(0.0141)\n",
      "197 tensor(0.0139)\n",
      "198 tensor(0.0137)\n",
      "199 tensor(0.0135)\n",
      "200 tensor(0.0133)\n",
      "201 tensor(0.0131)\n",
      "202 tensor(0.0129)\n",
      "203 tensor(0.0127)\n",
      "204 tensor(0.0126)\n",
      "205 tensor(0.0124)\n",
      "206 tensor(0.0122)\n",
      "207 tensor(0.0120)\n",
      "208 tensor(0.0119)\n",
      "209 tensor(0.0117)\n",
      "210 tensor(0.0115)\n",
      "211 tensor(0.0114)\n",
      "212 tensor(0.0112)\n",
      "213 tensor(0.0110)\n",
      "214 tensor(0.0109)\n",
      "215 tensor(0.0107)\n",
      "216 tensor(0.0106)\n",
      "217 tensor(0.0104)\n",
      "218 tensor(0.0103)\n",
      "219 tensor(0.0101)\n",
      "220 tensor(0.0100)\n",
      "221 tensor(0.0098)\n",
      "222 tensor(0.0097)\n",
      "223 tensor(0.0095)\n",
      "224 tensor(0.0094)\n",
      "225 tensor(0.0093)\n",
      "226 tensor(0.0091)\n",
      "227 tensor(0.0090)\n",
      "228 tensor(0.0089)\n",
      "229 tensor(0.0087)\n",
      "230 tensor(0.0086)\n",
      "231 tensor(0.0085)\n",
      "232 tensor(0.0084)\n",
      "233 tensor(0.0083)\n",
      "234 tensor(0.0081)\n",
      "235 tensor(0.0080)\n",
      "236 tensor(0.0079)\n",
      "237 tensor(0.0078)\n",
      "238 tensor(0.0077)\n",
      "239 tensor(0.0076)\n",
      "240 tensor(0.0075)\n",
      "241 tensor(0.0074)\n",
      "242 tensor(0.0072)\n",
      "243 tensor(0.0071)\n",
      "244 tensor(0.0070)\n",
      "245 tensor(0.0069)\n",
      "246 tensor(0.0068)\n",
      "247 tensor(0.0067)\n",
      "248 tensor(0.0066)\n",
      "249 tensor(0.0065)\n",
      "250 tensor(0.0065)\n",
      "251 tensor(0.0064)\n",
      "252 tensor(0.0063)\n",
      "253 tensor(0.0062)\n",
      "254 tensor(0.0061)\n",
      "255 tensor(0.0060)\n",
      "256 tensor(0.0059)\n",
      "257 tensor(0.0058)\n",
      "258 tensor(0.0057)\n",
      "259 tensor(0.0057)\n",
      "260 tensor(0.0056)\n",
      "261 tensor(0.0055)\n",
      "262 tensor(0.0054)\n",
      "263 tensor(0.0053)\n",
      "264 tensor(0.0053)\n",
      "265 tensor(0.0052)\n",
      "266 tensor(0.0051)\n",
      "267 tensor(0.0050)\n",
      "268 tensor(0.0050)\n",
      "269 tensor(0.0049)\n",
      "270 tensor(0.0048)\n",
      "271 tensor(0.0048)\n",
      "272 tensor(0.0047)\n",
      "273 tensor(0.0046)\n",
      "274 tensor(0.0046)\n",
      "275 tensor(0.0045)\n",
      "276 tensor(0.0044)\n",
      "277 tensor(0.0044)\n",
      "278 tensor(0.0043)\n",
      "279 tensor(0.0042)\n",
      "280 tensor(0.0042)\n",
      "281 tensor(0.0041)\n",
      "282 tensor(0.0041)\n",
      "283 tensor(0.0040)\n",
      "284 tensor(0.0039)\n",
      "285 tensor(0.0039)\n",
      "286 tensor(0.0038)\n",
      "287 tensor(0.0038)\n",
      "288 tensor(0.0037)\n",
      "289 tensor(0.0037)\n",
      "290 tensor(0.0036)\n",
      "291 tensor(0.0036)\n",
      "292 tensor(0.0035)\n",
      "293 tensor(0.0035)\n",
      "294 tensor(0.0034)\n",
      "295 tensor(0.0034)\n",
      "296 tensor(0.0033)\n",
      "297 tensor(0.0033)\n",
      "298 tensor(0.0032)\n",
      "299 tensor(0.0032)\n",
      "300 tensor(0.0031)\n",
      "301 tensor(0.0031)\n",
      "302 tensor(0.0030)\n",
      "303 tensor(0.0030)\n",
      "304 tensor(0.0030)\n",
      "305 tensor(0.0029)\n",
      "306 tensor(0.0029)\n",
      "307 tensor(0.0028)\n",
      "308 tensor(0.0028)\n",
      "309 tensor(0.0027)\n",
      "310 tensor(0.0027)\n",
      "311 tensor(0.0027)\n",
      "312 tensor(0.0026)\n",
      "313 tensor(0.0026)\n",
      "314 tensor(0.0026)\n",
      "315 tensor(0.0025)\n",
      "316 tensor(0.0025)\n",
      "317 tensor(0.0024)\n",
      "318 tensor(0.0024)\n",
      "319 tensor(0.0024)\n",
      "320 tensor(0.0023)\n",
      "321 tensor(0.0023)\n",
      "322 tensor(0.0023)\n",
      "323 tensor(0.0022)\n",
      "324 tensor(0.0022)\n",
      "325 tensor(0.0022)\n",
      "326 tensor(0.0021)\n",
      "327 tensor(0.0021)\n",
      "328 tensor(0.0021)\n",
      "329 tensor(0.0021)\n",
      "330 tensor(0.0020)\n",
      "331 tensor(0.0020)\n",
      "332 tensor(0.0020)\n",
      "333 tensor(0.0019)\n",
      "334 tensor(0.0019)\n",
      "335 tensor(0.0019)\n",
      "336 tensor(0.0019)\n",
      "337 tensor(0.0018)\n",
      "338 tensor(0.0018)\n",
      "339 tensor(0.0018)\n",
      "340 tensor(0.0018)\n",
      "341 tensor(0.0017)\n",
      "342 tensor(0.0017)\n",
      "343 tensor(0.0017)\n",
      "344 tensor(0.0017)\n",
      "345 tensor(0.0016)\n",
      "346 tensor(0.0016)\n",
      "347 tensor(0.0016)\n",
      "348 tensor(0.0016)\n",
      "349 tensor(0.0015)\n",
      "350 tensor(0.0015)\n",
      "351 tensor(0.0015)\n",
      "352 tensor(0.0015)\n",
      "353 tensor(0.0015)\n",
      "354 tensor(0.0014)\n",
      "355 tensor(0.0014)\n",
      "356 tensor(0.0014)\n",
      "357 tensor(0.0014)\n",
      "358 tensor(0.0014)\n",
      "359 tensor(0.0013)\n",
      "360 tensor(0.0013)\n",
      "361 tensor(0.0013)\n",
      "362 tensor(0.0013)\n",
      "363 tensor(0.0013)\n",
      "364 tensor(0.0012)\n",
      "365 tensor(0.0012)\n",
      "366 tensor(0.0012)\n",
      "367 tensor(0.0012)\n",
      "368 tensor(0.0012)\n",
      "369 tensor(0.0012)\n",
      "370 tensor(0.0011)\n",
      "371 tensor(0.0011)\n",
      "372 tensor(0.0011)\n",
      "373 tensor(0.0011)\n",
      "374 tensor(0.0011)\n",
      "375 tensor(0.0011)\n",
      "376 tensor(0.0010)\n",
      "377 tensor(0.0010)\n",
      "378 tensor(0.0010)\n",
      "379 tensor(0.0010)\n",
      "380 tensor(0.0010)\n",
      "381 tensor(0.0010)\n",
      "382 tensor(0.0010)\n",
      "383 tensor(0.0009)\n",
      "384 tensor(0.0009)\n",
      "385 tensor(0.0009)\n",
      "386 tensor(0.0009)\n",
      "387 tensor(0.0009)\n",
      "388 tensor(0.0009)\n",
      "389 tensor(0.0009)\n",
      "390 tensor(0.0009)\n",
      "391 tensor(0.0008)\n",
      "392 tensor(0.0008)\n",
      "393 tensor(0.0008)\n",
      "394 tensor(0.0008)\n",
      "395 tensor(0.0008)\n",
      "396 tensor(0.0008)\n",
      "397 tensor(0.0008)\n",
      "398 tensor(0.0008)\n",
      "399 tensor(0.0007)\n",
      "400 tensor(0.0007)\n",
      "401 tensor(0.0007)\n",
      "402 tensor(0.0007)\n",
      "403 tensor(0.0007)\n",
      "404 tensor(0.0007)\n",
      "405 tensor(0.0007)\n",
      "406 tensor(0.0007)\n",
      "407 tensor(0.0007)\n",
      "408 tensor(0.0007)\n",
      "409 tensor(0.0006)\n",
      "410 tensor(0.0006)\n",
      "411 tensor(0.0006)\n",
      "412 tensor(0.0006)\n",
      "413 tensor(0.0006)\n",
      "414 tensor(0.0006)\n",
      "415 tensor(0.0006)\n",
      "416 tensor(0.0006)\n",
      "417 tensor(0.0006)\n",
      "418 tensor(0.0006)\n",
      "419 tensor(0.0006)\n",
      "420 tensor(0.0006)\n",
      "421 tensor(0.0005)\n",
      "422 tensor(0.0005)\n",
      "423 tensor(0.0005)\n",
      "424 tensor(0.0005)\n",
      "425 tensor(0.0005)\n",
      "426 tensor(0.0005)\n",
      "427 tensor(0.0005)\n",
      "428 tensor(0.0005)\n",
      "429 tensor(0.0005)\n",
      "430 tensor(0.0005)\n",
      "431 tensor(0.0005)\n",
      "432 tensor(0.0005)\n",
      "433 tensor(0.0005)\n",
      "434 tensor(0.0004)\n",
      "435 tensor(0.0004)\n",
      "436 tensor(0.0004)\n",
      "437 tensor(0.0004)\n",
      "438 tensor(0.0004)\n",
      "439 tensor(0.0004)\n",
      "440 tensor(0.0004)\n",
      "441 tensor(0.0004)\n",
      "442 tensor(0.0004)\n",
      "443 tensor(0.0004)\n",
      "444 tensor(0.0004)\n",
      "445 tensor(0.0004)\n",
      "446 tensor(0.0004)\n",
      "447 tensor(0.0004)\n",
      "448 tensor(0.0004)\n",
      "449 tensor(0.0004)\n",
      "450 tensor(0.0004)\n",
      "451 tensor(0.0004)\n",
      "452 tensor(0.0003)\n",
      "453 tensor(0.0003)\n",
      "454 tensor(0.0003)\n",
      "455 tensor(0.0003)\n",
      "456 tensor(0.0003)\n",
      "457 tensor(0.0003)\n",
      "458 tensor(0.0003)\n",
      "459 tensor(0.0003)\n",
      "460 tensor(0.0003)\n",
      "461 tensor(0.0003)\n",
      "462 tensor(0.0003)\n",
      "463 tensor(0.0003)\n",
      "464 tensor(0.0003)\n",
      "465 tensor(0.0003)\n",
      "466 tensor(0.0003)\n",
      "467 tensor(0.0003)\n",
      "468 tensor(0.0003)\n",
      "469 tensor(0.0003)\n",
      "470 tensor(0.0003)\n",
      "471 tensor(0.0003)\n",
      "472 tensor(0.0003)\n",
      "473 tensor(0.0003)\n",
      "474 tensor(0.0003)\n",
      "475 tensor(0.0002)\n",
      "476 tensor(0.0002)\n",
      "477 tensor(0.0002)\n",
      "478 tensor(0.0002)\n",
      "479 tensor(0.0002)\n",
      "480 tensor(0.0002)\n",
      "481 tensor(0.0002)\n",
      "482 tensor(0.0002)\n",
      "483 tensor(0.0002)\n",
      "484 tensor(0.0002)\n",
      "485 tensor(0.0002)\n",
      "486 tensor(0.0002)\n",
      "487 tensor(0.0002)\n",
      "488 tensor(0.0002)\n",
      "489 tensor(0.0002)\n",
      "490 tensor(0.0002)\n",
      "491 tensor(0.0002)\n",
      "492 tensor(0.0002)\n",
      "493 tensor(0.0002)\n",
      "494 tensor(0.0002)\n",
      "495 tensor(0.0002)\n",
      "496 tensor(0.0002)\n",
      "497 tensor(0.0002)\n",
      "498 tensor(0.0002)\n",
      "499 tensor(0.0002)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_var = torch.tensor([[4.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (after training) 4 tensor([[7.9848]])\n"
     ]
    }
   ],
   "source": [
    "print(\"predict (after training)\", 4, model.forward(hour_var).data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch lecture 6 (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_data = torch.tensor([[0.], [0.], [1.], [1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (after training) 1.0 tensor(False)\n"
     ]
    }
   ],
   "source": [
    "hour_var = torch.tensor([[1.0]])\n",
    "print(\"predict (after training)\", 1.0, model.forward(hour_var).data[0][0] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (after training) 7.0 tensor(True)\n"
     ]
    }
   ],
   "source": [
    "hour_var = torch.tensor([[7.0]])\n",
    "print(\"predict (after training)\", 7.0, model.forward(hour_var).data[0][0] > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch lecture 7 (Wide & Deep Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('/home/a/Documents/Pytorch/data_diabetes.csv', delimiter = ',', dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "y_data = torch.from_numpy(xy[:, [-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([759, 8])\n",
      "torch.Size([759, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_data.data.shape)\n",
    "print(y_data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6)\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch lecture 8 (Data Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):  \n",
    "        xy = np.loadtxt('/home/a/Documents/Pytorch/data_diabetes.csv', delimiter = ',',\n",
    "                       dtype = np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset= dataset, \n",
    "                         batch_size= 32,\n",
    "                         shuffle=True,\n",
    "                         num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch lecture 9 (Softmax Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross entropy loss calculation using numpy (understanding fundamentals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 0, 0])\n",
    "y_pred1 = np.array([0.7, 0.2, 0.1])\n",
    "y_pred2 = np.array([0.1, 0.3, 0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1 =  0.35667494393873245\n",
      "loss 2 =  2.3025850929940455\n"
     ]
    }
   ],
   "source": [
    "print(\"loss 1 = \", np.sum(-y * np.log(y_pred1)))\n",
    "print(\"loss 2 = \", np.sum(-y * np.log(y_pred2)))  # cross entropy function is sum of product of -y and log y. D(y^, Y) = -YlogY^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross entropy in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "y = torch.tensor([0])   # input is class, not one-hot\n",
    "y_pred1 = torch.tensor([[2.0, 1.0, 0.1]])\n",
    "y_pred2 = torch.tensor([[0.5, 2.0, 0.3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Loss1 =  tensor(0.4170)\n",
      "PyTorch Loss2 =  tensor(1.8406)\n"
     ]
    }
   ],
   "source": [
    "l1 = loss(y_pred1, y)\n",
    "l2 = loss(y_pred2, y)\n",
    "\n",
    "print(\"PyTorch Loss1 = \", l1.data)\n",
    "print(\"PyTorch Loss2 = \", l2.data)\n",
    "\n",
    "# the output of linear model is in the form of logits.\n",
    "# softmax fucntion calculate probabilites of logits if applied to linear model.\n",
    "# for cross entropy loss calculation, no need to apply softmax function as it is built in feature of crossentropyloss in pytorch.\n",
    "# we need to provide logit values only\n",
    "# we don't need to provide one-hot labels but class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "y = torch.tensor([2, 0, 1])   # input is class, not one-hot\n",
    "y_pred1 = torch.tensor([[0.1, 0.2, 0.9],\n",
    "                        [1.1, 0.1, 0.2],\n",
    "                        [0.2, 2.1, 0.1]])\n",
    "y_pred2 = torch.tensor([[0.8, 0.2, 0.3],\n",
    "                        [0.2, 0.3, 0.5],\n",
    "                        [0.2, 0.2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss1 =  tensor(0.4966)\n",
      "\n",
      "Batch Loss2 =  tensor(1.2389)\n"
     ]
    }
   ],
   "source": [
    "l1 = loss(y_pred1, y)\n",
    "l2 = loss(y_pred2, y)\n",
    "\n",
    "print(\"Batch Loss1 = \", l1.data)\n",
    "print(\"\\nBatch Loss2 = \", l2.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax & nll loss (log softmax) on mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<torch.cuda.device object at 0x7f3b1f8664f0>\n",
      "1\n",
      "GeForce GTX 1650 Ti with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils import data\n",
    "from torch import cuda, nn, optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MNIST Model on cpu\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "# MNIST Dataset\n",
    "batch_size = 64\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')  \n",
    "\n",
    "# The f means Formatted string literals and\n",
    "# it's new in Python 3.6. A formatted string literal or f-string is a string literal that is prefixed with 'f' \n",
    "# or 'F'. These strings may contain replacement fields, which are expressions delimited by curly braces {}.\n",
    "# While other string literals always have a constant value, \n",
    "# formatted strings are really expressions evaluated at run time.\n",
    "\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnistt/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnistt/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of net(\n",
       "  (l1): Linear(in_features=784, out_features=520, bias=True)\n",
       "  (l2): Linear(in_features=520, out_features=320, bias=True)\n",
       "  (l3): Linear(in_features=320, out_features=240, bias=True)\n",
       "  (l4): Linear(in_features=240, out_features=120, bias=True)\n",
       "  (l5): Linear(in_features=120, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "\n",
    "# the model.train() sets the modules in the network in training mode.\n",
    "# It tells our model that we are currently in the training phase \n",
    "# so the model keeps some layers, like dropout, batch-normalization \n",
    "# which behaves differently depends on the current phase, active.\n",
    "# whereas the model.eval() does the opposite.\n",
    "# Therefore, once the model.eval() has been called then, our model deactivate such layers \n",
    "# so that the model outputs its inference as is expected.\n",
    "\n",
    "    model.train()   \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).item()\n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.0f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.310788\n",
      "Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.306849\n",
      "Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 2.301533\n",
      "Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 2.304139\n",
      "Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 2.304168\n",
      "Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 2.298782\n",
      "Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 2.301463\n",
      "Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 2.300391\n",
      "Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 2.296320\n",
      "Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 2.297339\n",
      "Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 2.297946\n",
      "Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 2.297056\n",
      "Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 2.297132\n",
      "Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 2.303136\n",
      "Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 2.293896\n",
      "Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 2.296070\n",
      "Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 2.295924\n",
      "Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 2.297413\n",
      "Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 2.290139\n",
      "Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 2.286154\n",
      "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 2.304445\n",
      "Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 2.296352\n",
      "Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 2.301550\n",
      "Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 2.299884\n",
      "Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 2.287610\n",
      "Train Epoch: 1 | Batch Status: 16000/60000 (27%) | Loss: 2.300564\n",
      "Train Epoch: 1 | Batch Status: 16640/60000 (28%) | Loss: 2.293710\n",
      "Train Epoch: 1 | Batch Status: 17280/60000 (29%) | Loss: 2.290856\n",
      "Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 2.285242\n",
      "Train Epoch: 1 | Batch Status: 18560/60000 (31%) | Loss: 2.281514\n",
      "Train Epoch: 1 | Batch Status: 19200/60000 (32%) | Loss: 2.290467\n",
      "Train Epoch: 1 | Batch Status: 19840/60000 (33%) | Loss: 2.291789\n",
      "Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 2.290868\n",
      "Train Epoch: 1 | Batch Status: 21120/60000 (35%) | Loss: 2.285002\n",
      "Train Epoch: 1 | Batch Status: 21760/60000 (36%) | Loss: 2.294516\n",
      "Train Epoch: 1 | Batch Status: 22400/60000 (37%) | Loss: 2.288011\n",
      "Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 2.275811\n",
      "Train Epoch: 1 | Batch Status: 23680/60000 (39%) | Loss: 2.285051\n",
      "Train Epoch: 1 | Batch Status: 24320/60000 (41%) | Loss: 2.296494\n",
      "Train Epoch: 1 | Batch Status: 24960/60000 (42%) | Loss: 2.278031\n",
      "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 2.282779\n",
      "Train Epoch: 1 | Batch Status: 26240/60000 (44%) | Loss: 2.286436\n",
      "Train Epoch: 1 | Batch Status: 26880/60000 (45%) | Loss: 2.284220\n",
      "Train Epoch: 1 | Batch Status: 27520/60000 (46%) | Loss: 2.273622\n",
      "Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 2.272877\n",
      "Train Epoch: 1 | Batch Status: 28800/60000 (48%) | Loss: 2.286092\n",
      "Train Epoch: 1 | Batch Status: 29440/60000 (49%) | Loss: 2.268580\n",
      "Train Epoch: 1 | Batch Status: 30080/60000 (50%) | Loss: 2.267596\n",
      "Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 2.277367\n",
      "Train Epoch: 1 | Batch Status: 31360/60000 (52%) | Loss: 2.261456\n",
      "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 2.276429\n",
      "Train Epoch: 1 | Batch Status: 32640/60000 (54%) | Loss: 2.264688\n",
      "Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 2.268743\n",
      "Train Epoch: 1 | Batch Status: 33920/60000 (57%) | Loss: 2.269137\n",
      "Train Epoch: 1 | Batch Status: 34560/60000 (58%) | Loss: 2.250778\n",
      "Train Epoch: 1 | Batch Status: 35200/60000 (59%) | Loss: 2.242362\n",
      "Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 2.252093\n",
      "Train Epoch: 1 | Batch Status: 36480/60000 (61%) | Loss: 2.245429\n",
      "Train Epoch: 1 | Batch Status: 37120/60000 (62%) | Loss: 2.248824\n",
      "Train Epoch: 1 | Batch Status: 37760/60000 (63%) | Loss: 2.239573\n",
      "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 2.254823\n",
      "Train Epoch: 1 | Batch Status: 39040/60000 (65%) | Loss: 2.240275\n",
      "Train Epoch: 1 | Batch Status: 39680/60000 (66%) | Loss: 2.247410\n",
      "Train Epoch: 1 | Batch Status: 40320/60000 (67%) | Loss: 2.224607\n",
      "Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 2.238562\n",
      "Train Epoch: 1 | Batch Status: 41600/60000 (69%) | Loss: 2.192659\n",
      "Train Epoch: 1 | Batch Status: 42240/60000 (70%) | Loss: 2.213003\n",
      "Train Epoch: 1 | Batch Status: 42880/60000 (71%) | Loss: 2.180743\n",
      "Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 2.195366\n",
      "Train Epoch: 1 | Batch Status: 44160/60000 (74%) | Loss: 2.177799\n",
      "Train Epoch: 1 | Batch Status: 44800/60000 (75%) | Loss: 2.158540\n",
      "Train Epoch: 1 | Batch Status: 45440/60000 (76%) | Loss: 2.146621\n",
      "Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 2.142777\n",
      "Train Epoch: 1 | Batch Status: 46720/60000 (78%) | Loss: 2.121197\n",
      "Train Epoch: 1 | Batch Status: 47360/60000 (79%) | Loss: 2.104291\n",
      "Train Epoch: 1 | Batch Status: 48000/60000 (80%) | Loss: 2.066183\n",
      "Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 2.020101\n",
      "Train Epoch: 1 | Batch Status: 49280/60000 (82%) | Loss: 1.991890\n",
      "Train Epoch: 1 | Batch Status: 49920/60000 (83%) | Loss: 2.044652\n",
      "Train Epoch: 1 | Batch Status: 50560/60000 (84%) | Loss: 1.974595\n",
      "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 1.965039\n",
      "Train Epoch: 1 | Batch Status: 51840/60000 (86%) | Loss: 1.925202\n",
      "Train Epoch: 1 | Batch Status: 52480/60000 (87%) | Loss: 1.795377\n",
      "Train Epoch: 1 | Batch Status: 53120/60000 (88%) | Loss: 1.863505\n",
      "Train Epoch: 1 | Batch Status: 53760/60000 (90%) | Loss: 1.758483\n",
      "Train Epoch: 1 | Batch Status: 54400/60000 (91%) | Loss: 1.746128\n",
      "Train Epoch: 1 | Batch Status: 55040/60000 (92%) | Loss: 1.671677\n",
      "Train Epoch: 1 | Batch Status: 55680/60000 (93%) | Loss: 1.536375\n",
      "Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 1.593194\n",
      "Train Epoch: 1 | Batch Status: 56960/60000 (95%) | Loss: 1.473475\n",
      "Train Epoch: 1 | Batch Status: 57600/60000 (96%) | Loss: 1.384547\n",
      "Train Epoch: 1 | Batch Status: 58240/60000 (97%) | Loss: 1.201526\n",
      "Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 1.402391\n",
      "Train Epoch: 1 | Batch Status: 59520/60000 (99%) | Loss: 1.164359\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0182, Accuracy: 6875/10000 (69%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 1.064914\n",
      "Train Epoch: 2 | Batch Status: 640/60000 (1%) | Loss: 1.105187\n",
      "Train Epoch: 2 | Batch Status: 1280/60000 (2%) | Loss: 1.245497\n",
      "Train Epoch: 2 | Batch Status: 1920/60000 (3%) | Loss: 1.050243\n",
      "Train Epoch: 2 | Batch Status: 2560/60000 (4%) | Loss: 1.050307\n",
      "Train Epoch: 2 | Batch Status: 3200/60000 (5%) | Loss: 1.055599\n",
      "Train Epoch: 2 | Batch Status: 3840/60000 (6%) | Loss: 0.843700\n",
      "Train Epoch: 2 | Batch Status: 4480/60000 (7%) | Loss: 0.946141\n",
      "Train Epoch: 2 | Batch Status: 5120/60000 (9%) | Loss: 0.901155\n",
      "Train Epoch: 2 | Batch Status: 5760/60000 (10%) | Loss: 0.845174\n",
      "Train Epoch: 2 | Batch Status: 6400/60000 (11%) | Loss: 0.654576\n",
      "Train Epoch: 2 | Batch Status: 7040/60000 (12%) | Loss: 0.739052\n",
      "Train Epoch: 2 | Batch Status: 7680/60000 (13%) | Loss: 0.803136\n",
      "Train Epoch: 2 | Batch Status: 8320/60000 (14%) | Loss: 0.939810\n",
      "Train Epoch: 2 | Batch Status: 8960/60000 (15%) | Loss: 0.804321\n",
      "Train Epoch: 2 | Batch Status: 9600/60000 (16%) | Loss: 0.791684\n",
      "Train Epoch: 2 | Batch Status: 10240/60000 (17%) | Loss: 0.646617\n",
      "Train Epoch: 2 | Batch Status: 10880/60000 (18%) | Loss: 0.618768\n",
      "Train Epoch: 2 | Batch Status: 11520/60000 (19%) | Loss: 0.793871\n",
      "Train Epoch: 2 | Batch Status: 12160/60000 (20%) | Loss: 1.055799\n",
      "Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 0.869032\n",
      "Train Epoch: 2 | Batch Status: 13440/60000 (22%) | Loss: 0.845720\n",
      "Train Epoch: 2 | Batch Status: 14080/60000 (23%) | Loss: 0.778491\n",
      "Train Epoch: 2 | Batch Status: 14720/60000 (25%) | Loss: 0.543528\n",
      "Train Epoch: 2 | Batch Status: 15360/60000 (26%) | Loss: 0.807607\n",
      "Train Epoch: 2 | Batch Status: 16000/60000 (27%) | Loss: 0.660100\n",
      "Train Epoch: 2 | Batch Status: 16640/60000 (28%) | Loss: 0.668407\n",
      "Train Epoch: 2 | Batch Status: 17280/60000 (29%) | Loss: 0.680894\n",
      "Train Epoch: 2 | Batch Status: 17920/60000 (30%) | Loss: 0.661463\n",
      "Train Epoch: 2 | Batch Status: 18560/60000 (31%) | Loss: 0.578868\n",
      "Train Epoch: 2 | Batch Status: 19200/60000 (32%) | Loss: 0.698310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 | Batch Status: 19840/60000 (33%) | Loss: 0.470087\n",
      "Train Epoch: 2 | Batch Status: 20480/60000 (34%) | Loss: 0.594828\n",
      "Train Epoch: 2 | Batch Status: 21120/60000 (35%) | Loss: 0.494504\n",
      "Train Epoch: 2 | Batch Status: 21760/60000 (36%) | Loss: 0.624721\n",
      "Train Epoch: 2 | Batch Status: 22400/60000 (37%) | Loss: 0.490421\n",
      "Train Epoch: 2 | Batch Status: 23040/60000 (38%) | Loss: 0.769661\n",
      "Train Epoch: 2 | Batch Status: 23680/60000 (39%) | Loss: 0.460985\n",
      "Train Epoch: 2 | Batch Status: 24320/60000 (41%) | Loss: 0.780533\n",
      "Train Epoch: 2 | Batch Status: 24960/60000 (42%) | Loss: 0.727740\n",
      "Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 0.517024\n",
      "Train Epoch: 2 | Batch Status: 26240/60000 (44%) | Loss: 0.544747\n",
      "Train Epoch: 2 | Batch Status: 26880/60000 (45%) | Loss: 0.532422\n",
      "Train Epoch: 2 | Batch Status: 27520/60000 (46%) | Loss: 0.626358\n",
      "Train Epoch: 2 | Batch Status: 28160/60000 (47%) | Loss: 0.451439\n",
      "Train Epoch: 2 | Batch Status: 28800/60000 (48%) | Loss: 0.598500\n",
      "Train Epoch: 2 | Batch Status: 29440/60000 (49%) | Loss: 0.576622\n",
      "Train Epoch: 2 | Batch Status: 30080/60000 (50%) | Loss: 0.470806\n",
      "Train Epoch: 2 | Batch Status: 30720/60000 (51%) | Loss: 0.360017\n",
      "Train Epoch: 2 | Batch Status: 31360/60000 (52%) | Loss: 0.328391\n",
      "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 0.667170\n",
      "Train Epoch: 2 | Batch Status: 32640/60000 (54%) | Loss: 0.517471\n",
      "Train Epoch: 2 | Batch Status: 33280/60000 (55%) | Loss: 0.703396\n",
      "Train Epoch: 2 | Batch Status: 33920/60000 (57%) | Loss: 0.424292\n",
      "Train Epoch: 2 | Batch Status: 34560/60000 (58%) | Loss: 0.559334\n",
      "Train Epoch: 2 | Batch Status: 35200/60000 (59%) | Loss: 0.472238\n",
      "Train Epoch: 2 | Batch Status: 35840/60000 (60%) | Loss: 0.582507\n",
      "Train Epoch: 2 | Batch Status: 36480/60000 (61%) | Loss: 0.405345\n",
      "Train Epoch: 2 | Batch Status: 37120/60000 (62%) | Loss: 0.533337\n",
      "Train Epoch: 2 | Batch Status: 37760/60000 (63%) | Loss: 0.362199\n",
      "Train Epoch: 2 | Batch Status: 38400/60000 (64%) | Loss: 0.668930\n",
      "Train Epoch: 2 | Batch Status: 39040/60000 (65%) | Loss: 0.625367\n",
      "Train Epoch: 2 | Batch Status: 39680/60000 (66%) | Loss: 0.545453\n",
      "Train Epoch: 2 | Batch Status: 40320/60000 (67%) | Loss: 0.423840\n",
      "Train Epoch: 2 | Batch Status: 40960/60000 (68%) | Loss: 0.474400\n",
      "Train Epoch: 2 | Batch Status: 41600/60000 (69%) | Loss: 0.438723\n",
      "Train Epoch: 2 | Batch Status: 42240/60000 (70%) | Loss: 0.678071\n",
      "Train Epoch: 2 | Batch Status: 42880/60000 (71%) | Loss: 0.802250\n",
      "Train Epoch: 2 | Batch Status: 43520/60000 (72%) | Loss: 0.646702\n",
      "Train Epoch: 2 | Batch Status: 44160/60000 (74%) | Loss: 0.413075\n",
      "Train Epoch: 2 | Batch Status: 44800/60000 (75%) | Loss: 0.455995\n",
      "Train Epoch: 2 | Batch Status: 45440/60000 (76%) | Loss: 0.731854\n",
      "Train Epoch: 2 | Batch Status: 46080/60000 (77%) | Loss: 0.543984\n",
      "Train Epoch: 2 | Batch Status: 46720/60000 (78%) | Loss: 0.440590\n",
      "Train Epoch: 2 | Batch Status: 47360/60000 (79%) | Loss: 0.778575\n",
      "Train Epoch: 2 | Batch Status: 48000/60000 (80%) | Loss: 0.504347\n",
      "Train Epoch: 2 | Batch Status: 48640/60000 (81%) | Loss: 0.616312\n",
      "Train Epoch: 2 | Batch Status: 49280/60000 (82%) | Loss: 0.529398\n",
      "Train Epoch: 2 | Batch Status: 49920/60000 (83%) | Loss: 0.498094\n",
      "Train Epoch: 2 | Batch Status: 50560/60000 (84%) | Loss: 0.484437\n",
      "Train Epoch: 2 | Batch Status: 51200/60000 (85%) | Loss: 0.464008\n",
      "Train Epoch: 2 | Batch Status: 51840/60000 (86%) | Loss: 0.313312\n",
      "Train Epoch: 2 | Batch Status: 52480/60000 (87%) | Loss: 0.414703\n",
      "Train Epoch: 2 | Batch Status: 53120/60000 (88%) | Loss: 0.491756\n",
      "Train Epoch: 2 | Batch Status: 53760/60000 (90%) | Loss: 0.579682\n",
      "Train Epoch: 2 | Batch Status: 54400/60000 (91%) | Loss: 0.350254\n",
      "Train Epoch: 2 | Batch Status: 55040/60000 (92%) | Loss: 0.777368\n",
      "Train Epoch: 2 | Batch Status: 55680/60000 (93%) | Loss: 0.496709\n",
      "Train Epoch: 2 | Batch Status: 56320/60000 (94%) | Loss: 0.433834\n",
      "Train Epoch: 2 | Batch Status: 56960/60000 (95%) | Loss: 0.596345\n",
      "Train Epoch: 2 | Batch Status: 57600/60000 (96%) | Loss: 0.600696\n",
      "Train Epoch: 2 | Batch Status: 58240/60000 (97%) | Loss: 0.793944\n",
      "Train Epoch: 2 | Batch Status: 58880/60000 (98%) | Loss: 0.395440\n",
      "Train Epoch: 2 | Batch Status: 59520/60000 (99%) | Loss: 0.475818\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0071, Accuracy: 8686/10000 (87%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 3 | Batch Status: 0/60000 (0%) | Loss: 0.445909\n",
      "Train Epoch: 3 | Batch Status: 640/60000 (1%) | Loss: 0.336414\n",
      "Train Epoch: 3 | Batch Status: 1280/60000 (2%) | Loss: 0.469778\n",
      "Train Epoch: 3 | Batch Status: 1920/60000 (3%) | Loss: 0.377743\n",
      "Train Epoch: 3 | Batch Status: 2560/60000 (4%) | Loss: 0.416961\n",
      "Train Epoch: 3 | Batch Status: 3200/60000 (5%) | Loss: 0.276029\n",
      "Train Epoch: 3 | Batch Status: 3840/60000 (6%) | Loss: 0.780532\n",
      "Train Epoch: 3 | Batch Status: 4480/60000 (7%) | Loss: 0.558311\n",
      "Train Epoch: 3 | Batch Status: 5120/60000 (9%) | Loss: 0.385853\n",
      "Train Epoch: 3 | Batch Status: 5760/60000 (10%) | Loss: 0.466617\n",
      "Train Epoch: 3 | Batch Status: 6400/60000 (11%) | Loss: 0.345692\n",
      "Train Epoch: 3 | Batch Status: 7040/60000 (12%) | Loss: 0.483129\n",
      "Train Epoch: 3 | Batch Status: 7680/60000 (13%) | Loss: 0.538824\n",
      "Train Epoch: 3 | Batch Status: 8320/60000 (14%) | Loss: 0.519031\n",
      "Train Epoch: 3 | Batch Status: 8960/60000 (15%) | Loss: 0.268258\n",
      "Train Epoch: 3 | Batch Status: 9600/60000 (16%) | Loss: 0.239750\n",
      "Train Epoch: 3 | Batch Status: 10240/60000 (17%) | Loss: 0.333816\n",
      "Train Epoch: 3 | Batch Status: 10880/60000 (18%) | Loss: 0.487855\n",
      "Train Epoch: 3 | Batch Status: 11520/60000 (19%) | Loss: 0.375733\n",
      "Train Epoch: 3 | Batch Status: 12160/60000 (20%) | Loss: 0.220048\n",
      "Train Epoch: 3 | Batch Status: 12800/60000 (21%) | Loss: 0.451160\n",
      "Train Epoch: 3 | Batch Status: 13440/60000 (22%) | Loss: 0.254301\n",
      "Train Epoch: 3 | Batch Status: 14080/60000 (23%) | Loss: 0.560339\n",
      "Train Epoch: 3 | Batch Status: 14720/60000 (25%) | Loss: 0.399324\n",
      "Train Epoch: 3 | Batch Status: 15360/60000 (26%) | Loss: 0.416235\n",
      "Train Epoch: 3 | Batch Status: 16000/60000 (27%) | Loss: 0.398861\n",
      "Train Epoch: 3 | Batch Status: 16640/60000 (28%) | Loss: 0.384861\n",
      "Train Epoch: 3 | Batch Status: 17280/60000 (29%) | Loss: 0.231342\n",
      "Train Epoch: 3 | Batch Status: 17920/60000 (30%) | Loss: 0.340362\n",
      "Train Epoch: 3 | Batch Status: 18560/60000 (31%) | Loss: 0.361941\n",
      "Train Epoch: 3 | Batch Status: 19200/60000 (32%) | Loss: 0.497228\n",
      "Train Epoch: 3 | Batch Status: 19840/60000 (33%) | Loss: 1.004386\n",
      "Train Epoch: 3 | Batch Status: 20480/60000 (34%) | Loss: 0.586363\n",
      "Train Epoch: 3 | Batch Status: 21120/60000 (35%) | Loss: 0.494337\n",
      "Train Epoch: 3 | Batch Status: 21760/60000 (36%) | Loss: 0.510219\n",
      "Train Epoch: 3 | Batch Status: 22400/60000 (37%) | Loss: 0.332400\n",
      "Train Epoch: 3 | Batch Status: 23040/60000 (38%) | Loss: 0.487628\n",
      "Train Epoch: 3 | Batch Status: 23680/60000 (39%) | Loss: 0.399344\n",
      "Train Epoch: 3 | Batch Status: 24320/60000 (41%) | Loss: 0.383317\n",
      "Train Epoch: 3 | Batch Status: 24960/60000 (42%) | Loss: 0.422134\n",
      "Train Epoch: 3 | Batch Status: 25600/60000 (43%) | Loss: 0.216543\n",
      "Train Epoch: 3 | Batch Status: 26240/60000 (44%) | Loss: 0.244005\n",
      "Train Epoch: 3 | Batch Status: 26880/60000 (45%) | Loss: 0.411457\n",
      "Train Epoch: 3 | Batch Status: 27520/60000 (46%) | Loss: 0.445300\n",
      "Train Epoch: 3 | Batch Status: 28160/60000 (47%) | Loss: 0.547766\n",
      "Train Epoch: 3 | Batch Status: 28800/60000 (48%) | Loss: 0.246355\n",
      "Train Epoch: 3 | Batch Status: 29440/60000 (49%) | Loss: 0.284986\n",
      "Train Epoch: 3 | Batch Status: 30080/60000 (50%) | Loss: 0.450257\n",
      "Train Epoch: 3 | Batch Status: 30720/60000 (51%) | Loss: 0.276802\n",
      "Train Epoch: 3 | Batch Status: 31360/60000 (52%) | Loss: 0.313133\n",
      "Train Epoch: 3 | Batch Status: 32000/60000 (53%) | Loss: 0.451130\n",
      "Train Epoch: 3 | Batch Status: 32640/60000 (54%) | Loss: 0.499741\n",
      "Train Epoch: 3 | Batch Status: 33280/60000 (55%) | Loss: 0.301442\n",
      "Train Epoch: 3 | Batch Status: 33920/60000 (57%) | Loss: 0.430822\n",
      "Train Epoch: 3 | Batch Status: 34560/60000 (58%) | Loss: 0.196250\n",
      "Train Epoch: 3 | Batch Status: 35200/60000 (59%) | Loss: 0.313601\n",
      "Train Epoch: 3 | Batch Status: 35840/60000 (60%) | Loss: 0.428312\n",
      "Train Epoch: 3 | Batch Status: 36480/60000 (61%) | Loss: 0.533426\n",
      "Train Epoch: 3 | Batch Status: 37120/60000 (62%) | Loss: 0.317754\n",
      "Train Epoch: 3 | Batch Status: 37760/60000 (63%) | Loss: 0.400809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 | Batch Status: 38400/60000 (64%) | Loss: 0.258070\n",
      "Train Epoch: 3 | Batch Status: 39040/60000 (65%) | Loss: 0.277935\n",
      "Train Epoch: 3 | Batch Status: 39680/60000 (66%) | Loss: 0.099071\n",
      "Train Epoch: 3 | Batch Status: 40320/60000 (67%) | Loss: 0.276987\n",
      "Train Epoch: 3 | Batch Status: 40960/60000 (68%) | Loss: 0.269845\n",
      "Train Epoch: 3 | Batch Status: 41600/60000 (69%) | Loss: 0.472186\n",
      "Train Epoch: 3 | Batch Status: 42240/60000 (70%) | Loss: 0.293287\n",
      "Train Epoch: 3 | Batch Status: 42880/60000 (71%) | Loss: 0.366023\n",
      "Train Epoch: 3 | Batch Status: 43520/60000 (72%) | Loss: 0.300302\n",
      "Train Epoch: 3 | Batch Status: 44160/60000 (74%) | Loss: 0.228050\n",
      "Train Epoch: 3 | Batch Status: 44800/60000 (75%) | Loss: 0.298473\n",
      "Train Epoch: 3 | Batch Status: 45440/60000 (76%) | Loss: 0.183068\n",
      "Train Epoch: 3 | Batch Status: 46080/60000 (77%) | Loss: 0.175977\n",
      "Train Epoch: 3 | Batch Status: 46720/60000 (78%) | Loss: 0.287106\n",
      "Train Epoch: 3 | Batch Status: 47360/60000 (79%) | Loss: 0.351118\n",
      "Train Epoch: 3 | Batch Status: 48000/60000 (80%) | Loss: 0.158670\n",
      "Train Epoch: 3 | Batch Status: 48640/60000 (81%) | Loss: 0.337670\n",
      "Train Epoch: 3 | Batch Status: 49280/60000 (82%) | Loss: 0.325563\n",
      "Train Epoch: 3 | Batch Status: 49920/60000 (83%) | Loss: 0.198300\n",
      "Train Epoch: 3 | Batch Status: 50560/60000 (84%) | Loss: 0.294344\n",
      "Train Epoch: 3 | Batch Status: 51200/60000 (85%) | Loss: 0.242169\n",
      "Train Epoch: 3 | Batch Status: 51840/60000 (86%) | Loss: 0.222564\n",
      "Train Epoch: 3 | Batch Status: 52480/60000 (87%) | Loss: 0.232634\n",
      "Train Epoch: 3 | Batch Status: 53120/60000 (88%) | Loss: 0.490483\n",
      "Train Epoch: 3 | Batch Status: 53760/60000 (90%) | Loss: 0.452663\n",
      "Train Epoch: 3 | Batch Status: 54400/60000 (91%) | Loss: 0.235787\n",
      "Train Epoch: 3 | Batch Status: 55040/60000 (92%) | Loss: 0.137062\n",
      "Train Epoch: 3 | Batch Status: 55680/60000 (93%) | Loss: 0.336635\n",
      "Train Epoch: 3 | Batch Status: 56320/60000 (94%) | Loss: 0.181582\n",
      "Train Epoch: 3 | Batch Status: 56960/60000 (95%) | Loss: 0.309828\n",
      "Train Epoch: 3 | Batch Status: 57600/60000 (96%) | Loss: 0.289574\n",
      "Train Epoch: 3 | Batch Status: 58240/60000 (97%) | Loss: 0.306079\n",
      "Train Epoch: 3 | Batch Status: 58880/60000 (98%) | Loss: 0.324733\n",
      "Train Epoch: 3 | Batch Status: 59520/60000 (99%) | Loss: 0.227006\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0045, Accuracy: 9166/10000 (92%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 4 | Batch Status: 0/60000 (0%) | Loss: 0.306614\n",
      "Train Epoch: 4 | Batch Status: 640/60000 (1%) | Loss: 0.295392\n",
      "Train Epoch: 4 | Batch Status: 1280/60000 (2%) | Loss: 0.211178\n",
      "Train Epoch: 4 | Batch Status: 1920/60000 (3%) | Loss: 0.318675\n",
      "Train Epoch: 4 | Batch Status: 2560/60000 (4%) | Loss: 0.423138\n",
      "Train Epoch: 4 | Batch Status: 3200/60000 (5%) | Loss: 0.348136\n",
      "Train Epoch: 4 | Batch Status: 3840/60000 (6%) | Loss: 0.232106\n",
      "Train Epoch: 4 | Batch Status: 4480/60000 (7%) | Loss: 0.213085\n",
      "Train Epoch: 4 | Batch Status: 5120/60000 (9%) | Loss: 0.287350\n",
      "Train Epoch: 4 | Batch Status: 5760/60000 (10%) | Loss: 0.300780\n",
      "Train Epoch: 4 | Batch Status: 6400/60000 (11%) | Loss: 0.195294\n",
      "Train Epoch: 4 | Batch Status: 7040/60000 (12%) | Loss: 0.314013\n",
      "Train Epoch: 4 | Batch Status: 7680/60000 (13%) | Loss: 0.354576\n",
      "Train Epoch: 4 | Batch Status: 8320/60000 (14%) | Loss: 0.347833\n",
      "Train Epoch: 4 | Batch Status: 8960/60000 (15%) | Loss: 0.373400\n",
      "Train Epoch: 4 | Batch Status: 9600/60000 (16%) | Loss: 0.309987\n",
      "Train Epoch: 4 | Batch Status: 10240/60000 (17%) | Loss: 0.290242\n",
      "Train Epoch: 4 | Batch Status: 10880/60000 (18%) | Loss: 0.184143\n",
      "Train Epoch: 4 | Batch Status: 11520/60000 (19%) | Loss: 0.280890\n",
      "Train Epoch: 4 | Batch Status: 12160/60000 (20%) | Loss: 0.253604\n",
      "Train Epoch: 4 | Batch Status: 12800/60000 (21%) | Loss: 0.243061\n",
      "Train Epoch: 4 | Batch Status: 13440/60000 (22%) | Loss: 0.235343\n",
      "Train Epoch: 4 | Batch Status: 14080/60000 (23%) | Loss: 0.295360\n",
      "Train Epoch: 4 | Batch Status: 14720/60000 (25%) | Loss: 0.262889\n",
      "Train Epoch: 4 | Batch Status: 15360/60000 (26%) | Loss: 0.228156\n",
      "Train Epoch: 4 | Batch Status: 16000/60000 (27%) | Loss: 0.272161\n",
      "Train Epoch: 4 | Batch Status: 16640/60000 (28%) | Loss: 0.107291\n",
      "Train Epoch: 4 | Batch Status: 17280/60000 (29%) | Loss: 0.120880\n",
      "Train Epoch: 4 | Batch Status: 17920/60000 (30%) | Loss: 0.188064\n",
      "Train Epoch: 4 | Batch Status: 18560/60000 (31%) | Loss: 0.332458\n",
      "Train Epoch: 4 | Batch Status: 19200/60000 (32%) | Loss: 0.397651\n",
      "Train Epoch: 4 | Batch Status: 19840/60000 (33%) | Loss: 0.484067\n",
      "Train Epoch: 4 | Batch Status: 20480/60000 (34%) | Loss: 0.359968\n",
      "Train Epoch: 4 | Batch Status: 21120/60000 (35%) | Loss: 0.302416\n",
      "Train Epoch: 4 | Batch Status: 21760/60000 (36%) | Loss: 0.254538\n",
      "Train Epoch: 4 | Batch Status: 22400/60000 (37%) | Loss: 0.175677\n",
      "Train Epoch: 4 | Batch Status: 23040/60000 (38%) | Loss: 0.127851\n",
      "Train Epoch: 4 | Batch Status: 23680/60000 (39%) | Loss: 0.199744\n",
      "Train Epoch: 4 | Batch Status: 24320/60000 (41%) | Loss: 0.166049\n",
      "Train Epoch: 4 | Batch Status: 24960/60000 (42%) | Loss: 0.345836\n",
      "Train Epoch: 4 | Batch Status: 25600/60000 (43%) | Loss: 0.292848\n",
      "Train Epoch: 4 | Batch Status: 26240/60000 (44%) | Loss: 0.145299\n",
      "Train Epoch: 4 | Batch Status: 26880/60000 (45%) | Loss: 0.123303\n",
      "Train Epoch: 4 | Batch Status: 27520/60000 (46%) | Loss: 0.210753\n",
      "Train Epoch: 4 | Batch Status: 28160/60000 (47%) | Loss: 0.323442\n",
      "Train Epoch: 4 | Batch Status: 28800/60000 (48%) | Loss: 0.231526\n",
      "Train Epoch: 4 | Batch Status: 29440/60000 (49%) | Loss: 0.336244\n",
      "Train Epoch: 4 | Batch Status: 30080/60000 (50%) | Loss: 0.269237\n",
      "Train Epoch: 4 | Batch Status: 30720/60000 (51%) | Loss: 0.125028\n",
      "Train Epoch: 4 | Batch Status: 31360/60000 (52%) | Loss: 0.128756\n",
      "Train Epoch: 4 | Batch Status: 32000/60000 (53%) | Loss: 0.193177\n",
      "Train Epoch: 4 | Batch Status: 32640/60000 (54%) | Loss: 0.200399\n",
      "Train Epoch: 4 | Batch Status: 33280/60000 (55%) | Loss: 0.312237\n",
      "Train Epoch: 4 | Batch Status: 33920/60000 (57%) | Loss: 0.196579\n",
      "Train Epoch: 4 | Batch Status: 34560/60000 (58%) | Loss: 0.217903\n",
      "Train Epoch: 4 | Batch Status: 35200/60000 (59%) | Loss: 0.109125\n",
      "Train Epoch: 4 | Batch Status: 35840/60000 (60%) | Loss: 0.070146\n",
      "Train Epoch: 4 | Batch Status: 36480/60000 (61%) | Loss: 0.116136\n",
      "Train Epoch: 4 | Batch Status: 37120/60000 (62%) | Loss: 0.271680\n",
      "Train Epoch: 4 | Batch Status: 37760/60000 (63%) | Loss: 0.215914\n",
      "Train Epoch: 4 | Batch Status: 38400/60000 (64%) | Loss: 0.297492\n",
      "Train Epoch: 4 | Batch Status: 39040/60000 (65%) | Loss: 0.419160\n",
      "Train Epoch: 4 | Batch Status: 39680/60000 (66%) | Loss: 0.261064\n",
      "Train Epoch: 4 | Batch Status: 40320/60000 (67%) | Loss: 0.214548\n",
      "Train Epoch: 4 | Batch Status: 40960/60000 (68%) | Loss: 0.379517\n",
      "Train Epoch: 4 | Batch Status: 41600/60000 (69%) | Loss: 0.184006\n",
      "Train Epoch: 4 | Batch Status: 42240/60000 (70%) | Loss: 0.213014\n",
      "Train Epoch: 4 | Batch Status: 42880/60000 (71%) | Loss: 0.275731\n",
      "Train Epoch: 4 | Batch Status: 43520/60000 (72%) | Loss: 0.244636\n",
      "Train Epoch: 4 | Batch Status: 44160/60000 (74%) | Loss: 0.142809\n",
      "Train Epoch: 4 | Batch Status: 44800/60000 (75%) | Loss: 0.210418\n",
      "Train Epoch: 4 | Batch Status: 45440/60000 (76%) | Loss: 0.339956\n",
      "Train Epoch: 4 | Batch Status: 46080/60000 (77%) | Loss: 0.331247\n",
      "Train Epoch: 4 | Batch Status: 46720/60000 (78%) | Loss: 0.166446\n",
      "Train Epoch: 4 | Batch Status: 47360/60000 (79%) | Loss: 0.267366\n",
      "Train Epoch: 4 | Batch Status: 48000/60000 (80%) | Loss: 0.254043\n",
      "Train Epoch: 4 | Batch Status: 48640/60000 (81%) | Loss: 0.234231\n",
      "Train Epoch: 4 | Batch Status: 49280/60000 (82%) | Loss: 0.171541\n",
      "Train Epoch: 4 | Batch Status: 49920/60000 (83%) | Loss: 0.201197\n",
      "Train Epoch: 4 | Batch Status: 50560/60000 (84%) | Loss: 0.261660\n",
      "Train Epoch: 4 | Batch Status: 51200/60000 (85%) | Loss: 0.290501\n",
      "Train Epoch: 4 | Batch Status: 51840/60000 (86%) | Loss: 0.145134\n",
      "Train Epoch: 4 | Batch Status: 52480/60000 (87%) | Loss: 0.291202\n",
      "Train Epoch: 4 | Batch Status: 53120/60000 (88%) | Loss: 0.238249\n",
      "Train Epoch: 4 | Batch Status: 53760/60000 (90%) | Loss: 0.159461\n",
      "Train Epoch: 4 | Batch Status: 54400/60000 (91%) | Loss: 0.259921\n",
      "Train Epoch: 4 | Batch Status: 55040/60000 (92%) | Loss: 0.167292\n",
      "Train Epoch: 4 | Batch Status: 55680/60000 (93%) | Loss: 0.329324\n",
      "Train Epoch: 4 | Batch Status: 56320/60000 (94%) | Loss: 0.267494\n",
      "Train Epoch: 4 | Batch Status: 56960/60000 (95%) | Loss: 0.284840\n",
      "Train Epoch: 4 | Batch Status: 57600/60000 (96%) | Loss: 0.273712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 | Batch Status: 58240/60000 (97%) | Loss: 0.082243\n",
      "Train Epoch: 4 | Batch Status: 58880/60000 (98%) | Loss: 0.307689\n",
      "Train Epoch: 4 | Batch Status: 59520/60000 (99%) | Loss: 0.407953\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0033, Accuracy: 9401/10000 (94%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 5 | Batch Status: 0/60000 (0%) | Loss: 0.171137\n",
      "Train Epoch: 5 | Batch Status: 640/60000 (1%) | Loss: 0.200580\n",
      "Train Epoch: 5 | Batch Status: 1280/60000 (2%) | Loss: 0.183387\n",
      "Train Epoch: 5 | Batch Status: 1920/60000 (3%) | Loss: 0.103449\n",
      "Train Epoch: 5 | Batch Status: 2560/60000 (4%) | Loss: 0.149939\n",
      "Train Epoch: 5 | Batch Status: 3200/60000 (5%) | Loss: 0.150600\n",
      "Train Epoch: 5 | Batch Status: 3840/60000 (6%) | Loss: 0.216037\n",
      "Train Epoch: 5 | Batch Status: 4480/60000 (7%) | Loss: 0.195243\n",
      "Train Epoch: 5 | Batch Status: 5120/60000 (9%) | Loss: 0.315436\n",
      "Train Epoch: 5 | Batch Status: 5760/60000 (10%) | Loss: 0.133608\n",
      "Train Epoch: 5 | Batch Status: 6400/60000 (11%) | Loss: 0.178145\n",
      "Train Epoch: 5 | Batch Status: 7040/60000 (12%) | Loss: 0.193969\n",
      "Train Epoch: 5 | Batch Status: 7680/60000 (13%) | Loss: 0.180898\n",
      "Train Epoch: 5 | Batch Status: 8320/60000 (14%) | Loss: 0.166372\n",
      "Train Epoch: 5 | Batch Status: 8960/60000 (15%) | Loss: 0.156500\n",
      "Train Epoch: 5 | Batch Status: 9600/60000 (16%) | Loss: 0.220451\n",
      "Train Epoch: 5 | Batch Status: 10240/60000 (17%) | Loss: 0.134415\n",
      "Train Epoch: 5 | Batch Status: 10880/60000 (18%) | Loss: 0.195563\n",
      "Train Epoch: 5 | Batch Status: 11520/60000 (19%) | Loss: 0.339801\n",
      "Train Epoch: 5 | Batch Status: 12160/60000 (20%) | Loss: 0.160486\n",
      "Train Epoch: 5 | Batch Status: 12800/60000 (21%) | Loss: 0.278061\n",
      "Train Epoch: 5 | Batch Status: 13440/60000 (22%) | Loss: 0.092904\n",
      "Train Epoch: 5 | Batch Status: 14080/60000 (23%) | Loss: 0.133523\n",
      "Train Epoch: 5 | Batch Status: 14720/60000 (25%) | Loss: 0.225096\n",
      "Train Epoch: 5 | Batch Status: 15360/60000 (26%) | Loss: 0.087068\n",
      "Train Epoch: 5 | Batch Status: 16000/60000 (27%) | Loss: 0.234993\n",
      "Train Epoch: 5 | Batch Status: 16640/60000 (28%) | Loss: 0.181484\n",
      "Train Epoch: 5 | Batch Status: 17280/60000 (29%) | Loss: 0.122668\n",
      "Train Epoch: 5 | Batch Status: 17920/60000 (30%) | Loss: 0.319623\n",
      "Train Epoch: 5 | Batch Status: 18560/60000 (31%) | Loss: 0.048963\n",
      "Train Epoch: 5 | Batch Status: 19200/60000 (32%) | Loss: 0.151501\n",
      "Train Epoch: 5 | Batch Status: 19840/60000 (33%) | Loss: 0.138959\n",
      "Train Epoch: 5 | Batch Status: 20480/60000 (34%) | Loss: 0.185232\n",
      "Train Epoch: 5 | Batch Status: 21120/60000 (35%) | Loss: 0.246874\n",
      "Train Epoch: 5 | Batch Status: 21760/60000 (36%) | Loss: 0.185434\n",
      "Train Epoch: 5 | Batch Status: 22400/60000 (37%) | Loss: 0.101636\n",
      "Train Epoch: 5 | Batch Status: 23040/60000 (38%) | Loss: 0.186246\n",
      "Train Epoch: 5 | Batch Status: 23680/60000 (39%) | Loss: 0.491445\n",
      "Train Epoch: 5 | Batch Status: 24320/60000 (41%) | Loss: 0.133687\n",
      "Train Epoch: 5 | Batch Status: 24960/60000 (42%) | Loss: 0.288752\n",
      "Train Epoch: 5 | Batch Status: 25600/60000 (43%) | Loss: 0.093070\n",
      "Train Epoch: 5 | Batch Status: 26240/60000 (44%) | Loss: 0.143202\n",
      "Train Epoch: 5 | Batch Status: 26880/60000 (45%) | Loss: 0.174509\n",
      "Train Epoch: 5 | Batch Status: 27520/60000 (46%) | Loss: 0.094393\n",
      "Train Epoch: 5 | Batch Status: 28160/60000 (47%) | Loss: 0.171182\n",
      "Train Epoch: 5 | Batch Status: 28800/60000 (48%) | Loss: 0.138146\n",
      "Train Epoch: 5 | Batch Status: 29440/60000 (49%) | Loss: 0.173950\n",
      "Train Epoch: 5 | Batch Status: 30080/60000 (50%) | Loss: 0.227733\n",
      "Train Epoch: 5 | Batch Status: 30720/60000 (51%) | Loss: 0.143487\n",
      "Train Epoch: 5 | Batch Status: 31360/60000 (52%) | Loss: 0.286461\n",
      "Train Epoch: 5 | Batch Status: 32000/60000 (53%) | Loss: 0.230938\n",
      "Train Epoch: 5 | Batch Status: 32640/60000 (54%) | Loss: 0.196736\n",
      "Train Epoch: 5 | Batch Status: 33280/60000 (55%) | Loss: 0.102178\n",
      "Train Epoch: 5 | Batch Status: 33920/60000 (57%) | Loss: 0.155853\n",
      "Train Epoch: 5 | Batch Status: 34560/60000 (58%) | Loss: 0.117414\n",
      "Train Epoch: 5 | Batch Status: 35200/60000 (59%) | Loss: 0.072414\n",
      "Train Epoch: 5 | Batch Status: 35840/60000 (60%) | Loss: 0.179158\n",
      "Train Epoch: 5 | Batch Status: 36480/60000 (61%) | Loss: 0.149197\n",
      "Train Epoch: 5 | Batch Status: 37120/60000 (62%) | Loss: 0.064806\n",
      "Train Epoch: 5 | Batch Status: 37760/60000 (63%) | Loss: 0.162770\n",
      "Train Epoch: 5 | Batch Status: 38400/60000 (64%) | Loss: 0.136251\n",
      "Train Epoch: 5 | Batch Status: 39040/60000 (65%) | Loss: 0.248445\n",
      "Train Epoch: 5 | Batch Status: 39680/60000 (66%) | Loss: 0.105055\n",
      "Train Epoch: 5 | Batch Status: 40320/60000 (67%) | Loss: 0.459420\n",
      "Train Epoch: 5 | Batch Status: 40960/60000 (68%) | Loss: 0.276831\n",
      "Train Epoch: 5 | Batch Status: 41600/60000 (69%) | Loss: 0.122088\n",
      "Train Epoch: 5 | Batch Status: 42240/60000 (70%) | Loss: 0.056700\n",
      "Train Epoch: 5 | Batch Status: 42880/60000 (71%) | Loss: 0.154928\n",
      "Train Epoch: 5 | Batch Status: 43520/60000 (72%) | Loss: 0.181955\n",
      "Train Epoch: 5 | Batch Status: 44160/60000 (74%) | Loss: 0.380003\n",
      "Train Epoch: 5 | Batch Status: 44800/60000 (75%) | Loss: 0.146016\n",
      "Train Epoch: 5 | Batch Status: 45440/60000 (76%) | Loss: 0.251845\n",
      "Train Epoch: 5 | Batch Status: 46080/60000 (77%) | Loss: 0.090687\n",
      "Train Epoch: 5 | Batch Status: 46720/60000 (78%) | Loss: 0.057904\n",
      "Train Epoch: 5 | Batch Status: 47360/60000 (79%) | Loss: 0.150967\n",
      "Train Epoch: 5 | Batch Status: 48000/60000 (80%) | Loss: 0.199188\n",
      "Train Epoch: 5 | Batch Status: 48640/60000 (81%) | Loss: 0.045007\n",
      "Train Epoch: 5 | Batch Status: 49280/60000 (82%) | Loss: 0.251838\n",
      "Train Epoch: 5 | Batch Status: 49920/60000 (83%) | Loss: 0.063037\n",
      "Train Epoch: 5 | Batch Status: 50560/60000 (84%) | Loss: 0.217175\n",
      "Train Epoch: 5 | Batch Status: 51200/60000 (85%) | Loss: 0.246796\n",
      "Train Epoch: 5 | Batch Status: 51840/60000 (86%) | Loss: 0.084091\n",
      "Train Epoch: 5 | Batch Status: 52480/60000 (87%) | Loss: 0.064204\n",
      "Train Epoch: 5 | Batch Status: 53120/60000 (88%) | Loss: 0.299880\n",
      "Train Epoch: 5 | Batch Status: 53760/60000 (90%) | Loss: 0.256647\n",
      "Train Epoch: 5 | Batch Status: 54400/60000 (91%) | Loss: 0.106628\n",
      "Train Epoch: 5 | Batch Status: 55040/60000 (92%) | Loss: 0.124763\n",
      "Train Epoch: 5 | Batch Status: 55680/60000 (93%) | Loss: 0.434847\n",
      "Train Epoch: 5 | Batch Status: 56320/60000 (94%) | Loss: 0.272521\n",
      "Train Epoch: 5 | Batch Status: 56960/60000 (95%) | Loss: 0.226079\n",
      "Train Epoch: 5 | Batch Status: 57600/60000 (96%) | Loss: 0.125822\n",
      "Train Epoch: 5 | Batch Status: 58240/60000 (97%) | Loss: 0.107106\n",
      "Train Epoch: 5 | Batch Status: 58880/60000 (98%) | Loss: 0.197714\n",
      "Train Epoch: 5 | Batch Status: 59520/60000 (99%) | Loss: 0.083759\n",
      "Training time: 0m 6s\n",
      "===========================\n",
      "Test set: Average loss: 0.0027, Accuracy: 9518/10000 (95%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 6 | Batch Status: 0/60000 (0%) | Loss: 0.127121\n",
      "Train Epoch: 6 | Batch Status: 640/60000 (1%) | Loss: 0.107688\n",
      "Train Epoch: 6 | Batch Status: 1280/60000 (2%) | Loss: 0.083011\n",
      "Train Epoch: 6 | Batch Status: 1920/60000 (3%) | Loss: 0.090790\n",
      "Train Epoch: 6 | Batch Status: 2560/60000 (4%) | Loss: 0.126630\n",
      "Train Epoch: 6 | Batch Status: 3200/60000 (5%) | Loss: 0.124699\n",
      "Train Epoch: 6 | Batch Status: 3840/60000 (6%) | Loss: 0.146266\n",
      "Train Epoch: 6 | Batch Status: 4480/60000 (7%) | Loss: 0.215834\n",
      "Train Epoch: 6 | Batch Status: 5120/60000 (9%) | Loss: 0.117486\n",
      "Train Epoch: 6 | Batch Status: 5760/60000 (10%) | Loss: 0.131844\n",
      "Train Epoch: 6 | Batch Status: 6400/60000 (11%) | Loss: 0.197915\n",
      "Train Epoch: 6 | Batch Status: 7040/60000 (12%) | Loss: 0.086297\n",
      "Train Epoch: 6 | Batch Status: 7680/60000 (13%) | Loss: 0.119113\n",
      "Train Epoch: 6 | Batch Status: 8320/60000 (14%) | Loss: 0.182018\n",
      "Train Epoch: 6 | Batch Status: 8960/60000 (15%) | Loss: 0.168524\n",
      "Train Epoch: 6 | Batch Status: 9600/60000 (16%) | Loss: 0.076645\n",
      "Train Epoch: 6 | Batch Status: 10240/60000 (17%) | Loss: 0.100755\n",
      "Train Epoch: 6 | Batch Status: 10880/60000 (18%) | Loss: 0.178204\n",
      "Train Epoch: 6 | Batch Status: 11520/60000 (19%) | Loss: 0.157719\n",
      "Train Epoch: 6 | Batch Status: 12160/60000 (20%) | Loss: 0.061606\n",
      "Train Epoch: 6 | Batch Status: 12800/60000 (21%) | Loss: 0.136569\n",
      "Train Epoch: 6 | Batch Status: 13440/60000 (22%) | Loss: 0.153760\n",
      "Train Epoch: 6 | Batch Status: 14080/60000 (23%) | Loss: 0.135496\n",
      "Train Epoch: 6 | Batch Status: 14720/60000 (25%) | Loss: 0.105117\n",
      "Train Epoch: 6 | Batch Status: 15360/60000 (26%) | Loss: 0.115328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 | Batch Status: 16000/60000 (27%) | Loss: 0.100422\n",
      "Train Epoch: 6 | Batch Status: 16640/60000 (28%) | Loss: 0.154272\n",
      "Train Epoch: 6 | Batch Status: 17280/60000 (29%) | Loss: 0.130436\n",
      "Train Epoch: 6 | Batch Status: 17920/60000 (30%) | Loss: 0.099936\n",
      "Train Epoch: 6 | Batch Status: 18560/60000 (31%) | Loss: 0.076543\n",
      "Train Epoch: 6 | Batch Status: 19200/60000 (32%) | Loss: 0.142934\n",
      "Train Epoch: 6 | Batch Status: 19840/60000 (33%) | Loss: 0.169804\n",
      "Train Epoch: 6 | Batch Status: 20480/60000 (34%) | Loss: 0.102548\n",
      "Train Epoch: 6 | Batch Status: 21120/60000 (35%) | Loss: 0.298933\n",
      "Train Epoch: 6 | Batch Status: 21760/60000 (36%) | Loss: 0.132138\n",
      "Train Epoch: 6 | Batch Status: 22400/60000 (37%) | Loss: 0.286423\n",
      "Train Epoch: 6 | Batch Status: 23040/60000 (38%) | Loss: 0.444421\n",
      "Train Epoch: 6 | Batch Status: 23680/60000 (39%) | Loss: 0.267681\n",
      "Train Epoch: 6 | Batch Status: 24320/60000 (41%) | Loss: 0.134622\n",
      "Train Epoch: 6 | Batch Status: 24960/60000 (42%) | Loss: 0.296805\n",
      "Train Epoch: 6 | Batch Status: 25600/60000 (43%) | Loss: 0.196302\n",
      "Train Epoch: 6 | Batch Status: 26240/60000 (44%) | Loss: 0.187251\n",
      "Train Epoch: 6 | Batch Status: 26880/60000 (45%) | Loss: 0.166667\n",
      "Train Epoch: 6 | Batch Status: 27520/60000 (46%) | Loss: 0.090001\n",
      "Train Epoch: 6 | Batch Status: 28160/60000 (47%) | Loss: 0.176237\n",
      "Train Epoch: 6 | Batch Status: 28800/60000 (48%) | Loss: 0.063673\n",
      "Train Epoch: 6 | Batch Status: 29440/60000 (49%) | Loss: 0.153386\n",
      "Train Epoch: 6 | Batch Status: 30080/60000 (50%) | Loss: 0.156025\n",
      "Train Epoch: 6 | Batch Status: 30720/60000 (51%) | Loss: 0.230689\n",
      "Train Epoch: 6 | Batch Status: 31360/60000 (52%) | Loss: 0.142779\n",
      "Train Epoch: 6 | Batch Status: 32000/60000 (53%) | Loss: 0.215628\n",
      "Train Epoch: 6 | Batch Status: 32640/60000 (54%) | Loss: 0.200972\n",
      "Train Epoch: 6 | Batch Status: 33280/60000 (55%) | Loss: 0.140590\n",
      "Train Epoch: 6 | Batch Status: 33920/60000 (57%) | Loss: 0.064446\n",
      "Train Epoch: 6 | Batch Status: 34560/60000 (58%) | Loss: 0.065604\n",
      "Train Epoch: 6 | Batch Status: 35200/60000 (59%) | Loss: 0.087148\n",
      "Train Epoch: 6 | Batch Status: 35840/60000 (60%) | Loss: 0.054557\n",
      "Train Epoch: 6 | Batch Status: 36480/60000 (61%) | Loss: 0.062678\n",
      "Train Epoch: 6 | Batch Status: 37120/60000 (62%) | Loss: 0.128933\n",
      "Train Epoch: 6 | Batch Status: 37760/60000 (63%) | Loss: 0.054927\n",
      "Train Epoch: 6 | Batch Status: 38400/60000 (64%) | Loss: 0.074064\n",
      "Train Epoch: 6 | Batch Status: 39040/60000 (65%) | Loss: 0.101720\n",
      "Train Epoch: 6 | Batch Status: 39680/60000 (66%) | Loss: 0.144691\n",
      "Train Epoch: 6 | Batch Status: 40320/60000 (67%) | Loss: 0.245949\n",
      "Train Epoch: 6 | Batch Status: 40960/60000 (68%) | Loss: 0.056495\n",
      "Train Epoch: 6 | Batch Status: 41600/60000 (69%) | Loss: 0.176070\n",
      "Train Epoch: 6 | Batch Status: 42240/60000 (70%) | Loss: 0.236280\n",
      "Train Epoch: 6 | Batch Status: 42880/60000 (71%) | Loss: 0.069509\n",
      "Train Epoch: 6 | Batch Status: 43520/60000 (72%) | Loss: 0.066239\n",
      "Train Epoch: 6 | Batch Status: 44160/60000 (74%) | Loss: 0.090869\n",
      "Train Epoch: 6 | Batch Status: 44800/60000 (75%) | Loss: 0.104221\n",
      "Train Epoch: 6 | Batch Status: 45440/60000 (76%) | Loss: 0.065952\n",
      "Train Epoch: 6 | Batch Status: 46080/60000 (77%) | Loss: 0.208235\n",
      "Train Epoch: 6 | Batch Status: 46720/60000 (78%) | Loss: 0.198629\n",
      "Train Epoch: 6 | Batch Status: 47360/60000 (79%) | Loss: 0.057653\n",
      "Train Epoch: 6 | Batch Status: 48000/60000 (80%) | Loss: 0.340884\n",
      "Train Epoch: 6 | Batch Status: 48640/60000 (81%) | Loss: 0.129283\n",
      "Train Epoch: 6 | Batch Status: 49280/60000 (82%) | Loss: 0.164881\n",
      "Train Epoch: 6 | Batch Status: 49920/60000 (83%) | Loss: 0.197426\n",
      "Train Epoch: 6 | Batch Status: 50560/60000 (84%) | Loss: 0.173914\n",
      "Train Epoch: 6 | Batch Status: 51200/60000 (85%) | Loss: 0.125713\n",
      "Train Epoch: 6 | Batch Status: 51840/60000 (86%) | Loss: 0.082995\n",
      "Train Epoch: 6 | Batch Status: 52480/60000 (87%) | Loss: 0.221124\n",
      "Train Epoch: 6 | Batch Status: 53120/60000 (88%) | Loss: 0.127734\n",
      "Train Epoch: 6 | Batch Status: 53760/60000 (90%) | Loss: 0.106981\n",
      "Train Epoch: 6 | Batch Status: 54400/60000 (91%) | Loss: 0.076921\n",
      "Train Epoch: 6 | Batch Status: 55040/60000 (92%) | Loss: 0.146534\n",
      "Train Epoch: 6 | Batch Status: 55680/60000 (93%) | Loss: 0.047197\n",
      "Train Epoch: 6 | Batch Status: 56320/60000 (94%) | Loss: 0.049681\n",
      "Train Epoch: 6 | Batch Status: 56960/60000 (95%) | Loss: 0.274502\n",
      "Train Epoch: 6 | Batch Status: 57600/60000 (96%) | Loss: 0.205450\n",
      "Train Epoch: 6 | Batch Status: 58240/60000 (97%) | Loss: 0.121344\n",
      "Train Epoch: 6 | Batch Status: 58880/60000 (98%) | Loss: 0.068553\n",
      "Train Epoch: 6 | Batch Status: 59520/60000 (99%) | Loss: 0.035696\n",
      "Training time: 0m 7s\n",
      "===========================\n",
      "Test set: Average loss: 0.0025, Accuracy: 9528/10000 (95%)\n",
      "Testing time: 0m 8s\n",
      "Train Epoch: 7 | Batch Status: 0/60000 (0%) | Loss: 0.023355\n",
      "Train Epoch: 7 | Batch Status: 640/60000 (1%) | Loss: 0.180722\n",
      "Train Epoch: 7 | Batch Status: 1280/60000 (2%) | Loss: 0.073983\n",
      "Train Epoch: 7 | Batch Status: 1920/60000 (3%) | Loss: 0.046329\n",
      "Train Epoch: 7 | Batch Status: 2560/60000 (4%) | Loss: 0.148075\n",
      "Train Epoch: 7 | Batch Status: 3200/60000 (5%) | Loss: 0.110904\n",
      "Train Epoch: 7 | Batch Status: 3840/60000 (6%) | Loss: 0.161396\n",
      "Train Epoch: 7 | Batch Status: 4480/60000 (7%) | Loss: 0.242372\n",
      "Train Epoch: 7 | Batch Status: 5120/60000 (9%) | Loss: 0.110163\n",
      "Train Epoch: 7 | Batch Status: 5760/60000 (10%) | Loss: 0.132560\n",
      "Train Epoch: 7 | Batch Status: 6400/60000 (11%) | Loss: 0.140831\n",
      "Train Epoch: 7 | Batch Status: 7040/60000 (12%) | Loss: 0.210699\n",
      "Train Epoch: 7 | Batch Status: 7680/60000 (13%) | Loss: 0.140759\n",
      "Train Epoch: 7 | Batch Status: 8320/60000 (14%) | Loss: 0.086319\n",
      "Train Epoch: 7 | Batch Status: 8960/60000 (15%) | Loss: 0.138169\n",
      "Train Epoch: 7 | Batch Status: 9600/60000 (16%) | Loss: 0.068088\n",
      "Train Epoch: 7 | Batch Status: 10240/60000 (17%) | Loss: 0.129071\n",
      "Train Epoch: 7 | Batch Status: 10880/60000 (18%) | Loss: 0.242858\n",
      "Train Epoch: 7 | Batch Status: 11520/60000 (19%) | Loss: 0.078319\n",
      "Train Epoch: 7 | Batch Status: 12160/60000 (20%) | Loss: 0.143085\n",
      "Train Epoch: 7 | Batch Status: 12800/60000 (21%) | Loss: 0.160883\n",
      "Train Epoch: 7 | Batch Status: 13440/60000 (22%) | Loss: 0.114620\n",
      "Train Epoch: 7 | Batch Status: 14080/60000 (23%) | Loss: 0.178072\n",
      "Train Epoch: 7 | Batch Status: 14720/60000 (25%) | Loss: 0.078847\n",
      "Train Epoch: 7 | Batch Status: 15360/60000 (26%) | Loss: 0.074420\n",
      "Train Epoch: 7 | Batch Status: 16000/60000 (27%) | Loss: 0.028453\n",
      "Train Epoch: 7 | Batch Status: 16640/60000 (28%) | Loss: 0.130515\n",
      "Train Epoch: 7 | Batch Status: 17280/60000 (29%) | Loss: 0.114419\n",
      "Train Epoch: 7 | Batch Status: 17920/60000 (30%) | Loss: 0.083955\n",
      "Train Epoch: 7 | Batch Status: 18560/60000 (31%) | Loss: 0.111281\n",
      "Train Epoch: 7 | Batch Status: 19200/60000 (32%) | Loss: 0.157058\n",
      "Train Epoch: 7 | Batch Status: 19840/60000 (33%) | Loss: 0.235834\n",
      "Train Epoch: 7 | Batch Status: 20480/60000 (34%) | Loss: 0.138030\n",
      "Train Epoch: 7 | Batch Status: 21120/60000 (35%) | Loss: 0.066826\n",
      "Train Epoch: 7 | Batch Status: 21760/60000 (36%) | Loss: 0.106792\n",
      "Train Epoch: 7 | Batch Status: 22400/60000 (37%) | Loss: 0.135940\n",
      "Train Epoch: 7 | Batch Status: 23040/60000 (38%) | Loss: 0.076310\n",
      "Train Epoch: 7 | Batch Status: 23680/60000 (39%) | Loss: 0.073298\n",
      "Train Epoch: 7 | Batch Status: 24320/60000 (41%) | Loss: 0.084841\n",
      "Train Epoch: 7 | Batch Status: 24960/60000 (42%) | Loss: 0.112761\n",
      "Train Epoch: 7 | Batch Status: 25600/60000 (43%) | Loss: 0.101351\n",
      "Train Epoch: 7 | Batch Status: 26240/60000 (44%) | Loss: 0.055495\n",
      "Train Epoch: 7 | Batch Status: 26880/60000 (45%) | Loss: 0.076164\n",
      "Train Epoch: 7 | Batch Status: 27520/60000 (46%) | Loss: 0.121678\n",
      "Train Epoch: 7 | Batch Status: 28160/60000 (47%) | Loss: 0.071681\n",
      "Train Epoch: 7 | Batch Status: 28800/60000 (48%) | Loss: 0.056198\n",
      "Train Epoch: 7 | Batch Status: 29440/60000 (49%) | Loss: 0.238963\n",
      "Train Epoch: 7 | Batch Status: 30080/60000 (50%) | Loss: 0.219888\n",
      "Train Epoch: 7 | Batch Status: 30720/60000 (51%) | Loss: 0.199714\n",
      "Train Epoch: 7 | Batch Status: 31360/60000 (52%) | Loss: 0.191610\n",
      "Train Epoch: 7 | Batch Status: 32000/60000 (53%) | Loss: 0.136703\n",
      "Train Epoch: 7 | Batch Status: 32640/60000 (54%) | Loss: 0.058160\n",
      "Train Epoch: 7 | Batch Status: 33280/60000 (55%) | Loss: 0.071087\n",
      "Train Epoch: 7 | Batch Status: 33920/60000 (57%) | Loss: 0.149749\n",
      "Train Epoch: 7 | Batch Status: 34560/60000 (58%) | Loss: 0.076202\n",
      "Train Epoch: 7 | Batch Status: 35200/60000 (59%) | Loss: 0.198751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 | Batch Status: 35840/60000 (60%) | Loss: 0.138338\n",
      "Train Epoch: 7 | Batch Status: 36480/60000 (61%) | Loss: 0.196670\n",
      "Train Epoch: 7 | Batch Status: 37120/60000 (62%) | Loss: 0.154664\n",
      "Train Epoch: 7 | Batch Status: 37760/60000 (63%) | Loss: 0.095878\n",
      "Train Epoch: 7 | Batch Status: 38400/60000 (64%) | Loss: 0.204984\n",
      "Train Epoch: 7 | Batch Status: 39040/60000 (65%) | Loss: 0.116927\n",
      "Train Epoch: 7 | Batch Status: 39680/60000 (66%) | Loss: 0.031301\n",
      "Train Epoch: 7 | Batch Status: 40320/60000 (67%) | Loss: 0.236929\n",
      "Train Epoch: 7 | Batch Status: 40960/60000 (68%) | Loss: 0.076864\n",
      "Train Epoch: 7 | Batch Status: 41600/60000 (69%) | Loss: 0.104300\n",
      "Train Epoch: 7 | Batch Status: 42240/60000 (70%) | Loss: 0.124216\n",
      "Train Epoch: 7 | Batch Status: 42880/60000 (71%) | Loss: 0.156327\n",
      "Train Epoch: 7 | Batch Status: 43520/60000 (72%) | Loss: 0.130405\n",
      "Train Epoch: 7 | Batch Status: 44160/60000 (74%) | Loss: 0.072816\n",
      "Train Epoch: 7 | Batch Status: 44800/60000 (75%) | Loss: 0.216302\n",
      "Train Epoch: 7 | Batch Status: 45440/60000 (76%) | Loss: 0.043002\n",
      "Train Epoch: 7 | Batch Status: 46080/60000 (77%) | Loss: 0.262210\n",
      "Train Epoch: 7 | Batch Status: 46720/60000 (78%) | Loss: 0.205586\n",
      "Train Epoch: 7 | Batch Status: 47360/60000 (79%) | Loss: 0.054462\n",
      "Train Epoch: 7 | Batch Status: 48000/60000 (80%) | Loss: 0.063820\n",
      "Train Epoch: 7 | Batch Status: 48640/60000 (81%) | Loss: 0.132906\n",
      "Train Epoch: 7 | Batch Status: 49280/60000 (82%) | Loss: 0.212922\n",
      "Train Epoch: 7 | Batch Status: 49920/60000 (83%) | Loss: 0.054684\n",
      "Train Epoch: 7 | Batch Status: 50560/60000 (84%) | Loss: 0.055249\n",
      "Train Epoch: 7 | Batch Status: 51200/60000 (85%) | Loss: 0.163864\n",
      "Train Epoch: 7 | Batch Status: 51840/60000 (86%) | Loss: 0.092161\n",
      "Train Epoch: 7 | Batch Status: 52480/60000 (87%) | Loss: 0.444141\n",
      "Train Epoch: 7 | Batch Status: 53120/60000 (88%) | Loss: 0.221983\n",
      "Train Epoch: 7 | Batch Status: 53760/60000 (90%) | Loss: 0.064140\n",
      "Train Epoch: 7 | Batch Status: 54400/60000 (91%) | Loss: 0.057517\n",
      "Train Epoch: 7 | Batch Status: 55040/60000 (92%) | Loss: 0.141171\n",
      "Train Epoch: 7 | Batch Status: 55680/60000 (93%) | Loss: 0.153210\n",
      "Train Epoch: 7 | Batch Status: 56320/60000 (94%) | Loss: 0.172502\n",
      "Train Epoch: 7 | Batch Status: 56960/60000 (95%) | Loss: 0.061689\n",
      "Train Epoch: 7 | Batch Status: 57600/60000 (96%) | Loss: 0.062095\n",
      "Train Epoch: 7 | Batch Status: 58240/60000 (97%) | Loss: 0.130298\n",
      "Train Epoch: 7 | Batch Status: 58880/60000 (98%) | Loss: 0.128361\n",
      "Train Epoch: 7 | Batch Status: 59520/60000 (99%) | Loss: 0.270088\n",
      "Training time: 0m 7s\n",
      "===========================\n",
      "Test set: Average loss: 0.0020, Accuracy: 9647/10000 (96%)\n",
      "Testing time: 0m 8s\n",
      "Train Epoch: 8 | Batch Status: 0/60000 (0%) | Loss: 0.066271\n",
      "Train Epoch: 8 | Batch Status: 640/60000 (1%) | Loss: 0.149918\n",
      "Train Epoch: 8 | Batch Status: 1280/60000 (2%) | Loss: 0.078665\n",
      "Train Epoch: 8 | Batch Status: 1920/60000 (3%) | Loss: 0.040904\n",
      "Train Epoch: 8 | Batch Status: 2560/60000 (4%) | Loss: 0.065505\n",
      "Train Epoch: 8 | Batch Status: 3200/60000 (5%) | Loss: 0.066302\n",
      "Train Epoch: 8 | Batch Status: 3840/60000 (6%) | Loss: 0.286788\n",
      "Train Epoch: 8 | Batch Status: 4480/60000 (7%) | Loss: 0.055638\n",
      "Train Epoch: 8 | Batch Status: 5120/60000 (9%) | Loss: 0.025171\n",
      "Train Epoch: 8 | Batch Status: 5760/60000 (10%) | Loss: 0.177465\n",
      "Train Epoch: 8 | Batch Status: 6400/60000 (11%) | Loss: 0.174981\n",
      "Train Epoch: 8 | Batch Status: 7040/60000 (12%) | Loss: 0.114328\n",
      "Train Epoch: 8 | Batch Status: 7680/60000 (13%) | Loss: 0.265602\n",
      "Train Epoch: 8 | Batch Status: 8320/60000 (14%) | Loss: 0.033946\n",
      "Train Epoch: 8 | Batch Status: 8960/60000 (15%) | Loss: 0.135358\n",
      "Train Epoch: 8 | Batch Status: 9600/60000 (16%) | Loss: 0.046637\n",
      "Train Epoch: 8 | Batch Status: 10240/60000 (17%) | Loss: 0.079042\n",
      "Train Epoch: 8 | Batch Status: 10880/60000 (18%) | Loss: 0.041454\n",
      "Train Epoch: 8 | Batch Status: 11520/60000 (19%) | Loss: 0.148679\n",
      "Train Epoch: 8 | Batch Status: 12160/60000 (20%) | Loss: 0.222331\n",
      "Train Epoch: 8 | Batch Status: 12800/60000 (21%) | Loss: 0.091376\n",
      "Train Epoch: 8 | Batch Status: 13440/60000 (22%) | Loss: 0.069935\n",
      "Train Epoch: 8 | Batch Status: 14080/60000 (23%) | Loss: 0.084864\n",
      "Train Epoch: 8 | Batch Status: 14720/60000 (25%) | Loss: 0.255159\n",
      "Train Epoch: 8 | Batch Status: 15360/60000 (26%) | Loss: 0.120212\n",
      "Train Epoch: 8 | Batch Status: 16000/60000 (27%) | Loss: 0.088859\n",
      "Train Epoch: 8 | Batch Status: 16640/60000 (28%) | Loss: 0.131007\n",
      "Train Epoch: 8 | Batch Status: 17280/60000 (29%) | Loss: 0.140731\n",
      "Train Epoch: 8 | Batch Status: 17920/60000 (30%) | Loss: 0.216819\n",
      "Train Epoch: 8 | Batch Status: 18560/60000 (31%) | Loss: 0.132889\n",
      "Train Epoch: 8 | Batch Status: 19200/60000 (32%) | Loss: 0.101928\n",
      "Train Epoch: 8 | Batch Status: 19840/60000 (33%) | Loss: 0.196923\n",
      "Train Epoch: 8 | Batch Status: 20480/60000 (34%) | Loss: 0.141642\n",
      "Train Epoch: 8 | Batch Status: 21120/60000 (35%) | Loss: 0.147311\n",
      "Train Epoch: 8 | Batch Status: 21760/60000 (36%) | Loss: 0.162252\n",
      "Train Epoch: 8 | Batch Status: 22400/60000 (37%) | Loss: 0.083345\n",
      "Train Epoch: 8 | Batch Status: 23040/60000 (38%) | Loss: 0.167875\n",
      "Train Epoch: 8 | Batch Status: 23680/60000 (39%) | Loss: 0.129243\n",
      "Train Epoch: 8 | Batch Status: 24320/60000 (41%) | Loss: 0.085836\n",
      "Train Epoch: 8 | Batch Status: 24960/60000 (42%) | Loss: 0.072692\n",
      "Train Epoch: 8 | Batch Status: 25600/60000 (43%) | Loss: 0.072471\n",
      "Train Epoch: 8 | Batch Status: 26240/60000 (44%) | Loss: 0.219813\n",
      "Train Epoch: 8 | Batch Status: 26880/60000 (45%) | Loss: 0.098430\n",
      "Train Epoch: 8 | Batch Status: 27520/60000 (46%) | Loss: 0.056154\n",
      "Train Epoch: 8 | Batch Status: 28160/60000 (47%) | Loss: 0.263162\n",
      "Train Epoch: 8 | Batch Status: 28800/60000 (48%) | Loss: 0.027544\n",
      "Train Epoch: 8 | Batch Status: 29440/60000 (49%) | Loss: 0.167680\n",
      "Train Epoch: 8 | Batch Status: 30080/60000 (50%) | Loss: 0.185290\n",
      "Train Epoch: 8 | Batch Status: 30720/60000 (51%) | Loss: 0.047333\n",
      "Train Epoch: 8 | Batch Status: 31360/60000 (52%) | Loss: 0.072812\n",
      "Train Epoch: 8 | Batch Status: 32000/60000 (53%) | Loss: 0.144623\n",
      "Train Epoch: 8 | Batch Status: 32640/60000 (54%) | Loss: 0.089636\n",
      "Train Epoch: 8 | Batch Status: 33280/60000 (55%) | Loss: 0.064084\n",
      "Train Epoch: 8 | Batch Status: 33920/60000 (57%) | Loss: 0.098262\n",
      "Train Epoch: 8 | Batch Status: 34560/60000 (58%) | Loss: 0.060617\n",
      "Train Epoch: 8 | Batch Status: 35200/60000 (59%) | Loss: 0.175971\n",
      "Train Epoch: 8 | Batch Status: 35840/60000 (60%) | Loss: 0.034786\n",
      "Train Epoch: 8 | Batch Status: 36480/60000 (61%) | Loss: 0.053542\n",
      "Train Epoch: 8 | Batch Status: 37120/60000 (62%) | Loss: 0.018117\n",
      "Train Epoch: 8 | Batch Status: 37760/60000 (63%) | Loss: 0.017327\n",
      "Train Epoch: 8 | Batch Status: 38400/60000 (64%) | Loss: 0.157623\n",
      "Train Epoch: 8 | Batch Status: 39040/60000 (65%) | Loss: 0.135898\n",
      "Train Epoch: 8 | Batch Status: 39680/60000 (66%) | Loss: 0.102503\n",
      "Train Epoch: 8 | Batch Status: 40320/60000 (67%) | Loss: 0.070823\n",
      "Train Epoch: 8 | Batch Status: 40960/60000 (68%) | Loss: 0.044314\n",
      "Train Epoch: 8 | Batch Status: 41600/60000 (69%) | Loss: 0.136430\n",
      "Train Epoch: 8 | Batch Status: 42240/60000 (70%) | Loss: 0.158834\n",
      "Train Epoch: 8 | Batch Status: 42880/60000 (71%) | Loss: 0.210458\n",
      "Train Epoch: 8 | Batch Status: 43520/60000 (72%) | Loss: 0.189453\n",
      "Train Epoch: 8 | Batch Status: 44160/60000 (74%) | Loss: 0.053123\n",
      "Train Epoch: 8 | Batch Status: 44800/60000 (75%) | Loss: 0.075207\n",
      "Train Epoch: 8 | Batch Status: 45440/60000 (76%) | Loss: 0.122006\n",
      "Train Epoch: 8 | Batch Status: 46080/60000 (77%) | Loss: 0.124571\n",
      "Train Epoch: 8 | Batch Status: 46720/60000 (78%) | Loss: 0.128444\n",
      "Train Epoch: 8 | Batch Status: 47360/60000 (79%) | Loss: 0.047512\n",
      "Train Epoch: 8 | Batch Status: 48000/60000 (80%) | Loss: 0.199700\n",
      "Train Epoch: 8 | Batch Status: 48640/60000 (81%) | Loss: 0.215449\n",
      "Train Epoch: 8 | Batch Status: 49280/60000 (82%) | Loss: 0.081054\n",
      "Train Epoch: 8 | Batch Status: 49920/60000 (83%) | Loss: 0.049672\n",
      "Train Epoch: 8 | Batch Status: 50560/60000 (84%) | Loss: 0.047126\n",
      "Train Epoch: 8 | Batch Status: 51200/60000 (85%) | Loss: 0.068804\n",
      "Train Epoch: 8 | Batch Status: 51840/60000 (86%) | Loss: 0.064199\n",
      "Train Epoch: 8 | Batch Status: 52480/60000 (87%) | Loss: 0.028390\n",
      "Train Epoch: 8 | Batch Status: 53120/60000 (88%) | Loss: 0.075757\n",
      "Train Epoch: 8 | Batch Status: 53760/60000 (90%) | Loss: 0.276967\n",
      "Train Epoch: 8 | Batch Status: 54400/60000 (91%) | Loss: 0.181221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 | Batch Status: 55040/60000 (92%) | Loss: 0.126224\n",
      "Train Epoch: 8 | Batch Status: 55680/60000 (93%) | Loss: 0.061800\n",
      "Train Epoch: 8 | Batch Status: 56320/60000 (94%) | Loss: 0.112623\n",
      "Train Epoch: 8 | Batch Status: 56960/60000 (95%) | Loss: 0.043430\n",
      "Train Epoch: 8 | Batch Status: 57600/60000 (96%) | Loss: 0.072011\n",
      "Train Epoch: 8 | Batch Status: 58240/60000 (97%) | Loss: 0.176626\n",
      "Train Epoch: 8 | Batch Status: 58880/60000 (98%) | Loss: 0.187947\n",
      "Train Epoch: 8 | Batch Status: 59520/60000 (99%) | Loss: 0.155622\n",
      "Training time: 0m 7s\n",
      "===========================\n",
      "Test set: Average loss: 0.0021, Accuracy: 9610/10000 (96%)\n",
      "Testing time: 0m 8s\n",
      "Train Epoch: 9 | Batch Status: 0/60000 (0%) | Loss: 0.158322\n",
      "Train Epoch: 9 | Batch Status: 640/60000 (1%) | Loss: 0.030324\n",
      "Train Epoch: 9 | Batch Status: 1280/60000 (2%) | Loss: 0.025376\n",
      "Train Epoch: 9 | Batch Status: 1920/60000 (3%) | Loss: 0.146930\n",
      "Train Epoch: 9 | Batch Status: 2560/60000 (4%) | Loss: 0.058884\n",
      "Train Epoch: 9 | Batch Status: 3200/60000 (5%) | Loss: 0.033188\n",
      "Train Epoch: 9 | Batch Status: 3840/60000 (6%) | Loss: 0.125712\n",
      "Train Epoch: 9 | Batch Status: 4480/60000 (7%) | Loss: 0.091649\n",
      "Train Epoch: 9 | Batch Status: 5120/60000 (9%) | Loss: 0.201794\n",
      "Train Epoch: 9 | Batch Status: 5760/60000 (10%) | Loss: 0.070385\n",
      "Train Epoch: 9 | Batch Status: 6400/60000 (11%) | Loss: 0.115719\n",
      "Train Epoch: 9 | Batch Status: 7040/60000 (12%) | Loss: 0.079701\n",
      "Train Epoch: 9 | Batch Status: 7680/60000 (13%) | Loss: 0.123860\n",
      "Train Epoch: 9 | Batch Status: 8320/60000 (14%) | Loss: 0.057069\n",
      "Train Epoch: 9 | Batch Status: 8960/60000 (15%) | Loss: 0.077343\n",
      "Train Epoch: 9 | Batch Status: 9600/60000 (16%) | Loss: 0.077377\n",
      "Train Epoch: 9 | Batch Status: 10240/60000 (17%) | Loss: 0.084521\n",
      "Train Epoch: 9 | Batch Status: 10880/60000 (18%) | Loss: 0.106721\n",
      "Train Epoch: 9 | Batch Status: 11520/60000 (19%) | Loss: 0.041836\n",
      "Train Epoch: 9 | Batch Status: 12160/60000 (20%) | Loss: 0.194996\n",
      "Train Epoch: 9 | Batch Status: 12800/60000 (21%) | Loss: 0.093307\n",
      "Train Epoch: 9 | Batch Status: 13440/60000 (22%) | Loss: 0.075078\n",
      "Train Epoch: 9 | Batch Status: 14080/60000 (23%) | Loss: 0.173074\n",
      "Train Epoch: 9 | Batch Status: 14720/60000 (25%) | Loss: 0.058013\n",
      "Train Epoch: 9 | Batch Status: 15360/60000 (26%) | Loss: 0.095417\n",
      "Train Epoch: 9 | Batch Status: 16000/60000 (27%) | Loss: 0.043099\n",
      "Train Epoch: 9 | Batch Status: 16640/60000 (28%) | Loss: 0.090443\n",
      "Train Epoch: 9 | Batch Status: 17280/60000 (29%) | Loss: 0.187222\n",
      "Train Epoch: 9 | Batch Status: 17920/60000 (30%) | Loss: 0.103013\n",
      "Train Epoch: 9 | Batch Status: 18560/60000 (31%) | Loss: 0.154235\n",
      "Train Epoch: 9 | Batch Status: 19200/60000 (32%) | Loss: 0.061046\n",
      "Train Epoch: 9 | Batch Status: 19840/60000 (33%) | Loss: 0.173312\n",
      "Train Epoch: 9 | Batch Status: 20480/60000 (34%) | Loss: 0.041165\n",
      "Train Epoch: 9 | Batch Status: 21120/60000 (35%) | Loss: 0.033653\n",
      "Train Epoch: 9 | Batch Status: 21760/60000 (36%) | Loss: 0.013937\n",
      "Train Epoch: 9 | Batch Status: 22400/60000 (37%) | Loss: 0.106097\n",
      "Train Epoch: 9 | Batch Status: 23040/60000 (38%) | Loss: 0.101844\n",
      "Train Epoch: 9 | Batch Status: 23680/60000 (39%) | Loss: 0.022173\n",
      "Train Epoch: 9 | Batch Status: 24320/60000 (41%) | Loss: 0.038800\n",
      "Train Epoch: 9 | Batch Status: 24960/60000 (42%) | Loss: 0.030289\n",
      "Train Epoch: 9 | Batch Status: 25600/60000 (43%) | Loss: 0.035970\n",
      "Train Epoch: 9 | Batch Status: 26240/60000 (44%) | Loss: 0.070233\n",
      "Train Epoch: 9 | Batch Status: 26880/60000 (45%) | Loss: 0.039510\n",
      "Train Epoch: 9 | Batch Status: 27520/60000 (46%) | Loss: 0.047185\n",
      "Train Epoch: 9 | Batch Status: 28160/60000 (47%) | Loss: 0.104980\n",
      "Train Epoch: 9 | Batch Status: 28800/60000 (48%) | Loss: 0.304680\n",
      "Train Epoch: 9 | Batch Status: 29440/60000 (49%) | Loss: 0.036914\n",
      "Train Epoch: 9 | Batch Status: 30080/60000 (50%) | Loss: 0.082310\n",
      "Train Epoch: 9 | Batch Status: 30720/60000 (51%) | Loss: 0.189462\n",
      "Train Epoch: 9 | Batch Status: 31360/60000 (52%) | Loss: 0.114496\n",
      "Train Epoch: 9 | Batch Status: 32000/60000 (53%) | Loss: 0.123771\n",
      "Train Epoch: 9 | Batch Status: 32640/60000 (54%) | Loss: 0.033603\n",
      "Train Epoch: 9 | Batch Status: 33280/60000 (55%) | Loss: 0.175752\n",
      "Train Epoch: 9 | Batch Status: 33920/60000 (57%) | Loss: 0.044310\n",
      "Train Epoch: 9 | Batch Status: 34560/60000 (58%) | Loss: 0.061477\n",
      "Train Epoch: 9 | Batch Status: 35200/60000 (59%) | Loss: 0.055410\n",
      "Train Epoch: 9 | Batch Status: 35840/60000 (60%) | Loss: 0.036175\n",
      "Train Epoch: 9 | Batch Status: 36480/60000 (61%) | Loss: 0.026771\n",
      "Train Epoch: 9 | Batch Status: 37120/60000 (62%) | Loss: 0.056967\n",
      "Train Epoch: 9 | Batch Status: 37760/60000 (63%) | Loss: 0.210083\n",
      "Train Epoch: 9 | Batch Status: 38400/60000 (64%) | Loss: 0.037210\n",
      "Train Epoch: 9 | Batch Status: 39040/60000 (65%) | Loss: 0.011775\n",
      "Train Epoch: 9 | Batch Status: 39680/60000 (66%) | Loss: 0.020216\n",
      "Train Epoch: 9 | Batch Status: 40320/60000 (67%) | Loss: 0.086522\n",
      "Train Epoch: 9 | Batch Status: 40960/60000 (68%) | Loss: 0.146655\n",
      "Train Epoch: 9 | Batch Status: 41600/60000 (69%) | Loss: 0.102290\n",
      "Train Epoch: 9 | Batch Status: 42240/60000 (70%) | Loss: 0.041851\n",
      "Train Epoch: 9 | Batch Status: 42880/60000 (71%) | Loss: 0.081236\n",
      "Train Epoch: 9 | Batch Status: 43520/60000 (72%) | Loss: 0.086685\n",
      "Train Epoch: 9 | Batch Status: 44160/60000 (74%) | Loss: 0.085391\n",
      "Train Epoch: 9 | Batch Status: 44800/60000 (75%) | Loss: 0.032702\n",
      "Train Epoch: 9 | Batch Status: 45440/60000 (76%) | Loss: 0.159467\n",
      "Train Epoch: 9 | Batch Status: 46080/60000 (77%) | Loss: 0.158961\n",
      "Train Epoch: 9 | Batch Status: 46720/60000 (78%) | Loss: 0.172004\n",
      "Train Epoch: 9 | Batch Status: 47360/60000 (79%) | Loss: 0.117624\n",
      "Train Epoch: 9 | Batch Status: 48000/60000 (80%) | Loss: 0.071739\n",
      "Train Epoch: 9 | Batch Status: 48640/60000 (81%) | Loss: 0.080624\n",
      "Train Epoch: 9 | Batch Status: 49280/60000 (82%) | Loss: 0.110063\n",
      "Train Epoch: 9 | Batch Status: 49920/60000 (83%) | Loss: 0.103753\n",
      "Train Epoch: 9 | Batch Status: 50560/60000 (84%) | Loss: 0.105950\n",
      "Train Epoch: 9 | Batch Status: 51200/60000 (85%) | Loss: 0.037144\n",
      "Train Epoch: 9 | Batch Status: 51840/60000 (86%) | Loss: 0.068441\n",
      "Train Epoch: 9 | Batch Status: 52480/60000 (87%) | Loss: 0.101702\n",
      "Train Epoch: 9 | Batch Status: 53120/60000 (88%) | Loss: 0.021044\n",
      "Train Epoch: 9 | Batch Status: 53760/60000 (90%) | Loss: 0.061789\n",
      "Train Epoch: 9 | Batch Status: 54400/60000 (91%) | Loss: 0.058320\n",
      "Train Epoch: 9 | Batch Status: 55040/60000 (92%) | Loss: 0.115499\n",
      "Train Epoch: 9 | Batch Status: 55680/60000 (93%) | Loss: 0.120509\n",
      "Train Epoch: 9 | Batch Status: 56320/60000 (94%) | Loss: 0.264407\n",
      "Train Epoch: 9 | Batch Status: 56960/60000 (95%) | Loss: 0.290630\n",
      "Train Epoch: 9 | Batch Status: 57600/60000 (96%) | Loss: 0.031412\n",
      "Train Epoch: 9 | Batch Status: 58240/60000 (97%) | Loss: 0.075769\n",
      "Train Epoch: 9 | Batch Status: 58880/60000 (98%) | Loss: 0.146994\n",
      "Train Epoch: 9 | Batch Status: 59520/60000 (99%) | Loss: 0.160291\n",
      "Training time: 0m 7s\n",
      "===========================\n",
      "Test set: Average loss: 0.0017, Accuracy: 9689/10000 (97%)\n",
      "Testing time: 0m 8s\n",
      "Total Time: 1m 5s\n",
      "Model was trained on cpu!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    since = time.time()\n",
    "    for epoch in range(1, 10):\n",
    "        epoch_start = time.time()\n",
    "        train(epoch)\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
    "        test()\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
    "\n",
    "    m, s = divmod(time.time() - since, 60)\n",
    "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install pytorch torchvision cudatoolkit=11.0 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"cnn1.jpg\" style=width:600px;height:400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"cnn2.jpg\" style=width:600px;height:400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"cnn3.jpg\" style=width:600px;height:400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"cnn4.jpg\" style=width:650px;height:400px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)  # inchannel = 1 = the number of color channels in the input image.\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(20,10)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
